{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db11fb09",
   "metadata": {},
   "source": [
    "## Notebook to understand NN differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a8be8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the sub-networks for your architecture\n",
    "class PositionEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super(PositionEncoder, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class PhysicsEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super(PhysicsEncoder, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class FinalNet(nn.Module):\n",
    "    def __init__(self, embedding_dim, output_dim):\n",
    "        super(FinalNet, self).__init__()\n",
    "        self.linear = nn.Linear(embedding_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Full network combining the sub-networks\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self, pos_dim, physics_dim, embedding_dim, output_dim):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        self.pos_encoder = PositionEncoder(pos_dim, embedding_dim)\n",
    "        self.physics_encoder = PhysicsEncoder(physics_dim, embedding_dim)\n",
    "        self.final_net = FinalNet(embedding_dim, output_dim)\n",
    "\n",
    "    def forward(self, pos, physics_params):\n",
    "        pos_embedding = self.pos_encoder(pos)\n",
    "        physics_embedding = self.physics_encoder(physics_params)\n",
    "        combined_embedding = pos_embedding * physics_embedding\n",
    "        output = self.final_net(combined_embedding)\n",
    "        return output\n",
    "\n",
    "# --- Jacobian Calculation ---\n",
    "\n",
    "# Instantiate your trained network\n",
    "pos_dim = 2\n",
    "physics_dim = 5\n",
    "embedding_dim = 64\n",
    "output_dim = 1\n",
    "model = MyNetwork(pos_dim, physics_dim, embedding_dim, output_dim)\n",
    "# model.load_state_dict(torch.load('your_model.pth')) # Load your trained weights\n",
    "model.eval() # Set the model to evaluation mode\n",
    "\n",
    "# Example input data\n",
    "# Create a batch of position tensors\n",
    "pos_input = torch.randn(10, pos_dim, requires_grad=True)\n",
    "physics_input = torch.randn(10, physics_dim)\n",
    "\n",
    "# Define a function that takes only the variable we want to differentiate with respect to\n",
    "def model_forward_for_jacobian(p):\n",
    "    return model(p, physics_input)\n",
    "\n",
    "# Compute the Jacobian of the output with respect to the position input\n",
    "jacobian_matrix = torch.autograd.functional.jacobian(model_forward_for_jacobian, pos_input)\n",
    "\n",
    "# The resulting jacobian_matrix will have the shape (batch_size, output_dim, batch_size, input_dim)\n",
    "# We are interested in the diagonal elements of the batch dimensions\n",
    "jacobian_per_sample = torch.diagonal(jacobian_matrix, dim1=0, dim2=2).permute(2, 0, 1)\n",
    "\n",
    "\n",
    "print(\"Shape of the Jacobian matrix per sample:\", jacobian_per_sample.shape)\n",
    "# For a single output, this will give you a tensor of shape (batch_size, output_dim, input_dim)\n",
    "# where each [i, 0, 0] is d(output_i)/dr_i and [i, 0, 1] is d(output_i)/dz_i\n",
    "\n",
    "# To get the derivatives for the first sample in the batch:\n",
    "# d(output)/dr and d(output)/dz\n",
    "derivatives_first_sample = jacobian_per_sample[0]\n",
    "print(\"Derivatives for the first sample (d(output)/dr, d(output)/dz):\", derivatives_first_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc88f645",
   "metadata": {},
   "source": [
    "### Test with double derivative, to backprop as a PINN loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e33cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# --- Assume the same MyNetwork class definition as before ---\n",
    "class PositionEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim): super().__init__(); self.linear = nn.Linear(input_dim, embedding_dim)\n",
    "    def forward(self, x): return self.linear(x)\n",
    "\n",
    "class PhysicsEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim): super().__init__(); self.linear = nn.Linear(input_dim, embedding_dim)\n",
    "    def forward(self, x): return self.linear(x)\n",
    "\n",
    "class FinalNet(nn.Module):\n",
    "    def __init__(self, embedding_dim, output_dim): super().__init__(); self.linear = nn.Linear(embedding_dim, output_dim)\n",
    "    def forward(self, x): return self.linear(x)\n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self, pos_dim, physics_dim, embedding_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = PositionEncoder(pos_dim, embedding_dim)\n",
    "        self.physics_encoder = PhysicsEncoder(physics_dim, embedding_dim)\n",
    "        self.final_net = FinalNet(embedding_dim, output_dim)\n",
    "\n",
    "    def forward(self, pos, physics_params):\n",
    "        pos_embedding = self.pos_encoder(pos)\n",
    "        physics_embedding = self.physics_encoder(physics_params)\n",
    "        combined_embedding = pos_embedding * physics_embedding\n",
    "        output = self.final_net(combined_embedding)\n",
    "        return output\n",
    "\n",
    "# --- Setup for Training ---\n",
    "model = MyNetwork(pos_dim=2, physics_dim=5, embedding_dim=64, output_dim=1)\n",
    "model.train() # Set to training mode\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# The weight for the derivative loss term\n",
    "lambda_deriv = 1.0\n",
    "\n",
    "# --- SIMULATED TRAINING BATCH ---\n",
    "# In your real code, this would come from your DataLoader\n",
    "pos_input = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32)\n",
    "physics_input = torch.randn(2, 5)\n",
    "\n",
    "# Your dataset must now provide these \"true\" target values\n",
    "true_function_values = torch.tensor([[0.5], [-0.2]])\n",
    "true_derivatives = torch.tensor([[-1.5, 0.8], [0.1, -0.4]]) # True (∂f/∂r, ∂f/∂z)\n",
    "\n",
    "# --- A SINGLE TRAINING STEP ---\n",
    "\n",
    "# Step 1: Tell PyTorch to track gradients for the input position\n",
    "pos_input.requires_grad = True\n",
    "\n",
    "# Step 2: Clear old gradients from the optimizer\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Step 3: Forward pass to get the predicted function value\n",
    "predicted_output = model(pos_input, physics_input)\n",
    "\n",
    "# Step 4: Calculate the loss on the function value\n",
    "loss_f = mse_loss(predicted_output, true_function_values)\n",
    "\n",
    "# Step 5: Calculate the network's derivative\n",
    "# We use torch.autograd.grad which is a more fundamental way to do this\n",
    "# and shows the need for create_graph=True clearly.\n",
    "predicted_derivatives = torch.autograd.grad(\n",
    "    outputs=predicted_output,\n",
    "    inputs=pos_input,\n",
    "    grad_outputs=torch.ones_like(predicted_output),\n",
    "    create_graph=True # <-- THIS IS THE CRITICAL PART!\n",
    ")[0]\n",
    "\n",
    "# Step 6: Calculate the loss on the derivatives\n",
    "loss_deriv = mse_loss(predicted_derivatives, true_derivatives)\n",
    "\n",
    "# Step 7: Combine the losses\n",
    "total_loss = loss_f + lambda_deriv * loss_deriv\n",
    "\n",
    "# Step 8: Backpropagate the total loss to compute gradients for the network's weights\n",
    "total_loss.backward()\n",
    "\n",
    "# Step 9: Update the network weights\n",
    "optimizer.step()\n",
    "\n",
    "# --- End of Training Step ---\n",
    "\n",
    "print(f\"Loss on function value: {loss_f.item():.4f}\")\n",
    "print(f\"Loss on derivative value: {loss_deriv.item():.4f}\")\n",
    "print(f\"Total Combined Loss: {total_loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
