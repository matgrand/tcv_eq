{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Prepapre dataset with the prepare_dataset notebook, before running this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from utils import *\n",
    "  \n",
    "# DEV = torch.device(CPU) # for debugging, use cpu\n",
    "os.makedirs(f\"{SAVE_DIR}\", exist_ok=True)\n",
    "print(f'DEV: {DEV}, has_screen: {LOCAL}, job id: {JOBID}')\n",
    "\n",
    "# copy the python training to the directory (for cluster) (for local, it fails silently)\n",
    "os.system(f\"cp train.py {SAVE_DIR}/train.py\")\n",
    "os.system(f\"cp utils.py {SAVE_DIR}/utils.py\")\n",
    "\n",
    "# SMALL, NORM, BIG = \"small\", \"norm\", \"big\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDEAS\n",
    "\n",
    "- 2 different opt1s for indepdendent nets, maybe 3 actually\n",
    "- train only one branch at a time to prove it works\n",
    "- add pre and post resclaing layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPOCHS = 12 if LOCAL else 20 # number of epochs # 1000\n",
    "# BATCH_SIZE = 128 # 128 \n",
    "BATCH_SIZE = 4 if LOCAL else 64 # 64 <-\n",
    "\n",
    "# LOAD_PRETRAINED = CURR_EVAL_MODEL if LOCAL else None # pretrained model\n",
    "LOAD_PRETRAINED = None # Set it to None if you don't want to load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schedulers\n",
    "from numpy import concatenate as cat, linspace as linsp, logspace as logsp\n",
    "### learning rate\n",
    "# LR = 3e-4*logsp(0, -2, EPOCHS) \n",
    "# LR = 3e-3*np.ones(EPOCHS) \n",
    "# LR = 3e-3*logsp(0, -2, EPOCHS)  \n",
    "LR = 1e-3*logsp(0, -2, EPOCHS) # <-\n",
    "# LR = 3e-3*np.ones(EPOCHS) \n",
    "\n",
    "### loss ratios \n",
    "## gso\n",
    "# RGSO = cat((linsp(1e-6, 3e-3, EPOCHS//2), linsp(3e-3, 0.0, EPOCHS//2))) \n",
    "MAX_GSO = 5e-4 # 3e-3 <-\n",
    "# RGSO = cat((linsp(1e-6*MAX_GSO, MAX_GSO, EPOCHS//4), \n",
    "#             linsp(MAX_GSO, MAX_GSO, EPOCHS//4), \n",
    "#             linsp(MAX_GSO, 0.0, EPOCHS//4), \n",
    "#             linsp(0.0, 0.0, EPOCHS//4))) # <-\n",
    "# RGSO = cat((MAX_GSO*logsp(-6, 0, EPOCHS//4), \n",
    "#                           MAX_GSO*logsp(0, 0, EPOCHS//4), \n",
    "#                           MAX_GSO*logsp(0, -10, EPOCHS//4), \n",
    "#                           logsp(-12, -12, EPOCHS//4))) \n",
    "RGSO = np.zeros(EPOCHS) \n",
    "\n",
    "## r1\n",
    "MAX_R1 = 1.0 # 1.0 <-\n",
    "R1 = MAX_R1 * np.ones(EPOCHS) \n",
    "# R1 = cat((np.ones(3*EPOCHS//4)*MAX_R1, \n",
    "#           np.zeros(EPOCHS//4))) # <-\n",
    "\n",
    "## r2\n",
    "MAX_R2 = 1.0\n",
    "# R2 = MAX_R2 * np.ones(EPOCHS)\n",
    "# R2 = cat((linsp(1e-6*MAX_R2, MAX_R2, EPOCHS//4),\n",
    "#             linsp(MAX_R2, MAX_R2, EPOCHS//4), \n",
    "#             linsp(MAX_R2, 0.0, EPOCHS//4), \n",
    "#             linsp(0.0, 0.0, EPOCHS//4))) # <-\n",
    "R2 = np.zeros(EPOCHS)\n",
    "\n",
    "## r3\n",
    "MAX_R3 = 1.0 # 1.0 <-\n",
    "# R3 = MAX_R3 * np.ones(EPOCHS)\n",
    "# R3 = cat((np.zeros(EPOCHS//4),\n",
    "#          MAX_R3*np.ones(3*EPOCHS//4))) # <-\n",
    "R3 = np.ones(EPOCHS) * MAX_R3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks\n",
    "if LOAD_PRETRAINED is not None: assert os.path.exists(LOAD_PRETRAINED), \"Pretrained model does not exist\"\n",
    "assert os.path.exists(TRAIN_DS_PATH), \"Training dataset does not exist\"\n",
    "assert os.path.exists(EVAL_DS_PATH), \"Evaluation dataset does not exist\"\n",
    "assert os.path.exists(SAVE_DIR), \"Save directory does not exist\"\n",
    "assert len(LR) == EPOCHS, \"Learning rate array length does not match epochs\"\n",
    "assert len(RGSO) == EPOCHS, \"GSO loss ratio array length does not match epochs\"\n",
    "assert len(R1) == EPOCHS, \"R1 loss ratio array length does not match epochs\"\n",
    "assert len(R2) == EPOCHS, \"R2 loss ratio array length does not match epochs\"\n",
    "assert len(R3) == EPOCHS, \"R3 loss ratio array length does not match epochs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "plt.subplot(5, 1, 1)\n",
    "plt.plot(LR, color='red')\n",
    "plt.ylim(0, np.max(LR)*1.1)\n",
    "plt.title('Learning Rate')\n",
    "plt.subplot(5, 1, 2)\n",
    "plt.plot(R1, color='red')\n",
    "plt.ylim(0, np.max(R1)*1.1)\n",
    "plt.title('R1 Loss Ratio')\n",
    "plt.subplot(5, 1, 3)\n",
    "plt.plot(R2, color='red')\n",
    "plt.ylim(0, np.max(R2)*1.1)\n",
    "plt.title('R2 Loss Ratio')\n",
    "plt.subplot(5, 1, 4)\n",
    "plt.plot(R3, color='red')\n",
    "plt.ylim(0, np.max(R3)*1.1)\n",
    "plt.title('R3 Loss Ratio')\n",
    "plt.subplot(5, 1, 5)\n",
    "plt.plot(RGSO, color='red')\n",
    "plt.ylim(0, np.max(RGSO)*1.1)\n",
    "plt.title('GSO Loss Ratio')\n",
    "plt.show() if LOCAL else plt.savefig(f\"{SAVE_DIR}/schedulers.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and evaluation datasets\n",
    "train_ds = LiuqeDataset(TRAIN_DS_PATH)\n",
    "val_ds = LiuqeDataset(EVAL_DS_PATH)\n",
    "test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test net I/O\n",
    "test_network_io()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ovveride networks for testing\n",
    "# class LiuqeNet(Module):\n",
    "#     def __init__(self, latent_size=32):\n",
    "#         super(LiuqeNet, self).__init__()\n",
    "#         assert latent_size % 2 == 0, \"latent size should be even\"\n",
    "#         self.ngr, self.ngz = NGR, NGZ # grid size\n",
    "#         #branch\n",
    "#         self.branch = Sequential(\n",
    "#             # View(-1, input_size),\n",
    "#             Linear(NIN, 64), ActF(),\n",
    "#             Linear(64, 32), ActF(),\n",
    "#             Linear(32, latent_size), ActF(),\n",
    "#         )\n",
    "#         #trunk\n",
    "#         def trunk_block(s): \n",
    "#             return  Sequential(\n",
    "#                 # View(-1, s),\n",
    "#                 Linear(s, 32), ActF(),\n",
    "#                 Linear(32, latent_size//2), ActF(),\n",
    "#             )\n",
    "#         self.trunk_r, self.trunk_z = trunk_block(self.ngr), trunk_block(self.ngz)\n",
    "#         # head\n",
    "#         self.head = Sequential(\n",
    "#             Linear(latent_size, 64), ActF(),\n",
    "#             Linear(64, self.ngr*self.ngz), ActF(),\n",
    "#             # View(-1, 1, *self.grid_size),\n",
    "#         )\n",
    "#     def forward(self, xb, r, z):\n",
    "#         xb = self.branch(xb)\n",
    "#         r, z = self.trunk_r(r), self.trunk_z(z) \n",
    "#         xt = torch.cat((r, z), 1) # concatenate\n",
    "#         x = xt * xb # multiply trunk and branch\n",
    "#         x = self.head(x) # head net\n",
    "#         x = x.view(-1, 1, self.ngr, self.ngz) # reshape to grid\n",
    "#         return x\n",
    "    \n",
    "# class LCFSNet(Module): \n",
    "#     def __init__(self):\n",
    "#         super(LCFSNet, self).__init__()\n",
    "#         self.lin = Sequential(\n",
    "#             Linear(NIN, 64), ActF(),\n",
    "#             Linear(64, 64), ActF(),\n",
    "#             Linear(64, 2*NLCFS), ActF(),\n",
    "#         )\n",
    "#     def forward(self, x):\n",
    "#         return self.lin(x) # linear layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    print(\"Ep:  || l1  | l2  | l3  | gso || lr  | r1  | r2  | r3  | gso || t | eta| improved    \")\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True) # initialize DataLoader\n",
    "    val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)  \n",
    "    \n",
    "    # network\n",
    "    inet, gnet, h1, h2, h3 = InputNet(), GridNet(), FluxHead(), FluxHead(), LCFSHead() # initialize networks \n",
    "    model = LiuqeNet(inet, gnet, h1, h2, h3) # initialize model\n",
    "    \n",
    "    # net1 = LiuqeNet() # initialize model\n",
    "    # net2 = LCFSNet() # initialize model\n",
    "\n",
    "\n",
    "    \n",
    "    opt1 = torch.optim.Adam(model.parameters(), lr=LR[0])\n",
    "    opt1 = torch.optim.Adam( # Separate parameters for two optimizers\n",
    "        list(gnet.parameters()) + list(h1.parameters()) + list(h2.parameters()),\n",
    "        lr=LR[0]\n",
    "    )\n",
    "    opt2 = torch.optim.Adam(\n",
    "        list(h3.parameters()) + list(inet.parameters()), \n",
    "        lr=LR[0]\n",
    "    )\n",
    "\n",
    "    # opt1 = torch.optim.Adam(net1.parameters(), lr=LR[0]) # optimizer for net1\n",
    "    # opt2 = torch.optim.Adam(net2.parameters(), lr=LR[0]) # optimizer for net2\n",
    "\n",
    "    if LOAD_PRETRAINED is not None: # load pretrained model\n",
    "        model.load_state_dict(torch.load(LOAD_PRETRAINED, map_location=torch.device(CPU)), strict=STRICT_LOAD) # load pretrained model\n",
    "        print(f\"Pretrained model loaded: {LOAD_PRETRAINED}\")\n",
    "\n",
    "    model.to(DEV) # move model to DEV\n",
    "    # net1.to(DEV) # move model to DEV\n",
    "    # net2.to(DEV) # move model to DEV\n",
    "\n",
    "    loss_fn = torch.nn.MSELoss() # Mean Squared Error Loss\n",
    "    tlog, elog = np.inf*np.ones((EPOCHS, len(train_dl), len(LOSS_NAMES))), np.inf*np.ones((EPOCHS, len(val_dl), len(LOSS_NAMES))) # init log\n",
    "    start_time = time() # start time\n",
    "    for ep in range(EPOCHS): # epochs\n",
    "        epoch_time = time()\n",
    "        for pg in opt1.param_groups: pg['lr'] = LR[ep] # update learning rate\n",
    "        for pg in opt2.param_groups: pg['lr'] = LR[ep] # update learning rate\n",
    "        model.train()\n",
    "        # net1.train() # training mode\n",
    "        # net2.train() # training mode\n",
    "        for ib, batch in enumerate(train_dl):\n",
    "            if train_ds.on_dev: x, r, z, y1, y2, y3 = batch # unpack batch\n",
    "            else: x, r, z, y1, y2, y3 = map(lambda t: t.to(DEV), batch) # move to DEV\n",
    "            opt1.zero_grad(); opt2.zero_grad() # zero gradients\n",
    "            yp1, yp2, yp3 = model(x, r, z) # forward pass\n",
    "            # yp1 = net1(x, r, z) \n",
    "            # yp2 = y2\n",
    "            # yp3 = net2(x) \n",
    "            gso, gsop = calc_gso_batch(y1, r, z, dev=DEV), calc_gso_batch(yp1, r, z, dev=DEV) # calculate grad shafranov\n",
    "            # losses\n",
    "            l1 = loss_fn(y1, yp1) \n",
    "            l2 = loss_fn(y2, yp2) \n",
    "            l3 = loss_fn(y3, yp3) \n",
    "            lgso = loss_fn(gsop, gso) # PINN loss on grad shafranov\n",
    "            loss = R1[ep]*l1 + R2[ep]*l2 + R3[ep]*l3 + RGSO[ep]*lgso # total loss\n",
    "            loss.backward() # backprop\n",
    "            # l1.backward() # backprop\n",
    "            # l3.backward() # backprop\n",
    "            opt1.step(); opt2.step() # update weights\n",
    "            tlog[ep, ib] = (l1.item(), l2.item(), l3.item(), lgso.item()) # save batch losses\n",
    "        model.eval() # evaluation mode\n",
    "        # net1.eval()\n",
    "        # net2.eval()\n",
    "        with torch.no_grad():\n",
    "            for ib, batch in enumerate(val_dl):\n",
    "                if val_ds.on_dev: x, r, z, y1, y2, y3 = batch # unpack batch\n",
    "                else: x, r, z, y1, y2, y3 = map(lambda t: t.to(DEV), batch) # move to DEV\n",
    "                yp1, yp2, yp3 = model(x, r, z) # forward pass\n",
    "                # yp1 = net1(x, r, z)\n",
    "                # yp2 = y2\n",
    "                # yp3 = net2(x)\n",
    "                gso, gsop = calc_gso_batch(y1, r, z, dev=DEV), calc_gso_batch(yp1, r, z, dev=DEV)\n",
    "                # losses\n",
    "                l1 = loss_fn(y1, yp1)\n",
    "                l2 = loss_fn(y2, yp2)\n",
    "                l3 = loss_fn(y3, yp3)\n",
    "                lgso = loss_fn(gsop, gso)\n",
    "                assert not torch.isnan(l1) and not torch.isnan(l2) and not torch.isnan(l3) and not torch.isnan(lgso), \"Loss is NaN\"\n",
    "                elog[ep, ib] = (l1.item(), l2.item(), l3.item(), lgso.item()) # save batch losses\n",
    "        # save model if improved        \n",
    "        endp = \"\" \n",
    "        epoch_losses = np.mean(elog[ep,:,:], axis=0) # epoch losses: mean across batchess\n",
    "        best_losses = np.min(np.mean(elog[:ep,:,:], axis=1), axis=0) if ep > 0 else epoch_losses # best losses: min across epochs\n",
    "        for el, bl, n in zip(epoch_losses, best_losses, LOSS_NAMES):\n",
    "            if el <= bl: torch.save(model.state_dict(), model_path(n)); endp+=f\"*{n}\"\n",
    "            # if el <= bl: \n",
    "            #     torch.save(net1.state_dict(), f\"{SAVE_DIR}/net1.pt\")\n",
    "            #     torch.save(net2.state_dict(), f\"{SAVE_DIR}/net2.pt\")\n",
    "            #     endp+=f\"*{n}\"\n",
    "        # print progress\n",
    "        print(f\"{ep+1:02d}/{EPOCHS:02d}||\" +\n",
    "            '|'.join([f\"{v:.0e}\" for v in epoch_losses]) + \"||\" +\n",
    "            f\"{LR[ep]:.0e}|{R1[ep]:.0e}|{R2[ep]:.0e}|{R3[ep]:.0e}|{RGSO[ep]:.0e}||\" +\n",
    "            f\"{int(time()-epoch_time):02d}s|{int((time()-start_time)*(EPOCHS-ep)/(ep+1)/60):03d}m|\", end=endp+'\\n')\n",
    "        if ep >= 10 and best_losses[2] > 0.1: return False #01/20|1e+00|1e+00|1e-01|1e+05||1e-03|1e+00|1e-18|1e+00|0e+00|00s|000m|*l1*l2*l3*gso stop training, if not converging, try again\n",
    "\n",
    "    tlosses, elosses = np.mean(tlog, axis=1), np.mean(elog, axis=1) # losses across epochs\n",
    "    print(f\"Training time: {(time()-start_time)/60:.0f}mins\")\n",
    "    print(\"Best losses  : \" + \", \".join([f\"{n} {bl:.0e} (ep {be})\" for n, bl, be in zip(LOSS_NAMES, np.min(elosses, axis=0), np.argmin(elosses, axis=0))]))\n",
    "    print(\"Estimated MAE: \" + \", \".join([f\"{n} {np.sqrt(np.min(np.mean(elog[:,:,i], axis=1))):.3e}\" for i, n in enumerate(LOSS_NAMES)]))\n",
    "    np.save(f\"{SAVE_DIR}/train_losses.npy\", tlosses) # save training losses\n",
    "    np.save(f\"{SAVE_DIR}/eval_losses.npy\", elosses) # save evaluation losses\n",
    "    return True\n",
    "\n",
    "# train the model (multiple attempts)\n",
    "for i in range(20): \n",
    "    print('Starting training...')\n",
    "    success = train()\n",
    "    if success: break\n",
    "    else: print(f\"Convergence failed, retrying... {i+1}/20\")\n",
    "# if not success: delete the files and exit:\n",
    "if not success: os.system(f\"rm -rf {SAVE_DIR}\")\n",
    "assert success, \"Training failed, no model saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "print(\"Plotting losses...\")\n",
    "fig, ax = plt.subplots(2, len(LOSS_NAMES), figsize=(4*len(LOSS_NAMES), 8))\n",
    "# Load losses\n",
    "train_loss = np.load(f\"{SAVE_DIR}/train_losses.npy\")\n",
    "eval_loss = np.load(f\"{SAVE_DIR}/eval_losses.npy\")\n",
    "for i, name in enumerate(LOSS_NAMES):\n",
    "    # Linear scale\n",
    "    ax[0,i].plot(train_loss[:,i], label='train')\n",
    "    ax[0,i].plot(eval_loss[:,i], label='eval')\n",
    "    ax[0,i].set_title(f\"{name}\")\n",
    "    ax[0,i].set_xlabel(\"Epoch\")\n",
    "    ax[0,i].set_ylabel(\"Loss\")\n",
    "    ax[0,i].legend(); ax[0,i].grid(True)\n",
    "    # Log scale\n",
    "    ax[1,i].plot(train_loss[:,i], label='train')\n",
    "    ax[1,i].plot(eval_loss[:,i], label='eval')\n",
    "    ax[1,i].set_yscale('log')\n",
    "    ax[1,i].set_title(f\"{name} (log)\")\n",
    "    ax[1,i].set_xlabel(\"Epoch\")\n",
    "    ax[1,i].set_ylabel(\"Loss\")\n",
    "    ax[1,i].legend(); ax[1,i].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show() if LOCAL else plt.savefig(f\"{SAVE_DIR}/losses.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing network output\n",
    "print(\"Testing network output...\")\n",
    "for ln in LOSS_NAMES:\n",
    "    model = LiuqeNet(InputNet(), GridNet(), FluxHead(), FluxHead(), LCFSHead()) # initialize model\n",
    "    model.load_state_dict(torch.load(model_path(ln), map_location=torch.device(CPU))) # load pretrained model\n",
    "    plot_network_outputs(val_ds, model, title=ln) # plot network outputs\n",
    "\n",
    "# net1 = LiuqeNet() # initialize model\n",
    "# net2 = LCFSNet() # initialize model\n",
    "# net1.load_state_dict(torch.load(f\"{SAVE_DIR}/net1.pt\", map_location=torch.device(CPU))) # load pretrained model\n",
    "# net2.load_state_dict(torch.load(f\"{SAVE_DIR}/net2.pt\", map_location=torch.device(CPU))) # load pretrained model\n",
    "# net1.eval() # evaluation mode\n",
    "# net2.eval() # evaluation mode\n",
    "# net1.to(CPU)\n",
    "# net2.to(CPU)\n",
    "\n",
    "# os.makedirs(f\"{SAVE_DIR}/imgs\", exist_ok=True)\n",
    "# for i in np.random.randint(0, len(val_ds), 3 if LOCAL else 50):  \n",
    "#     x, r, z, y1, _, y3 = val_ds[i] # get random sample\n",
    "#     x, r, z, y1, y3 = map(lambda t: t.to(CPU), (x, r, z, y1, y3)) # move to DEV\n",
    "#     x, r, z, y1, y3 = x.reshape(1,-1), r.reshape(1,NGR), z.reshape(1,NGZ), y1.reshape(1,1,NGZ,NGR), y3.reshape(1,2*NLCFS)\n",
    "#     yp1 = net1(x, r, z)\n",
    "#     yp3 = net2(x)\n",
    "#     rr, zz = np.meshgrid(r.detach().cpu().numpy(), z.detach().cpu().numpy())\n",
    "#     yp1 = yp1.detach().numpy().reshape(NGZ,NGR)\n",
    "#     y1 = y1.detach().numpy().reshape(NGZ,NGR)\n",
    "#     yp3 = yp3.detach().numpy().reshape(2*NLCFS)\n",
    "#     y3 = y3.detach().numpy().reshape(2*NLCFS)\n",
    "#     s1 = 4\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.subplot(1, 3, 1)\n",
    "#     plt.scatter(rr,zz, c=y1, s=s1)\n",
    "#     # plt.plot(y3[:NLCFS], y3[NLCFS:], lw=2)\n",
    "#     plt.colorbar()\n",
    "#     plot_vessel()\n",
    "#     plt.axis('equal')\n",
    "#     plt.subplot(1, 3, 2)\n",
    "#     plt.scatter(rr,zz, c=yp1, s=s1)\n",
    "#     # plt.plot(yp3[:NLCFS], yp3[NLCFS:], lw=2)\n",
    "#     plt.colorbar()\n",
    "#     plot_vessel()\n",
    "#     plt.axis('equal')\n",
    "#     plt.subplot(1, 3, 3)\n",
    "#     plt.scatter(rr,zz, c=np.abs(y1-yp1), s=s1)\n",
    "#     plt.colorbar()\n",
    "#     plot_vessel()\n",
    "#     plt.axis('equal')\n",
    "#     plt.suptitle(f\"Sample {i} (y1)\")\n",
    "#     plt.tight_layout()\n",
    "#     plt.show() if LOCAL else plt.savefig(f\"{SAVE_DIR}/imgs/net_example_l1_{i}.png\")\n",
    "#     plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test LCFS net\n",
    "m = LiuqeNet(InputNet(), GridNet(), FluxHead(), FluxHead(), LCFSHead()) # initialize model\n",
    "m.load_state_dict(torch.load(model_path('l3'), map_location=torch.device(CPU))) # load pretrained model\n",
    "lcfs = LCFSNet(m.input_net, m.lcfs_head)\n",
    "\n",
    "# lcfs = LCFSNet() # initialize model\n",
    "# lcfs.load_state_dict(torch.load(f\"{SAVE_DIR}/net2.pt\", map_location=torch.device(CPU))) # load pretrained model\n",
    "plot_lcfs_net_out(val_ds, lcfs, title='') # plot LCFS net outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test inference speed\n",
    "print(\"Testing inference speed...\")\n",
    "try:\n",
    "    model = LiuqeNet(InputNet(), GridNet(), FluxHead(), FluxHead(), LCFSHead()) # initialize model\n",
    "    model.load_state_dict(torch.load(model_path(LOSS_NAMES[2]), map_location=torch.device(CPU))) # load pretrained model\n",
    "    model.eval()\n",
    "    ds = val_ds\n",
    "    n_samples = 100\n",
    "    random_idxs = np.random.choice(n_samples, len(ds))\n",
    "    #cpu\n",
    "    cpu_times1, cpu_times2 = [], []\n",
    "    for i in random_idxs:\n",
    "        start_t = time()\n",
    "        x, r, z, y1, y2, y3 = ds[i]\n",
    "        x, r, z = x.to(CPU), r.to(CPU), z.to(CPU)\n",
    "        x = x.view(1, -1)\n",
    "        r = r.view(1, NGZ)\n",
    "        z = z.view(1, NGR)\n",
    "        start_t2 = time()\n",
    "        yp = model(x, r, z)\n",
    "        end_t = time()\n",
    "        cpu_times1.append(end_t - start_t)\n",
    "        cpu_times2.append(end_t - start_t2)\n",
    "    # DEV\n",
    "    model.to(DEV)\n",
    "    dev_times1, dev_times2 = [], []\n",
    "    for i in random_idxs:\n",
    "        x, r, z, y1, y2, y3 = ds[i]\n",
    "        x, r, z = x.to(DEV), r.to(DEV), z.to(DEV)\n",
    "        x = x.view(1, -1)\n",
    "        r = r.view(1, NGZ)\n",
    "        z = z.view(1, NGR)\n",
    "        start_t = time()\n",
    "        start_t2 = time()\n",
    "        yp = model(x, r, z)\n",
    "        end_t = time()\n",
    "        dev_times1.append(end_t - start_t)\n",
    "        dev_times2.append(end_t - start_t2)\n",
    "    cpu_times1, dev_times1 = np.array(cpu_times1)*1000, np.array(dev_times1)*1000\n",
    "    cpu_times2, dev_times2 = np.array(cpu_times2)*1000, np.array(dev_times2)*1000\n",
    "    print(f\"cpu: inference time: [full -> {cpu_times1.mean():.5f}ms, std: {cpu_times1.std():.5f}]\")\n",
    "    print(f\"cpu: inference time: [inference only -> {cpu_times2.mean():.5f}ms, std: {cpu_times2.std():.5f}]\")\n",
    "    print(f\"dev: inference time: [full -> {dev_times1.mean():.5f}ms, std: {dev_times1.std():.5f}]\")\n",
    "    print(f\"dev: inference time: [inference only -> {dev_times2.mean():.5f}ms, std: {dev_times2.std():.5f}]\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in inference speed test: {e}\")\n",
    "    print(\"Inference speed test failed\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert best l3 network to ONNX\n",
    "try:\n",
    "    print(\"Converting best l3 network to ONNX...\")\n",
    "    m = LiuqeNet(InputNet(), GridNet(), FluxHead(), FluxHead(), LCFSHead()) # initialize model\n",
    "    m.load_state_dict(torch.load(model_path(LOSS_NAMES[2]), map_location=torch.device(CPU))) # load pretrained model\n",
    "    net = LCFSNet(m.input_net, m.lcfs_head)\n",
    "    net.to(CPU)  \n",
    "\n",
    "    dummy_input = torch.randn(1, NIN, device=CPU)\n",
    "    try:\n",
    "        torch.onnx.export(net, dummy_input, f'{SAVE_DIR}/net.onnx', export_params=True,\n",
    "                        opset_version=12, do_constant_folding=True,\n",
    "                        input_names=['x'], output_names=['y'])\n",
    "        print(\"ONNX model saved\")\n",
    "    except Exception as e: print(f\"Error exporting to ONNX: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in ONNX conversion: {e}\")\n",
    "    print(\"ONNX conversion failed\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{JOBID} done\")\n",
    "if not LOCAL: sleep(30) # wait for files to update (for cluster)\n",
    "#copy the log file to the folder\n",
    "os.system(f\"cp jobs/{JOBID}.txt {SAVE_DIR}/log.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
