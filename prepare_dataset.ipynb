{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training dataset from the equilibria dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" # disable GPU\n",
    "import numpy as np\n",
    "from time import time, sleep\n",
    "from os.path import join, exists\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "print(\"Preparing data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "RAW_DIR = 'raw_ds' if LOCAL else '/nfsd/automatica/grandinmat/raw_ds' # where the raw data is stored\n",
    "\n",
    "# hyperparameters\n",
    "N_SAMPLES = 1000 if LOCAL else 200_000 #850_000 # number of samples to use for training\n",
    "SM = 1000 if LOCAL else 3_000 # 20 # number of points per sample (SM = SAMPLE MULTIPLIER)\n",
    "TRAIN_EVAL_SPLIT = 0.8 # percentage of the dataset to use for training\n",
    "\n",
    "print(f'Total samples: {N_SAMPLES:.0f}, train samples: {N_SAMPLES*TRAIN_EVAL_SPLIT:.0f}, eval samples: {N_SAMPLES*(1-TRAIN_EVAL_SPLIT):.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check RAW_DIR exists and is not empty\n",
    "assert exists(RAW_DIR), f'RAW_DIR {RAW_DIR} does not exist'\n",
    "assert len(os.listdir(RAW_DIR)) > 0, f'RAW_DIR {RAW_DIR} is empty'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "print(\"Loading data...\")\n",
    "\n",
    "# list all the files inside RAW_DIR/ds\n",
    "files = sorted([f for f in os.listdir(RAW_DIR) if f.endswith('.mat')])\n",
    "print(f'Found {len(files)} files.')\n",
    "vals = {n:[] for n in DS_NAMES}  # dictionary to hold the numpy arrays\n",
    "files_iter = tqdm(files, desc=\"Loading files\", unit=\"file\") if LOCAL else files\n",
    "for f in files_iter:\n",
    "    try:\n",
    "    # if True:\n",
    "        d = loadmat(join(RAW_DIR, f))\n",
    "        for n in DS_NAMES: assert n in list(d.keys()), f'Key {n} not found in {f}' \n",
    "        t = d['t'].flatten()  # time\n",
    "        nt = t.shape[0]  # number of time points\n",
    "        for n in DS_NAMES:\n",
    "            if n in [BR, BZ]: continue  # skip BR and BZ for now, they will be calculated later\n",
    "            v = d[n] # get the variable\n",
    "            v = v.reshape((*DS_SIZES[n], nt)) # try to reshape it to the expected shape\n",
    "            assert v.shape == (*DS_SIZES[n], nt), f'Variable {n} shape mismatch: {v.shape} != {((*DS_SIZES[n], nt))}'\n",
    "            assert not np.isnan(v).any(), f'Variable {n} contains NaN values in {f}'\n",
    "            assert np.isfinite(v).all(), f'Variable {n} contains infinite values in {f}'\n",
    "            vals[n].append(v)  # append to the list\n",
    "    except Exception as e:\n",
    "        print(f'Error loading {f}: {e}')\n",
    "        continue\n",
    "\n",
    "print(f'Loaded {len(vals[FX])} files.')\n",
    "assert len(vals[FX]) > 0, f'No samples: {len(vals[FX])}'\n",
    "\n",
    "# convert lists to numpy arrays\n",
    "print('---------------------------------------------------')\n",
    "for n in DS_NAMES:\n",
    "    if n in [BR, BZ]: continue  # skip BR and BZ for now, they will be calculated later\n",
    "    vals[n] = np.concatenate(vals[n], axis=-1).astype(DTYPE)  # concatenate along the first dimension and convert to DTYPE\n",
    "    sh, ndim = vals[n].shape, vals[n].ndim\n",
    "    vals[n] = vals[n].transpose(ndim-1, *range(ndim-1))  # move the last dimension to the first\n",
    "    # print(f'{n} -> shape: {vals[n].shape}, dtype: {vals[n].dtype}')  # print the shape and dtype of the variable\n",
    "print('---------------------------------------------------')\n",
    "\n",
    "# calculate BR and BZ from Fx\n",
    "print('Calculating BR and BZ...')\n",
    "vals[BR], vals[BZ] = meqBrBz(vals[FX])\n",
    "\n",
    "for n in DS_NAMES: print(f'{n} -> shape: {vals[n].shape}, dtype: {vals[n].dtype}')  # print the shape and dtype of the variable\n",
    "\n",
    "for n in INPUT_NAMES: assert vals[n].ndim == 2, f'Variable {n} is not 2D: {vals[n].ndim}'\n",
    "\n",
    "# assign values to variables\n",
    "X = np.concatenate([vals[n] for n in INPUT_NAMES], axis=1)  # concatenate along the first dimension\n",
    "print(f'-----------------------------------------\\n X -> shape: {X.shape}, dtype: {X.dtype}')\n",
    "\n",
    "# make IY the same shape as FX (and pad the sides)\n",
    "tmpIy = np.zeros_like(vals[FX])  # create a zero array with the same shape as FX\n",
    "tmpIy[:, 1:-1, 1:-1] = vals[IY]  # fill the inner part with IY values\n",
    "# fill sides \n",
    "tmpIy[:, 0, 1:-1] = vals[IY][:, 0, :]  # left side\n",
    "tmpIy[:, -1, 1:-1] = vals[IY][:, -1, :]  # right side\n",
    "tmpIy[:, 1:-1, 0] = vals[IY][:, :, 0]  # top side\n",
    "tmpIy[:, 1:-1, -1] = vals[IY][:, :, -1]  # bottom side\n",
    "# corners\n",
    "tmpIy[:, 0, 0] = vals[IY][:, 0, 0]  # top left corner\n",
    "tmpIy[:, 0, -1] = vals[IY][:, 0, -1]  # top right corner\n",
    "tmpIy[:, -1, 0] = vals[IY][:, -1, 0]  # bottom left corner\n",
    "tmpIy[:, -1, -1] = vals[IY][:, -1, -1]  # bottom right corner\n",
    "vals[IY] = tmpIy  # replace IY with the new array\n",
    "\n",
    "# reassign the values to the variables\n",
    "Y = {  \n",
    "    FX:vals[FX], \n",
    "    IY:vals[IY], \n",
    "    BR:vals[BR],\n",
    "    BZ:vals[BZ],\n",
    "    RQ:vals[RQ],\n",
    "    ZQ:vals[ZQ],\n",
    "}\n",
    "\n",
    "assert Y[FX].shape[0] > 0, f'No samples: {Y[FX].shape}'\n",
    "N_OR = Y[FX].shape[0]  # number of original samples\n",
    "print(f'Loaded {N_OR} samples.')\n",
    "\n",
    "assert X.dtype == DTYPE, f'X dtype mismatch: {X.dtype} != {DTYPE}'\n",
    "assert all(y.dtype == DTYPE for y in Y.values()), f'Y dtype mismatch: {[y.dtype for y in Y.values()]} != {DTYPE}'\n",
    "\n",
    "assert X.shape == (N_OR, NIN), f'X shape mismatch: {X.shape} != ({N_OR}, {NIN})'\n",
    "\n",
    "# check the shapes\n",
    "print(f'X -> {X.shape}')\n",
    "print(f'Y -> {[y.shape for y in Y.values()]}')\n",
    "\n",
    "# print sizes in MB\n",
    "print(f'X size: {X.nbytes / 1024**2:.2f} MB, Y size: {sum([y.nbytes for y in Y.values()]) / 1024**2:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some examples\n",
    "n_plot = 3 if LOCAL else 15\n",
    "rand_idxs = np.random.randint(0, N_OR, n_plot)\n",
    "for i, ri in enumerate(rand_idxs):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.subplot(2, 4, 1)\n",
    "    plt.scatter(RRD, ZZD, c=vals[FX][ri], s=4), plt.title('FX')\n",
    "    plt.plot(vals[RQ][ri], vals[ZQ][ri], 'gray', lw=2)\n",
    "    plot_vessel(), plt.axis('equal'), plt.axis('off'), plt.colorbar()\n",
    "    plt.subplot(2, 4, 2)\n",
    "    plt.scatter(RRD, ZZD, c=vals[IY][ri], s=4), plt.title('IY')\n",
    "    plt.plot(vals[RQ][ri], vals[ZQ][ri], 'gray', lw=2)\n",
    "    plot_vessel(), plt.axis('equal'), plt.axis('off'), plt.colorbar()\n",
    "    plt.subplot(2, 4, 3)\n",
    "    plt.scatter(RRD, ZZD, c=vals[BR][ri], s=4), plt.title('BR')\n",
    "    plt.plot(vals[RQ][ri], vals[ZQ][ri], 'gray', lw=2)\n",
    "    plot_vessel(), plt.axis('equal'), plt.axis('off'), plt.colorbar()\n",
    "    plt.subplot(2, 4, 4)\n",
    "    plt.scatter(RRD, ZZD, c=vals[BZ][ri], s=4), plt.title('BZ')\n",
    "    plt.plot(vals[RQ][ri], vals[ZQ][ri], 'gray', lw=2)\n",
    "    plot_vessel(), plt.axis('equal'), plt.axis('off'), plt.colorbar()\n",
    "    plt.subplot(2, 4, 5)\n",
    "    plt.bar(np.arange(vals[BM].shape[1]), vals[BM][ri]), plt.title('BM')\n",
    "    plt.subplot(2, 4, 6)\n",
    "    plt.bar(np.arange(vals[FF].shape[1]), vals[FF][ri]), plt.title('FF')\n",
    "    plt.subplot(2, 4, 7)\n",
    "    plt.bar(np.arange(vals[IA].shape[1]), vals[IA][ri]), plt.title('IA')\n",
    "    plt.subplot(2, 4, 8)\n",
    "    plt.bar(np.arange(vals[IU].shape[1]), vals[IU][ri]), plt.title('IU')\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'SHOT {ri}')\n",
    "    plt.show() if LOCAL else plt.savefig(f'{DS_DIR}/imgs/original_{i}.png')\n",
    "    plt.close()\n",
    "del vals  # free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test interpolation\n",
    "idx = np.random.randint(0, N_OR)\n",
    "pts = sample_random_points(2000)\n",
    "intFx = interp_pts(Y[FX][idx], pts)\n",
    "intIy = interp_pts(Y[IY][idx], pts)\n",
    "intBr = interp_pts(Y[BR][idx], pts)\n",
    "intBz = interp_pts(Y[BZ][idx], pts)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "ms = 2\n",
    "plt.subplot(2, 4, 1)\n",
    "plt.scatter(RRD, ZZD, c=Y[FX][idx], s=ms), plt.title('FX')\n",
    "plt.plot(Y[RQ][idx], Y[ZQ][idx], 'gray', lw=2)\n",
    "plot_vessel(), plt.axis('equal'), plt.axis('off'), plt.colorbar()\n",
    "plt.subplot(2, 4, 5)\n",
    "plt.scatter(pts[:,0], pts[:,1], c=intFx, s=ms), plt.title('Interpolated FX')\n",
    "plot_vessel(), plt.axis('equal'), plt.axis('off'), plt.colorbar()\n",
    "plt.subplot(2, 4, 2)\n",
    "plt.scatter(RRD, ZZD, c=Y[IY][idx], s=ms), plt.title('IY')\n",
    "plt.plot(Y[RQ][idx], Y[ZQ][idx], 'gray', lw=2)\n",
    "plot_vessel(), plt.axis('equal'), plt.axis('off'), plt.colorbar()\n",
    "plt.subplot(2, 4, 6)\n",
    "plt.scatter(pts[:,0], pts[:,1], c=intIy, s=ms), plt.title('Interpolated IY')\n",
    "plot_vessel(), plt.axis('equal'), plt.axis('off'), plt.colorbar()\n",
    "plt.subplot(2, 4, 3)\n",
    "plt.scatter(RRD, ZZD, c=Y[BR][idx], s=ms), plt.title('BR')\n",
    "plt.plot(Y[RQ][idx], Y[ZQ][idx], 'gray', lw=2)\n",
    "plot_vessel(), plt.axis('equal'), plt.axis('off'), plt.colorbar()\n",
    "plt.subplot(2, 4, 7)\n",
    "plt.scatter(pts[:,0], pts[:,1], c=intBr, s=ms), plt.title('Interpolated BR')\n",
    "plot_vessel(), plt.axis('equal'), plt.axis('off'), plt.colorbar()\n",
    "plt.subplot(2, 4, 4)\n",
    "plt.scatter(RRD, ZZD, c=Y[BZ][idx], s=ms), plt.title('BZ')\n",
    "plt.plot(Y[RQ][idx], Y[ZQ][idx], 'gray', lw=2)\n",
    "plot_vessel(), plt.axis('equal'), plt.axis('off'), plt.colorbar()\n",
    "plt.subplot(2, 4, 8)\n",
    "plt.scatter(pts[:,0], pts[:,1], c=intBz, s=ms), plt.title('Interpolated BZ')\n",
    "plot_vessel(), plt.axis('equal'), plt.axis('off'), plt.colorbar()\n",
    "plt.suptitle(f'SHOT {idx} - Interpolation Example')\n",
    "plt.tight_layout()\n",
    "plt.show() if LOCAL else plt.savefig(join(DS_DIR, 'imgs', 'interpolation_example.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset splitting (N_TOP = original dataset size)\n",
    "NT = int(N_SAMPLES*TRAIN_EVAL_SPLIT)    # training\n",
    "NE = N_SAMPLES - NT                     # evaluation\n",
    "print(f\"Train -> NT:{NT}\")\n",
    "print(f\"Eval  -> NE:{NE}\")\n",
    "orig_idxs = np.random.permutation(N_OR)\n",
    "orig_idxs_train = orig_idxs[:int(N_OR*TRAIN_EVAL_SPLIT)] # original indices for training\n",
    "orig_idxs_eval = orig_idxs[int(N_OR*TRAIN_EVAL_SPLIT):] # original indices for evaluation\n",
    "# splitting the idxs\n",
    "assert len(orig_idxs_train) > NT, f\"Training set is too small, {len(orig_idxs_train)} < {NT}\"\n",
    "idxs_t = np.random.choice(orig_idxs_train, NT, replace=False) \n",
    "assert len(orig_idxs_eval) > NE, f\"Evaluation set is too small, {len(orig_idxs_eval)} < {NE}\"\n",
    "idxs_e = np.random.choice(orig_idxs_eval, NE, replace=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create arrays to store the dataset\n",
    "print(f\"Preallocating arrays for the dataset...\")\n",
    "\n",
    "vt = {\n",
    "    PHYS:   np.zeros((NT, NIN), dtype=DTYPE),\n",
    "    PTS:    np.zeros((NT, SM, 2), dtype=DTYPE),\n",
    "    FX:     np.zeros((NT, SM), dtype=DTYPE),\n",
    "    IY:     np.zeros((NT, SM), dtype=DTYPE),\n",
    "    BR:     np.zeros((NT, SM), dtype=DTYPE),\n",
    "    BZ:     np.zeros((NT, SM), dtype=DTYPE),\n",
    "    SEP:    np.zeros((NT, 2*NLCFS), dtype=DTYPE),\n",
    "}\n",
    "ve = {\n",
    "    PHYS:   np.zeros((NE, NIN), dtype=DTYPE),\n",
    "    PTS:    np.zeros((NE, SM, 2), dtype=DTYPE),\n",
    "    FX:     np.zeros((NE, SM), dtype=DTYPE),\n",
    "    IY:     np.zeros((NE, SM), dtype=DTYPE),\n",
    "    BR:     np.zeros((NE, SM), dtype=DTYPE),\n",
    "    BZ:     np.zeros((NE, SM), dtype=DTYPE),\n",
    "    SEP:    np.zeros((NE, 2*NLCFS), dtype=DTYPE),\n",
    "    FG:     np.zeros((NE, 4, 65, 28), dtype=DTYPE), # store the full grid maps for evaluation\n",
    "}\n",
    "\n",
    "# estimate RAM usage\n",
    "print(f\"Estimated RAM usage: {sum(arr.nbytes for arr in vt.values()) + sum(arr.nbytes for arr in ve.values()) / (1024**3):.2f} GB\")\n",
    "print(\"Filling arrays...\")\n",
    "\n",
    "## fill the arrays\n",
    "print_every = 2000\n",
    "start_time = time()\n",
    "for i, idx in enumerate(idxs_t):\n",
    "    vt[PHYS][i] = X[idx] # physical inputs\n",
    "    vt[SEP][i] = np.concatenate([Y[RQ][idx], Y[ZQ][idx]], axis=0)  # LCFS/separatrix points\n",
    "    pts = sample_random_points(SM)  # sample random points\n",
    "    vt[PTS][i] = pts\n",
    "    vt[FX][i] = interp_pts(Y[FX][idx], pts)  \n",
    "    vt[IY][i] = interp_pts(Y[IY][idx], pts)  \n",
    "    vt[BR][i] = interp_pts(Y[BR][idx], pts)  \n",
    "    vt[BZ][i] = interp_pts(Y[BZ][idx], pts)   \n",
    "    if (i+1) % print_every == 0: print(f\"Train -> {100*(i+1)/NT:.1f}%, eta: {((time()-start_time)/(i+1)*(NT-i-1))/60:.1f} min\")\n",
    "\n",
    "start_time = time()\n",
    "for i, idx in enumerate(idxs_e):\n",
    "    ve[PHYS][i] = X[idx] # physical inputs\n",
    "    ve[SEP][i] = np.concatenate([Y[RQ][idx], Y[ZQ][idx]], axis=0)  # LCFS/separatrix points\n",
    "    pts = sample_random_points(SM)  # sample random points\n",
    "    ve[PTS][i] = pts\n",
    "    ve[FX][i] = interp_pts(Y[FX][idx], pts)  \n",
    "    ve[IY][i] = interp_pts(Y[IY][idx], pts)  \n",
    "    ve[BR][i] = interp_pts(Y[BR][idx], pts)  \n",
    "    ve[BZ][i] = interp_pts(Y[BZ][idx], pts)  \n",
    "    ve[FG][i] = (Y[FX][idx], Y[IY][idx], Y[BR][idx], Y[BZ][idx])  # store the full maps for evaluation\n",
    "    if (i+1) % print_every == 0: print(f\"Eval -> {100*(i+1)/NE:.1f}%, eta: {((time()-start_time)/(i+1)*(NE-i-1))/60:.1f} min\")\n",
    "# print shapes\n",
    "print(f\"Train -> PHYS:{vt[PHYS].shape}, PTS:{vt[PTS].shape}, FX:{vt[FX].shape}, IY:{vt[IY].shape}, BR:{vt[BR].shape}, BZ:{vt[BZ].shape}, SEP:{vt[SEP].shape}\")\n",
    "print(f\"Eval  -> PHYS:{ve[PHYS].shape}, PTS:{ve[PTS].shape}, FX:{ve[FX].shape}, IY:{ve[IY].shape}, BR:{ve[BR].shape}, BZ:{ve[BZ].shape}, SEP:{ve[SEP].shape}, FG:{ve[FG].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and normalize, NOTE: y3 does not require normalization\n",
    "print(\"Getting normalization constants for the dataset...\")\n",
    "μx, Σx = np.mean(vt[PHYS], axis=0), np.std(vt[PHYS], axis=0)  # mean and std of the physical inputs\n",
    "# normalize (NOTE: both with the training means and stds)\n",
    "# xt, xe = (xt - μx) / Σx, (xe - μx) / Σx # not normalizing the inputs bc we added a normalization layer in the network architecture\n",
    "print(f'μx: {μx.shape}, Σx: {Σx.shape}')\n",
    "x_mean_std = np.array([μx, Σx])\n",
    "print(f'x_mean_std: {x_mean_std.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset as numpy compressed\n",
    "print(f\"Saving datasets to {TRAIN_DS_PATH} and {EVAL_DS_PATH}...\")\n",
    "try:\n",
    "    np.savez_compressed(TRAIN_DS_PATH, **vt, x_mean_std=x_mean_std)\n",
    "    np.savez_compressed(EVAL_DS_PATH, **ve, x_mean_std=x_mean_std)\n",
    "    print(f\"Datasets saved to {TRAIN_DS_PATH} and {EVAL_DS_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving datasets: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a .mat demo file for testing\n",
    "print(f\"Saving demo file to {DS_DIR}/demo.mat and {DS_DIR}/demo.npz\")\n",
    "rand_idxs = np.random.randint(0, len(vt[PHYS]), 100)  # random indices for the demo\n",
    "demo = {PHYS:vt[PHYS][rand_idxs], PTS:vt[PTS][rand_idxs], FX:vt[FX][rand_idxs], IY:vt[IY][rand_idxs], BR:vt[BR][rand_idxs], BZ:vt[BZ][rand_idxs], SEP:vt[SEP][rand_idxs]}\n",
    "try:\n",
    "    savemat(join(DS_DIR, 'demo.mat'), demo)\n",
    "    np.savez_compressed(join(DS_DIR, 'demo.npz'), **demo)\n",
    "except Exception as e:\n",
    "    print(f\"Error saving demo file: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some examples\n",
    "print(\"Testing the dataset...\")\n",
    "tds, eds = np.load(TRAIN_DS_PATH), np.load(EVAL_DS_PATH)\n",
    "print(f'train_ds: {tds.keys()}')\n",
    "print(f'eval_ds: {eds.keys()}')\n",
    "# plot some examples\n",
    "n_plot = 3 if LOCAL else 15\n",
    "for ds, ds_name in zip([tds, eds], ['Train', 'Eval']):\n",
    "    for i in range(n_plot):\n",
    "        r, z = ds[PTS][i, :, 0], ds[PTS][i, :, 1]  # get the points\n",
    "        x = (ds[PHYS][i]-ds['x_mean_std'][0]) / ds['x_mean_std'][1]  # normalize the physical inputs\n",
    "        plt.figure(figsize=(15, 4))\n",
    "        plt.subplot(1, 5, 1)\n",
    "        plt.scatter(r, z, c=ds[FX][i], s=4), plt.title('FX')\n",
    "        plt.plot(ds[SEP][i, :NLCFS], ds[SEP][i, NLCFS:], 'gray', lw=2)\n",
    "        plot_vessel(), plt.axis('equal'), plt.axis('off'), plt.colorbar()\n",
    "        plt.subplot(1, 5, 2)\n",
    "        plt.scatter(r, z, c=ds[IY][i], s=4), plt.title('IY')\n",
    "        plt.plot(ds[SEP][i, :NLCFS], ds[SEP][i, NLCFS:], 'gray', lw=2)\n",
    "        plot_vessel(), plt.axis('equal'), plt.axis('off'), plt.colorbar()\n",
    "        plt.subplot(1, 5, 3)\n",
    "        plt.scatter(r, z, c=ds[BR][i], s=4), plt.title('BR')\n",
    "        plt.plot(ds[SEP][i, :NLCFS], ds[SEP][i, NLCFS:], 'gray', lw=2)\n",
    "        plot_vessel(), plt.axis('equal'), plt.axis('off'), plt.colorbar()\n",
    "        plt.subplot(1, 5, 4)\n",
    "        plt.scatter(r, z, c=ds[BZ][i], s=4), plt.title('BZ')\n",
    "        plt.plot(ds[SEP][i, :NLCFS], ds[SEP][i, NLCFS:], 'gray', lw=2)\n",
    "        plot_vessel(), plt.axis('equal'), plt.axis('off'), plt.colorbar()\n",
    "        plt.subplot(1, 5, 5)\n",
    "        plt.bar(np.arange(len(x)), x), plt.title('Physical Inputs')\n",
    "        plt.suptitle(f'SHOT {i} - {ds_name} Example')\n",
    "        plt.tight_layout()\n",
    "        plt.show() if LOCAL else plt.savefig(join(DS_DIR, 'imgs', f'{ds_name.lower()}_example_{i}.png'))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done! Space used:')\n",
    "os.system(f'du -h {TRAIN_DS_PATH}')\n",
    "os.system(f'du -h {EVAL_DS_PATH}')\n",
    "assert os.path.exists(TRAIN_DS_PATH), f\"Dataset not saved: {TRAIN_DS_PATH}\"\n",
    "assert os.path.exists(EVAL_DS_PATH), f\"Dataset not saved: {EVAL_DS_PATH}\"\n",
    "print(f\"{JOBID} done\")\n",
    "if not LOCAL: sleep(30) # wait for files to update (for cluster)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
