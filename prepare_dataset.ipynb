{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training dataset from the equilibria dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" # disable GPU\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import io\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "from time import time\n",
    "from matplotlib import cm\n",
    "from os.path import join, exists\n",
    "from utils import resample_on_new_subgrid, interp_fun, get_box_from_grid, sample_random_subgrid, calc_laplace_df_dr_ker, INTERP_METHOD\n",
    "print(\"Preparing data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "HAS_SCREEN = False # whether to plot the data or save it\n",
    "DTYPE = 'float32'\n",
    "DATA_DIR = \"data\" # where the data is stored\n",
    "full_ds_mat_path = join(DATA_DIR, 'ITER_like_equilibrium_dataset.mat')\n",
    "sample_ds_mat_path = join(DATA_DIR, 'ITER_like_equilibrium_dataset_sample.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download datasets from gdrive, # uncomment if you want to download the dataset\n",
    "# import gdown\n",
    "# if not exists(DATA_DIR): os.makedirs(DATA_DIR)\n",
    "# gdown.download(id=\"1-5KP7_OYIvDD_QXvIr5sDihVxZx1qJCN\", output=sample_ds_mat_path, quiet=False)\n",
    "# gdown.download(id=\"1Gn_OrMzxPRkTk-i77--HiWmWZyd8i8ue\", output=full_ds_mat_path, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all files and dirs in the data directory except for full_ds_mat_path and sample_ds_mat_path\n",
    "if not exists(DATA_DIR): os.makedirs(DATA_DIR) # create data directory \n",
    "file_white_list = [full_ds_mat_path, sample_ds_mat_path]\n",
    "extension_white_list = ['.mat']\n",
    "for file_or_dir in os.listdir(DATA_DIR):\n",
    "    fn = os.path.join(DATA_DIR, file_or_dir)\n",
    "    to_be_removed = fn not in file_white_list or not any(fn.endswith(ext) for ext in extension_white_list)\n",
    "    print(f\"{'XXXX' if to_be_removed else 'KEEP    '} {fn}\")\n",
    "    # if os.path.isfile(fn) and fn not in [full_ds_mat_path, sample_ds_mat_path]: os.remove(fn)\n",
    "    if os.path.isfile(fn) and to_be_removed: os.remove(fn)\n",
    "    elif os.path.isdir(fn): \n",
    "        for root, dirs, files in os.walk(fn, topdown=False):\n",
    "            for name in files: os.remove(os.path.join(root, name))\n",
    "            for name in dirs: os.rmdir(os.path.join(root, name))\n",
    "        os.rmdir(fn)\n",
    "    else: print(f\"Skipping {fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from mat file\n",
    "mat_ds = io.loadmat(full_ds_mat_path)\n",
    "DB_psi_pixel_test_ConvNet = mat_ds['DB_psi_pixel_test_ConvNet'].astype(DTYPE)\n",
    "DB_meas_Bpickup_test_ConvNet = mat_ds['DB_meas_Bpickup_test_ConvNet'].astype(DTYPE)\n",
    "DB_coils_curr_test_ConvNet = mat_ds['DB_coils_curr_test_ConvNet'].astype(DTYPE)\n",
    "DB_p_test_ConvNet = mat_ds['DB_p_test_ConvNet'].astype(DTYPE)\n",
    "DB_f_test_ConvNet = mat_ds['DB_f_test_ConvNet'].astype(DTYPE)\n",
    "RRf = mat_ds['RR_pixels'].astype(DTYPE) # radial coordinate f=Full grid\n",
    "ZZf = mat_ds['ZZ_pixels'].astype(DTYPE) # vertical coordinate f=Full grid\n",
    "DB_res_RHS_pixel_test_ConvNet = mat_ds['DB_res_RHS_pixel_test_ConvNet'].astype(DTYPE)\n",
    "DB_separatrix_200_test_ConvNet = mat_ds['DB_separatrix_200_test_ConvNet'].astype(DTYPE)\n",
    "DB_Jpla_pixel_test_ConvNet = mat_ds['DB_Jpla_pixel_test_ConvNet'].astype(DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataset input and output\n",
    "X_currs = DB_coils_curr_test_ConvNet\n",
    "X_magnetic = DB_meas_Bpickup_test_ConvNet\n",
    "X_p_profiles = DB_p_test_ConvNet\n",
    "X_f_profiles = DB_f_test_ConvNet\n",
    "X = [X_currs, X_magnetic, X_p_profiles, X_f_profiles]\n",
    "assert len(X_currs) == len(X_magnetic) == len(X_p_profiles) == len(X_f_profiles)\n",
    "Y = DB_psi_pixel_test_ConvNet\n",
    "RHS = DB_res_RHS_pixel_test_ConvNet\n",
    "N_ORIG = len(X_currs) # number of samples in the original dataset\n",
    "print(f\"Original dataset size: {N_ORIG}\")\n",
    "# Save RRf, ZZf\n",
    "nr,nz = RRf.shape\n",
    "io.savemat(join(DATA_DIR, f'start_grid_{nr}x{nz}.mat'),{'RRf':RRf,'ZZf':ZZf})   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot dataset example\n",
    "cmap = cm.inferno\n",
    "for i in range(0,1):\n",
    "    ind_plot = np.random.randint(0,N_ORIG,1)[0]\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(15, 3), sharey=True)\n",
    "    ax0 = axs[0].contour(RRf,ZZf,DB_psi_pixel_test_ConvNet[ind_plot,:,:],15)\n",
    "    fig.colorbar(ax0)\n",
    "    axs[0].plot(DB_separatrix_200_test_ConvNet[ind_plot,:,0],DB_separatrix_200_test_ConvNet[ind_plot,:,1],c='g')\n",
    "    axs[0].axis('equal')\n",
    "    axs[0].set_xlabel('r [m]')\n",
    "    axs[0].set_ylabel('z [m]')\n",
    "    axs[0].set_title('Ψ [Wb] - equil. #{}'.format(ind_plot))\n",
    "    img = axs[1].contourf(RRf,ZZf,DB_Jpla_pixel_test_ConvNet[ind_plot,:,:],15)\n",
    "    fig.colorbar(img)\n",
    "    axs[1].axis('equal')\n",
    "    axs[1].set_title('$J_Ψ$ [A/m2] - equil. #{}'.format(ind_plot))\n",
    "    axs[1].set_xlabel('r [m]')\n",
    "    axs[1].set_ylabel('z [m]')\n",
    "    axs[1].plot(DB_separatrix_200_test_ConvNet[ind_plot,:,0],DB_separatrix_200_test_ConvNet[ind_plot,:,1],c='g')\n",
    "\n",
    "    rm, rM, zm, zM = RRf.min(), RRf.max(), ZZf.min(), ZZf.max()\n",
    "    img = axs[2].contourf(RRf,ZZf,DB_psi_pixel_test_ConvNet[ind_plot,:,:],15,cmap=cmap)\n",
    "    axs[2].plot( DB_separatrix_200_test_ConvNet[ind_plot,:,0], DB_separatrix_200_test_ConvNet[ind_plot,:,1], c='g')\n",
    "    axs[2].set_xlim([rm,rM])\n",
    "    axs[2].set_ylim([zm,zM])\n",
    "    axs[2].axis('equal')\n",
    "    axs[2].set_axis_off()\n",
    "    axs[2].set_title('Ψ')\n",
    "    qq = DB_res_RHS_pixel_test_ConvNet[ind_plot,:,:]\n",
    "    img = axs[3].contourf(RRf,ZZf,qq,15,cmap=cmap)\n",
    "    axs[3].set_title('GS operator')\n",
    "    axs[3].axis('equal')\n",
    "    axs[3].set_xlim([rm,rM])\n",
    "    axs[3].set_ylim([zm,zM])\n",
    "    axs[3].axis('equal')\n",
    "    axs[3].plot(DB_separatrix_200_test_ConvNet[ind_plot,:,0], DB_separatrix_200_test_ConvNet[ind_plot,:,1], c='g')\n",
    "    axs[3].set_axis_off()\n",
    "plt.show() if HAS_SCREEN else plt.savefig(join(DATA_DIR, 'dataset_example.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation on subgrids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test interpolation\n",
    "idx = np.random.randint(0, Y.shape[0], 1)[0]\n",
    "f, rhs = Y[idx,:,:], DB_res_RHS_pixel_test_ConvNet[idx,:,:]\n",
    "rrg, zzg = sample_random_subgrid(RRf,ZZf)\n",
    "box = get_box_from_grid(rrg, zzg)\n",
    "f_grid = interp_fun(Y[idx,:,:], RRf, ZZf, rrg, zzg)\n",
    "rhs_grid = interp_fun(rhs, RRf, ZZf, rrg, zzg)\n",
    "\n",
    "fig,ax = plt.subplots(1,5, figsize=(20,5))\n",
    "ax[0].scatter(RRf, ZZf, marker='.')\n",
    "ax[0].scatter(rrg, zzg, marker='.')\n",
    "ax[0].set_aspect('equal')\n",
    "\n",
    "im1 = ax[1].contourf(RRf, ZZf, f, 20)\n",
    "ax[1].plot(box[:,0],box[:,1])\n",
    "ax[1].set_aspect('equal')\n",
    "\n",
    "im2 = ax[2].contourf(rrg, zzg, f_grid, 20)\n",
    "ax[2].set_aspect('equal')\n",
    "\n",
    "im3 = ax[3].contourf(RRf, ZZf, rhs, 20)\n",
    "ax[3].set_aspect('equal')\n",
    "ax[3].plot(box[:,0],box[:,1])\n",
    "\n",
    "im4 = ax[4].contourf(rrg, zzg, rhs_grid, 20)\n",
    "ax[4].set_aspect('equal')\n",
    "\n",
    "plt.colorbar(im1,ax=ax[1])\n",
    "plt.colorbar(im2,ax=ax[2])\n",
    "plt.colorbar(im3,ax=ax[3])\n",
    "plt.colorbar(im4,ax=ax[4])\n",
    "\n",
    "plt.show() if HAS_SCREEN else plt.savefig(join(DATA_DIR, 'interpolation_example.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "N_SAMPLES = 50_000 #100_000 # number of samples to use for training\n",
    "TRAIN_EVAL_SPLIT = 0.8 # percentage of the dataset to use for training\n",
    "FULL_SUBGRID_SPLIT = 1 #0.25 # percentage of the full grid \n",
    "N_GRID = 64 # number of points in the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset splitting (N_TOP = original dataset size)\n",
    "NT = int(N_SAMPLES*TRAIN_EVAL_SPLIT)    # training\n",
    "NE = N_SAMPLES - NT                     # evaluation\n",
    "NTF = int(NT*FULL_SUBGRID_SPLIT)        # training full\n",
    "NTS = NT - NTF                          # training subgrid\n",
    "NEF = int(NE*FULL_SUBGRID_SPLIT)        # evaluation full\n",
    "NES = NE - NEF                          # evaluation subgrid \n",
    "assert NTF+NTS == NT\n",
    "assert NEF+NES == NE\n",
    "assert NTF + NTS + NEF + NES == N_SAMPLES\n",
    "print(f\"Training: {NT}, full: {NTF}, subgrid: {NTS}\")\n",
    "print(f\"Eval:     {NE}, full: {NEF}, subgrid: {NES}\")\n",
    "orig_idxs = np.random.permutation(N_ORIG)\n",
    "orig_idxs_train = orig_idxs[:int(N_ORIG*TRAIN_EVAL_SPLIT)] # original indices for training\n",
    "orig_idxs_eval = orig_idxs[int(N_ORIG*TRAIN_EVAL_SPLIT):] # original indices for evaluation\n",
    "# splitting the idxs\n",
    "assert len(orig_idxs_train) > NTF, f\"Training set is too small, {len(orig_idxs_train)} < {NTF}\"\n",
    "idxs_tf = np.random.choice(orig_idxs_train, NTF, replace=False) # training full\n",
    "assert len(orig_idxs_train) > NTS, f\"Training set is too small, {len(orig_idxs_train)} < {NTS}\"\n",
    "idxs_ts = np.random.choice(orig_idxs_train, NTS, replace=False) # can overlap with idxs_tf\n",
    "assert len(orig_idxs_eval) > NEF, f\"Evaluation set is too small, {len(orig_idxs_eval)} < {NEF}\"\n",
    "idxs_ef = np.random.choice(orig_idxs_eval, NEF, replace=False) # evaluation full\n",
    "assert len(orig_idxs_eval) > NES, f\"Evaluation set is too small, {len(orig_idxs_eval)} < {NES}\"\n",
    "idxs_es = np.random.choice(orig_idxs_eval, NES, replace=False) # can overlap with idxs_ef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create arrays to store the dataset\n",
    "x_tf = [np.zeros((NTF, *x[0].shape), dtype=DTYPE) for x in X]\n",
    "y_tf = np.zeros((NTF, N_GRID, N_GRID), dtype=DTYPE)\n",
    "rr_tf = np.zeros((NTF, N_GRID, N_GRID), dtype=DTYPE)\n",
    "zz_tf = np.zeros((NTF, N_GRID, N_GRID), dtype=DTYPE)\n",
    "rhs_tf = np.zeros((NTF, N_GRID, N_GRID), dtype=DTYPE)\n",
    "\n",
    "x_ts = [np.zeros((NTS, *x[0].shape), dtype=DTYPE) for x in X]\n",
    "y_ts = np.zeros((NTS, N_GRID, N_GRID), dtype=DTYPE)\n",
    "rr_ts = np.zeros((NTS, N_GRID, N_GRID), dtype=DTYPE)\n",
    "zz_ts = np.zeros((NTS, N_GRID, N_GRID), dtype=DTYPE)\n",
    "rhs_ts = np.zeros((NTS, N_GRID, N_GRID), dtype=DTYPE)\n",
    "\n",
    "x_ef = [np.zeros((NEF, *x[0].shape), dtype=DTYPE) for x in X]\n",
    "y_ef = np.zeros((NEF, N_GRID, N_GRID), dtype=DTYPE)\n",
    "rr_ef = np.zeros((NEF, N_GRID, N_GRID), dtype=DTYPE)\n",
    "zz_ef = np.zeros((NEF, N_GRID, N_GRID), dtype=DTYPE)\n",
    "rhs_ef = np.zeros((NEF, N_GRID, N_GRID), dtype=DTYPE)\n",
    "\n",
    "x_es = [np.zeros((NES, *x[0].shape), dtype=DTYPE) for x in X]\n",
    "y_es = np.zeros((NES, N_GRID, N_GRID), dtype=DTYPE)\n",
    "rr_es = np.zeros((NES, N_GRID, N_GRID), dtype=DTYPE)\n",
    "zz_es = np.zeros((NES, N_GRID, N_GRID), dtype=DTYPE)\n",
    "rhs_es = np.zeros((NES, N_GRID, N_GRID), dtype=DTYPE)\n",
    "\n",
    "## fill the arrays\n",
    "# Train Full -> just copy the data\n",
    "for ix, x in enumerate(X): x_tf[ix][:] = x[idxs_tf]\n",
    "y_tf[:], rhs_tf[:] = Y[idxs_tf], RHS[idxs_tf]\n",
    "rr_tf[:], zz_tf[:] = RRf, ZZf\n",
    "# Train Subgrid -> interpolate the data\n",
    "for ix, x in enumerate(X): x_ts[ix][:] = x[idxs_ts]\n",
    "start_time = time()\n",
    "for i, idx in enumerate(idxs_ts):\n",
    "    (yi, rhsi), rri, zzi = resample_on_new_subgrid([Y[idx], RHS[idx]], RRf, ZZf, N_GRID, N_GRID)\n",
    "    y_ts[i], rhs_ts[i], rr_ts[i], zz_ts[i] = yi, rhsi, rri, zzi\n",
    "    if (i+1) % 1000 == 0: print(f\"Train Subgrid: {i+1}/{NTS}, eta: {((time()-start_time)/(i+1)*(NTS-i-1))/60:.1f} min\", flush=True)\n",
    "# Eval Full -> just copy the data\n",
    "for ix, x in enumerate(X): x_ef[ix][:] = x[idxs_ef]\n",
    "y_ef[:], rhs_ef[:] = Y[idxs_ef], RHS[idxs_ef]\n",
    "rr_ef[:], zz_ef[:] = RRf, ZZf\n",
    "# Eval Subgrid -> interpolate the data\n",
    "for ix, x in enumerate(X): x_es[ix][:] = x[idxs_es]\n",
    "start_time = time()\n",
    "for i, idx in enumerate(idxs_es):\n",
    "    (yi, rhsi), rri, zzi = resample_on_new_subgrid([Y[idx], RHS[idx]], RRf, ZZf, N_GRID, N_GRID)\n",
    "    y_es[i], rhs_es[i], rr_es[i], zz_es[i] = yi, rhsi, rri, zzi\n",
    "    if (i+1) % 1000 == 0: print(f\"Eval Subgrid: {i+1}/{NES}, eta: {((time()-start_time)/(i+1)*(NES-i-1))/60:.1f} min\", flush=True)\n",
    "\n",
    "# concatenate the arrays\n",
    "X_train = [np.concatenate((x_tfi, x_tsi)) for x_tfi, x_tsi in zip(x_tf, x_ts)]\n",
    "y_train = np.concatenate((y_tf, y_ts))\n",
    "rr_train = np.concatenate((rr_tf, rr_ts))\n",
    "zz_train = np.concatenate((zz_tf, zz_ts))\n",
    "rhs_train = np.concatenate((rhs_tf, rhs_ts))\n",
    "del x_tf, y_tf, rr_tf, zz_tf, rhs_tf, x_ts, y_ts, rr_ts, zz_ts, rhs_ts\n",
    "\n",
    "X_eval = [np.concatenate((x_efi, x_esi)) for x_efi, x_esi in zip(x_ef, x_es)]\n",
    "y_eval = np.concatenate((y_ef, y_es))\n",
    "rr_eval = np.concatenate((rr_ef, rr_es))\n",
    "zz_eval = np.concatenate((zz_ef, zz_es))\n",
    "rhs_eval = np.concatenate((rhs_ef, rhs_es))\n",
    "del x_ef, y_ef, rr_ef, zz_ef, rhs_ef, x_es, y_es, rr_es, zz_es, rhs_es\n",
    "print(f\"x_train: [{[x.shape for x in X_train]}], y_train: {y_train.shape}, rr_train: {rr_train.shape}, zz_train: {zz_train.shape}, rhs_train: {rhs_train.shape}\")\n",
    "print(f\"x_eval: [{[x.shape for x in X_eval]}], y_eval: {y_eval.shape}, rr_eval: {rr_eval.shape}, zz_eval: {zz_eval.shape}, rhs_eval: {rhs_eval.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate kernels for Grad-Shafranov equation # NOTE: DEPRECATED\n",
    "# so we don't have to do it during training\n",
    "laplace_ker_t = np.zeros((len(X_train[0]), 3, 3), dtype=DTYPE)\n",
    "laplace_ker_e = np.zeros((len(X_eval[0]), 3, 3), dtype=DTYPE)\n",
    "df_dr_ker_t = np.zeros((len(X_train[0]), 3, 3), dtype=DTYPE)\n",
    "df_dr_ker_e = np.zeros((len(X_eval[0]), 3, 3), dtype=DTYPE)\n",
    "hrs_t, hzs_t = rr_train[:,1,2]-rr_train[:,1,1], zz_train[:,2,1]-zz_train[:,1,1]\n",
    "hrs_e, hzs_e = rr_eval[:,1,2]-rr_eval[:,1,1], zz_eval[:,2,1]-zz_eval[:,1,1]\n",
    "for i in range(len(X_train[0])):\n",
    "    laplace_ker_t[i,:,:], df_dr_ker_t[i,:,:] = calc_laplace_df_dr_ker(hrs_t[i], hzs_t[i])\n",
    "for i in range(len(X_eval[0])):\n",
    "    laplace_ker_e[i,:,:], df_dr_ker_e[i,:,:] = calc_laplace_df_dr_ker(hrs_e[i], hzs_e[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dataset\n",
    "rows = 3\n",
    "idxs_train = np.random.randint(0, len(X_train[0]), rows)\n",
    "idxs_eval = np.random.randint(0, len(X_eval[0]), rows)\n",
    "fig,ax = plt.subplots(rows,6, figsize=(15,3*rows))\n",
    "box0 = get_box_from_grid(RRf, ZZf)\n",
    "for i, (it, ie)  in enumerate(zip(idxs_train, idxs_eval)):\n",
    "    # training\n",
    "    boxi = get_box_from_grid(rr_train[it], zz_train[it])\n",
    "    ax[i,0].plot(box0[:,0], box0[:,1])\n",
    "    ax[i,0].plot(boxi[:,0], boxi[:,1])\n",
    "    ax[i,0].set_aspect('equal')\n",
    "    ax[i,0].set_title(f\"Train {it}\")\n",
    "    a1 = ax[i,1].contourf(rr_train[it], zz_train[it], y_train[it], 20)\n",
    "    ax[i,1].plot(box0[:,0], box0[:,1])\n",
    "    ax[i,1].set_aspect('equal')\n",
    "    plt.colorbar(a1,ax=ax[i,1])\n",
    "    a2 = ax[i,2].contourf(rr_train[it], zz_train[it] ,-rhs_train[it], 20)\n",
    "    ax[i,2].plot(box0[:,0], box0[:,1])\n",
    "    ax[i,2].set_aspect('equal')\n",
    "    plt.colorbar(a2,ax=ax[i,2])\n",
    "    # evaluation\n",
    "    boxi = get_box_from_grid(rr_eval[ie], zz_eval[ie])\n",
    "    ax[i,3].plot(box0[:,0], box0[:,1])\n",
    "    ax[i,3].plot(boxi[:,0], boxi[:,1])\n",
    "    ax[i,3].set_aspect('equal')\n",
    "    ax[i,3].set_title(f\"Eval {ie}\")\n",
    "    a1 = ax[i,4].contourf(rr_eval[ie], zz_eval[ie], y_eval[ie], 20)\n",
    "    ax[i,4].plot(box0[:,0], box0[:,1])\n",
    "    ax[i,4].set_aspect('equal')\n",
    "    plt.colorbar(a1,ax=ax[i,4])\n",
    "    a2 = ax[i,5].contourf(rr_eval[ie], zz_eval[ie] ,-rhs_eval[ie], 20)\n",
    "    ax[i,5].plot(box0[:,0], box0[:,1])\n",
    "    ax[i,5].set_aspect('equal')\n",
    "    plt.colorbar(a2,ax=ax[i,5])\n",
    "plt.show() if HAS_SCREEN else plt.savefig(join(DATA_DIR, 'dataset_check.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Grad-Shafranov Operator with convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import calc_gso, calc_gso_batch\n",
    "import torch\n",
    "n_plots = 7\n",
    "idxs = np.random.randint(0, len(X_train[0]), n_plots)\n",
    "psis, rhss = y_train[idxs], rhs_train[idxs]\n",
    "rrs, zzs = rr_train[idxs], zz_train[idxs]\n",
    "big_box = get_box_from_grid(RRf, ZZf)\n",
    "#batched version\n",
    "psist = torch.tensor(psis, dtype=torch.float32).view(n_plots, 1, N_GRID, N_GRID)\n",
    "rrst = torch.tensor(rrs, dtype=torch.float32).view(n_plots, 1, N_GRID, N_GRID)\n",
    "zzst = torch.tensor(zzs, dtype=torch.float32).view(n_plots, 1, N_GRID, N_GRID)\n",
    "print(f\"psi: {psist.shape}, rr: {rrst.shape}, zz: {zzst.shape}\")\n",
    "gsos = calc_gso_batch(psist, rrst, zzst)\n",
    "print(f\"gsos: {gsos.shape}\")\n",
    "gsos = gsos.view(n_plots, N_GRID, N_GRID).numpy()\n",
    "# single version\n",
    "for i in range(n_plots):\n",
    "    psi, rr, zz, rhs = psis[i], rrs[i], zzs[i], rhss[i]\n",
    "    box = get_box_from_grid(rr, zz)\n",
    "    gso = calc_gso(psi, rr, zz) # calculate the Grad-Shafranov operator\n",
    "    gso2 = gsos[i]\n",
    "    #plot error gso vs gso2\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "    im = ax.contourf(rr, zz, np.abs(gso-gso2), 20)\n",
    "    ax.plot(big_box[:,0], big_box[:,1])\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(f\"Error batch/no batch {i}\")\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    plt.show() if HAS_SCREEN else plt.savefig(join(DATA_DIR, f'gso_error_{i}.png'))\n",
    "    # NOTE: the error between the batched and non-batched version can be non-zero due to different\n",
    "    # implementations in gpu\n",
    "    print(f\"max error batch/no batch: {np.abs(gso-gso2).max()}\")\n",
    "    # assert np.allclose(gso, gso2, rtol=1e-2), f\"Error in the calculation of the Grad-Shafranov operator: \\ngso:\\n{gso}, \\ngso2:\\n{gso2}\"\n",
    "    # psi, gso, rhs = psi[1:-1,1:-1], gso[1:-1,1:-1], rhs[1:-1,1:-1]\n",
    "    # rr, zz = rr[1:-1,1:-1], zz[1:-1,1:-1] \n",
    "    fig,ax = plt.subplots(1,5, figsize=(20,5))\n",
    "    ax[0].plot(big_box[:,0], big_box[:,1])\n",
    "    ax[0].plot(box[:,0], box[:,1])\n",
    "    ax[0].set_aspect('equal')\n",
    "    ax[0].set_xticks([]), ax[0].set_yticks([])\n",
    "    ax[0].set_title(f\"Train {idxs}\")\n",
    "    im1 = ax[1].contourf(rr, zz, psi, 20)\n",
    "    ax[1].plot(big_box[:,0], big_box[:,1])\n",
    "    ax[1].set_aspect('equal')\n",
    "    ax[1].set_xticks([]), ax[1].set_yticks([])\n",
    "    ax[1].set_title(\"Ψ\")\n",
    "    im2 = ax[2].contourf(rr, zz, -gso, 20)\n",
    "    ax[2].plot(big_box[:,0], big_box[:,1])\n",
    "    ax[2].set_aspect('equal')\n",
    "    ax[2].set_xticks([]), ax[2].set_yticks([])\n",
    "    ax[2].set_title(\"GSO recalculated\")\n",
    "    im3 = ax[3].contourf(rr, zz, -rhs, 20)\n",
    "    ax[3].plot(big_box[:,0], big_box[:,1])\n",
    "    ax[3].set_aspect('equal')\n",
    "    ax[3].set_xticks([]), ax[3].set_yticks([])\n",
    "    ax[3].set_title(\"GSO from dataset\")\n",
    "    im4 = ax[4].contourf(rr, zz, np.abs(gso-rhs), 20)\n",
    "    ax[4].plot(big_box[:,0], big_box[:,1])\n",
    "    ax[4].set_aspect('equal')\n",
    "    ax[4].set_xticks([]), ax[4].set_yticks([])\n",
    "    ax[4].set_title(\"Absolute error\")\n",
    "    plt.colorbar(im1,ax=ax[1])\n",
    "    plt.colorbar(im2,ax=ax[2])\n",
    "    plt.colorbar(im3,ax=ax[3])\n",
    "    plt.colorbar(im4,ax=ax[4])\n",
    "    plt.show() if HAS_SCREEN else plt.savefig(join(DATA_DIR, f'gso_check_{i}.png'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export dataset Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the datasets as mat files\n",
    "# hyperparameters\n",
    "N_SAMPLES = 10_000 # number of samples to use for training\n",
    "TRAIN_EVAL_SPLIT = 0.8 # percentage of the dataset to use for training\n",
    "FULL_SUBGRID_SPLIT = 1 #0.25 # percentage of the full grid \n",
    "N_GRID = 64 # number of points in the grid\n",
    "\n",
    "io.savemat(join(DATA_DIR, f'train_ds_{N_SAMPLES}_{TRAIN_EVAL_SPLIT*100:.0f}_{FULL_SUBGRID_SPLIT*100:.0f}.mat'), {'currs':X_train[0], 'magnetic':X_train[1], 'p_profiles':X_train[2], 'f_profiles':X_train[3], 'psi':y_train, 'rr':rr_train, 'zz':zz_train})\n",
    "io.savemat(join(DATA_DIR, f'eval_ds_{N_SAMPLES}_{TRAIN_EVAL_SPLIT*100:.0f}_{FULL_SUBGRID_SPLIT*100:.0f}.mat'), {'currs':X_eval[0], 'magnetic':X_eval[1], 'p_profiles':X_eval[2], 'f_profiles':X_eval[3], 'psi':y_eval, 'rr':rr_eval, 'zz':zz_eval})\n",
    "print(\"Pytorch (.mat) dataset saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit(0) # exit here to avoid saving tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export dataset Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "X_train_tf = np.column_stack((X_train[0], X_train[1]))\n",
    "X_eval_tf = np.column_stack((X_eval[0], X_eval[1]))\n",
    "train_ds_tf = tf.data.Dataset.from_tensor_slices((X_train_tf, y_train, rhs_train[:,1:-1,1:-1], rr_train, zz_train, laplace_ker_t, df_dr_ker_t))\n",
    "eval_ds_tf = tf.data.Dataset.from_tensor_slices((X_eval_tf, y_eval, rhs_eval[:,1:-1,1:-1], rr_eval, zz_eval, laplace_ker_e, df_dr_ker_e))\n",
    "# save the datasets\n",
    "tf.data.Dataset.save(train_ds_tf, join(DATA_DIR, 'train_ds_tf'))\n",
    "tf.data.Dataset.save(eval_ds_tf, join(DATA_DIR, 'eval_ds_tf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test loading the datasets\n",
    "del train_ds_tf, eval_ds_tf\n",
    "train_ds_tf = tf.data.Dataset.load(join(DATA_DIR, 'train_ds_tf'))\n",
    "eval_ds_tf = tf.data.Dataset.load(join(DATA_DIR, 'eval_ds_tf'))\n",
    "#shuffle and batch the datasets\n",
    "train_ds_tf = train_ds_tf.shuffle(len(X_train_tf))\n",
    "eval_ds_tf = eval_ds_tf.shuffle(len(X_eval_tf))\n",
    "print(train_ds_tf.element_spec)\n",
    "print(eval_ds_tf.element_spec)\n",
    "print(f\"Training dataset size: {len(train_ds_tf)}\")\n",
    "print(f\"Eval dataset size: {len(eval_ds_tf)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
