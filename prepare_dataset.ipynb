{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training dataset from the equilibria dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" # disable GPU\n",
    "import numpy as np\n",
    "from scipy.io import loadmat, savemat\n",
    "from time import time\n",
    "from os.path import join, exists\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "print(\"Preparing data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "# HAS_SCREEN = True # prototypying\n",
    "HAS_SCREEN = False # for cluster\n",
    "DTYPE = 'float32'\n",
    "# TAR_GZ_FILE = 'dss/ds1.tar.gz' # the tar.gz file containing the data\n",
    "# TAR_GZ_FILE = 'dss/ds3.tar.gz' # the tar.gz file containing the data\n",
    "TAR_GZ_FILE = 'dss/ds5.tar.gz' # the tar.gz file containing the data\n",
    "TMP_DIR = 'dss/tmp' # where the data will be stored\n",
    "DATA_DIR = 'dss/ds' # where the data will be stored\n",
    "\n",
    "# hyperparameters\n",
    "N_SAMPLES = 200_000 #100_000 # number of samples to use for training\n",
    "TRAIN_EVAL_SPLIT = 0.8 # percentage of the dataset to use for training\n",
    "FULL_SUBGRID_SPLIT = 0.0 #0.25 # percentage of the full grid \n",
    "assert FULL_SUBGRID_SPLIT == 0.0, \"Full subgrid split must be zero\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract the tar.gz file into the tmp directory\n",
    "# if exists(TMP_DIR):\n",
    "#     print(f\"Removing {TMP_DIR}...\")\n",
    "#     os.system(f\"rm -rf {TMP_DIR}\")\n",
    "# os.mkdir(TMP_DIR)\n",
    "# os.makedirs(DATA_DIR, exist_ok=True)\n",
    "# print(f\"Extracting {TAR_GZ_FILE} into {TMP_DIR}...\")\n",
    "# os.system(f\"tar -xzf {TAR_GZ_FILE} -C {TMP_DIR}\")\n",
    "# print(f\"Extracted {TAR_GZ_FILE} into {TMP_DIR}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the grid coordinates\n",
    "d = loadmat('tcv_params/grid.mat')\n",
    "RD, ZD = d['r'].flatten(), d['z'].flatten() # original grid coordinates (DATA)\n",
    "r0, r1 = RD[0], RD[-1]  # min and max r\n",
    "z0, z1 = ZD[0], ZD[-1]  # min and max z\n",
    "r,z = np.linspace(r0, r1, N_GRID_R), np.linspace(z0, z1, N_GRID_Z)  # grid coordinates\n",
    "\n",
    "RRD, ZZD = np.meshgrid(RD, ZD)  # meshgrid for the original grid coordinates (from the data)\n",
    "print(f'RRD shape: {RRD.shape}, ZZD shape: {ZZD.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "print(\"Loading data...\")\n",
    "\n",
    "# list all the files inside TMP_DIR/ds\n",
    "files = sorted([f for f in os.listdir(f'{TMP_DIR}/ds') if f.endswith('.mat')])\n",
    "print(f'Found {len(files)} files.')\n",
    "Fxs, Iys, Ias, Bms, Ufs = [], [], [], [], []\n",
    "files_iter = tqdm(files, desc=\"Loading files\", unit=\"file\") if HAS_SCREEN else files\n",
    "# files_iter = files\n",
    "for f in files_iter:\n",
    "    try:\n",
    "    # if True:\n",
    "        d = loadmat(join(TMP_DIR, 'ds', f))\n",
    "        # print(f'file: {f}, keys: {d.keys()}') #  'Bm', 'Fx', 'Ia', 'Ip', 'Iy', 'Uf', 't'\n",
    "        t, Ip = d['t'].flatten(), d['Ip'].flatten()  # time and plasma current\n",
    "        sip = np.sign(np.mean(Ip)) # sign of the plasma current\n",
    "        Fx = d['Fx']  # flux map\n",
    "        Iy = d['Iy']  # current density map\n",
    "        Ia = d['Ia']  # coil currents\n",
    "        Bm = d['Bm']  # magnetic probe measurements\n",
    "        Uf = d['Uf']  # flux loop poloidal flux\n",
    "\n",
    "        nt = t.shape[0]  # number of time points\n",
    "        assert Fx.shape == (28, 65, nt), f'Fx shape mismatch: {Fx.shape} != (65, 28, {nt})'\n",
    "        assert Iy.shape == (28, 65, nt), f'Iy shape mismatch: {Iy.shape} != (65, 28, {nt})'\n",
    "        assert Ia.shape == (19, nt), f'Ia shape mismatch: {Ia.shape} != (19, {nt})'\n",
    "        assert Bm.shape == (38, nt), f'Bm shape mismatch: {Bm.shape} != (38, {nt})'\n",
    "        assert Uf.shape == (38, nt), f'Uf shape mismatch: {Uf.shape} != (38, {nt})'\n",
    "\n",
    "        # check none of the values are NaN\n",
    "        assert not np.isnan(Fx).any(), f'Fx contains NaN values: {f}'\n",
    "        assert not np.isnan(Iy).any(), f'Iy contains NaN values: {f}'\n",
    "        assert not np.isnan(Ia).any(), f'Ia contains NaN values: {f}'\n",
    "        assert not np.isnan(Bm).any(), f'Bm contains NaN values: {f}'\n",
    "        assert not np.isnan(Uf).any(), f'Uf contains NaN values: {f}'\n",
    "        # check the values are finite\n",
    "        assert np.isfinite(Fx).all(), f'Fx contains infinite values: {f}'\n",
    "        assert np.isfinite(Iy).all(), f'Iy contains infinite values: {f}'\n",
    "        assert np.isfinite(Ia).all(), f'Ia contains infinite values: {f}'\n",
    "        assert np.isfinite(Bm).all(), f'Bm contains infinite values: {f}'\n",
    "        assert np.isfinite(Uf).all(), f'Uf contains infinite values: {f}'\n",
    "\n",
    "        ## Pad the current density map\n",
    "        Iy_padded = np.zeros_like(Fx)  # padded current density map\n",
    "        Iy_padded[1:-1, 1:-1, :] = Iy  # pad the current density map\n",
    "        # 4 sides\n",
    "        Iy_padded[0, 1:-1, :] = Iy[0, :, :]  # top\n",
    "        Iy_padded[-1, 1:-1, :] = Iy[-1, :, :]  # bottom\n",
    "        Iy_padded[1:-1, 0, :] = Iy[:, 0, :]  # left\n",
    "        Iy_padded[1:-1, -1, :] = Iy[:, -1, :]  # right\n",
    "        # 4 corners\n",
    "        Iy_padded[0, 0, :] = Iy[0, 0, :]  # top left\n",
    "        Iy_padded[0, -1, :] = Iy[0, -1, :]  # top right\n",
    "        Iy_padded[-1, 0, :] = Iy[-1, 0, :]  # bottom left\n",
    "        Iy_padded[-1, -1, :] = Iy[-1, -1, :]  # bottom righ\n",
    "        Iy = Iy_padded  # use the padded current density map\n",
    "    \n",
    "        Fxs.append(Fx), Iys.append(Iy), Ias.append(Ia), Bms.append(Bm), Ufs.append(Uf)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error loading {f}: {e}')\n",
    "        continue\n",
    "\n",
    "print(f'Loaded {len(Fxs)} files.')\n",
    "assert len(Fxs) > 0, f'No samples: {len(Fxs)}'\n",
    "\n",
    "# convert to numpy arrays\n",
    "Fx = np.concatenate(Fxs, axis=-1).astype(DTYPE).transpose(2,0,1)  # flux map\n",
    "Iy = np.concatenate(Iys, axis=-1).astype(DTYPE).transpose(2,0,1)  # current density map\n",
    "Ia = np.concatenate(Ias, axis=-1).astype(DTYPE).transpose(1,0)  # coil currents\n",
    "Bm = np.concatenate(Bms, axis=-1).astype(DTYPE).transpose(1,0)  # magnetic probe measurements\n",
    "Uf = np.concatenate(Ufs, axis=-1).astype(DTYPE).transpose(1,0)  # flux loop poloidal flux\n",
    "\n",
    "assert Fx.shape[0] > 0, f'No samples: {Fx.shape}'\n",
    "\n",
    "N = Fx.shape[0]  # number of samples\n",
    "print(f'Loaded {N} samples.')\n",
    "\n",
    "# assign to standard values\n",
    "X = [Ia, Bm, Uf]  # inputs\n",
    "Y = Fx # outputs\n",
    "RHS = Iy  # right hand side\n",
    "\n",
    "# remove the tmp directory\n",
    "print(f\"Removing {TMP_DIR}...\")\n",
    "os.system(f\"rm -rf {TMP_DIR}\")\n",
    "print(\"Data loaded.\")\n",
    "# check the shapes\n",
    "print(f'Fx shape: {Fx.shape}, Iy shape: {Iy.shape}, Ia shape: {Ia.shape}, Bm shape: {Bm.shape}, Uf shape: {Uf.shape}')\n",
    "    \n",
    "# print sizes in MB\n",
    "print(f'Fx size: {Fx.nbytes / 1024**2:.2f} MB, Iy size: {Iy.nbytes / 1024**2:.2f} MB, Ia size: {Ia.nbytes / 1024**2:.2f} MB, Bm size: {Bm.nbytes / 1024**2:.2f} MB, Uf size: {Uf.nbytes / 1024**2:.2f} MB')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some examples\n",
    "n_plot = 3\n",
    "rand_idxs = np.random.randint(0, N, n_plot)\n",
    "for i, ri in enumerate(rand_idxs):\n",
    "    plt.figure(figsize=(16, 3))\n",
    "    plt.subplot(1, 5, 1)\n",
    "    plt.contourf(RRD, ZZD, Fx[ri], levels=20)\n",
    "    plt.axis('equal'), plt.axis('off')\n",
    "    plt.title('Fx')\n",
    "    plt.colorbar()\n",
    "    plt.subplot(1, 5, 2)\n",
    "    plt.contourf(RRD, ZZD, Iy[ri], levels=20)\n",
    "    plt.axis('equal'), plt.axis('off')\n",
    "    plt.title('Iy')\n",
    "    plt.colorbar()\n",
    "    plt.subplot(1, 5, 3)\n",
    "    plt.bar(np.arange(Ia.shape[1]), Ia[ri])\n",
    "    plt.title('Ia')\n",
    "    plt.subplot(1, 5, 4)\n",
    "    plt.bar(np.arange(Bm.shape[1]), Bm[ri])\n",
    "    plt.title('Bm')\n",
    "    plt.subplot(1, 5, 5)\n",
    "    plt.bar(np.arange(Uf.shape[1]), Uf[ri])\n",
    "    plt.title('Uf')\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'SHOT {ri}')\n",
    "    plt.show() if HAS_SCREEN else plt.savefig(f'{DATA_DIR}/example_{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test interpolation\n",
    "idx = np.random.randint(0, N)\n",
    "f, rhs = Fx[idx,:,:], Iy[idx,:,:]\n",
    "rrg, zzg = sample_random_subgrid(RRD,ZZD, N_GRID_Z, N_GRID_R)\n",
    "print(f.shape, rhs.shape, rrg.shape, zzg.shape)\n",
    "box = get_box_from_grid(rrg, zzg)\n",
    "f_grid = interp_fun(Fx[idx,:,:], RRD, ZZD, rrg, zzg)\n",
    "rhs_grid = interp_fun(rhs, RRD, ZZD, rrg, zzg)\n",
    "\n",
    "fig,ax = plt.subplots(1,5, figsize=(20,5))\n",
    "ax[0].scatter(RRD, ZZD, marker='.')\n",
    "ax[0].scatter(rrg, zzg, marker='.')\n",
    "ax[0].set_aspect('equal')\n",
    "\n",
    "im1 = ax[1].contourf(RRD, ZZD, f, 20)\n",
    "ax[1].plot(box[:,0],box[:,1])\n",
    "ax[1].set_aspect('equal')\n",
    "\n",
    "im2 = ax[2].contourf(rrg, zzg, f_grid, 20)\n",
    "ax[2].set_aspect('equal')\n",
    "\n",
    "im3 = ax[3].contourf(RRD, ZZD, rhs, 20)\n",
    "ax[3].set_aspect('equal')\n",
    "ax[3].plot(box[:,0],box[:,1])\n",
    "\n",
    "im4 = ax[4].contourf(rrg, zzg, rhs_grid, 20)\n",
    "ax[4].set_aspect('equal')\n",
    "\n",
    "plt.colorbar(im1,ax=ax[1])\n",
    "plt.colorbar(im2,ax=ax[2])\n",
    "plt.colorbar(im3,ax=ax[3])\n",
    "plt.colorbar(im4,ax=ax[4])\n",
    "\n",
    "plt.show() if HAS_SCREEN else plt.savefig(join(DATA_DIR, 'interpolation_example.png'))\n",
    "\n",
    "plt.figure(figsize=(5,10))\n",
    "rrg = rrg.reshape(-1)\n",
    "zzg = zzg.reshape(-1)\n",
    "plt.plot(rrg, zzg)\n",
    "plt.scatter(rrg, zzg, c=np.linspace(0,1,N_GRID_R*N_GRID_Z), cmap='inferno')\n",
    "plt.scatter(rrg[0], zzg[0], c='r', marker='o')\n",
    "plt.scatter(rrg[-1], zzg[-1], c='b', marker='o')\n",
    "plt.axis('equal')\n",
    "plt.title('Subgrid')\n",
    "plt.show() if HAS_SCREEN else plt.savefig(join(DATA_DIR, 'subgrid_example.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset splitting (N_TOP = original dataset size)\n",
    "NT = int(N_SAMPLES*TRAIN_EVAL_SPLIT)    # training\n",
    "NE = N_SAMPLES - NT                     # evaluation\n",
    "NTF = int(NT*FULL_SUBGRID_SPLIT)        # training full\n",
    "NTS = NT - NTF                          # training subgrid\n",
    "NEF = int(NE*FULL_SUBGRID_SPLIT)        # evaluation full\n",
    "NES = NE - NEF                          # evaluation subgrid \n",
    "assert NTF+NTS == NT\n",
    "assert NEF+NES == NE\n",
    "assert NTF + NTS + NEF + NES == N_SAMPLES\n",
    "print(f\"Training: {NT}, full: {NTF}, subgrid: {NTS}\")\n",
    "print(f\"Eval:     {NE}, full: {NEF}, subgrid: {NES}\")\n",
    "orig_idxs = np.random.permutation(N)\n",
    "orig_idxs_train = orig_idxs[:int(N*TRAIN_EVAL_SPLIT)] # original indices for training\n",
    "orig_idxs_eval = orig_idxs[int(N*TRAIN_EVAL_SPLIT):] # original indices for evaluation\n",
    "# splitting the idxs\n",
    "assert len(orig_idxs_train) > NTF, f\"Training set is too small, {len(orig_idxs_train)} < {NTF}\"\n",
    "idxs_tf = np.random.choice(orig_idxs_train, NTF, replace=False) # training full\n",
    "assert len(orig_idxs_train) > NTS, f\"Training set is too small, {len(orig_idxs_train)} < {NTS}\"\n",
    "idxs_ts = np.random.choice(orig_idxs_train, NTS, replace=False) # can overlap with idxs_tf\n",
    "assert len(orig_idxs_eval) > NEF, f\"Evaluation set is too small, {len(orig_idxs_eval)} < {NEF}\"\n",
    "idxs_ef = np.random.choice(orig_idxs_eval, NEF, replace=False) # evaluation full\n",
    "assert len(orig_idxs_eval) > NES, f\"Evaluation set is too small, {len(orig_idxs_eval)} < {NES}\"\n",
    "idxs_es = np.random.choice(orig_idxs_eval, NES, replace=False) # can overlap with idxs_ef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create arrays to store the dataset\n",
    "# x_tf = [np.zeros((NTF, *x[0].shape), dtype=DTYPE) for x in X]\n",
    "# y_tf = np.zeros((NTF, N_GRID_Z, N_GRID_R), dtype=DTYPE)\n",
    "# rr_tf = np.zeros((NTF, N_GRID_Z, N_GRID_R), dtype=DTYPE)\n",
    "# zz_tf = np.zeros((NTF, N_GRID_Z, N_GRID_R), dtype=DTYPE)\n",
    "# rhs_tf = np.zeros((NTF, N_GRID_Z, N_GRID_R), dtype=DTYPE)\n",
    "\n",
    "x_ts = [np.zeros((NTS, *x[0].shape), dtype=DTYPE) for x in X]\n",
    "y_ts = np.zeros((NTS, N_GRID_Z, N_GRID_R), dtype=DTYPE)\n",
    "rr_ts = np.zeros((NTS, N_GRID_Z, N_GRID_R), dtype=DTYPE)\n",
    "zz_ts = np.zeros((NTS, N_GRID_Z, N_GRID_R), dtype=DTYPE)\n",
    "rhs_ts = np.zeros((NTS, N_GRID_Z, N_GRID_R), dtype=DTYPE)\n",
    "\n",
    "# x_ef = [np.zeros((NEF, *x[0].shape), dtype=DTYPE) for x in X]\n",
    "# y_ef = np.zeros((NEF, N_GRID_Z, N_GRID_R), dtype=DTYPE)\n",
    "# rr_ef = np.zeros((NEF, N_GRID_Z, N_GRID_R), dtype=DTYPE)\n",
    "# zz_ef = np.zeros((NEF, N_GRID_Z, N_GRID_R), dtype=DTYPE)\n",
    "# rhs_ef = np.zeros((NEF, N_GRID_Z, N_GRID_R), dtype=DTYPE)\n",
    "\n",
    "x_es = [np.zeros((NES, *x[0].shape), dtype=DTYPE) for x in X]\n",
    "y_es = np.zeros((NES, N_GRID_Z, N_GRID_R), dtype=DTYPE)\n",
    "rr_es = np.zeros((NES, N_GRID_Z, N_GRID_R), dtype=DTYPE)\n",
    "zz_es = np.zeros((NES, N_GRID_Z, N_GRID_R), dtype=DTYPE)\n",
    "rhs_es = np.zeros((NES, N_GRID_Z, N_GRID_R), dtype=DTYPE)\n",
    "\n",
    "## fill the arrays\n",
    "# # Train Full -> just copy the data\n",
    "# for ix, x in enumerate(X): x_tf[ix][:] = x[idxs_tf]\n",
    "# y_tf[:], rhs_tf[:] = Y[idxs_tf], RHS[idxs_tf]\n",
    "# rr_tf[:], zz_tf[:] = RRD, ZZD\n",
    "# Train Subgrid -> interpolate the data\n",
    "for ix, x in enumerate(X): x_ts[ix][:] = x[idxs_ts]\n",
    "start_time = time()\n",
    "for i, idx in enumerate(idxs_ts):\n",
    "    (yi, rhsi), rri, zzi = resample_on_new_subgrid([Y[idx], RHS[idx]], RRD, ZZD, N_GRID_Z, N_GRID_R)\n",
    "    y_ts[i], rhs_ts[i], rr_ts[i], zz_ts[i] = yi, rhsi, rri, zzi\n",
    "    if (i+1) % 1000 == 0: print(f\"Train Subgrid: {i+1}/{NTS}, eta: {((time()-start_time)/(i+1)*(NTS-i-1))/60:.1f} min\", flush=True)\n",
    "# # Eval Full -> just copy the data\n",
    "# for ix, x in enumerate(X): x_ef[ix][:] = x[idxs_ef]\n",
    "# y_ef[:], rhs_ef[:] = Y[idxs_ef], RHS[idxs_ef]\n",
    "# rr_ef[:], zz_ef[:] = RRD, ZZD\n",
    "# Eval Subgrid -> interpolate the data\n",
    "for ix, x in enumerate(X): x_es[ix][:] = x[idxs_es]\n",
    "start_time = time()\n",
    "for i, idx in enumerate(idxs_es):\n",
    "    (yi, rhsi), rri, zzi = resample_on_new_subgrid([Y[idx], RHS[idx]], RRD, ZZD, N_GRID_Z, N_GRID_R)\n",
    "    y_es[i], rhs_es[i], rr_es[i], zz_es[i] = yi, rhsi, rri, zzi\n",
    "    if (i+1) % 1000 == 0: print(f\"Eval Subgrid: {i+1}/{NES}, eta: {((time()-start_time)/(i+1)*(NES-i-1))/60:.1f} min\", flush=True)\n",
    "\n",
    "# concatenate the arrays\n",
    "X_train = x_ts\n",
    "y_train = y_ts\n",
    "rr_train = rr_ts\n",
    "zz_train = zz_ts\n",
    "rhs_train = rhs_ts\n",
    "del x_ts, y_ts, rr_ts, zz_ts, rhs_ts\n",
    "\n",
    "X_eval = x_es\n",
    "y_eval = y_es\n",
    "rr_eval = rr_es\n",
    "zz_eval = zz_es\n",
    "rhs_eval = rhs_es\n",
    "del x_es, y_es, rr_es, zz_es, rhs_es\n",
    "print(f\"x_train: [{[x.shape for x in X_train]}], y_train: {y_train.shape}, rr_train: {rr_train.shape}, zz_train: {zz_train.shape}, rhs_train: {rhs_train.shape}\")\n",
    "print(f\"x_eval: [{[x.shape for x in X_eval]}], y_eval: {y_eval.shape}, rr_eval: {rr_eval.shape}, zz_eval: {zz_eval.shape}, rhs_eval: {rhs_eval.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate kernels for Grad-Shafranov equation\n",
    "# so we don't have to do it during training\n",
    "laplace_ker_t = np.zeros((len(X_train[0]), 3, 3), dtype=DTYPE)\n",
    "laplace_ker_e = np.zeros((len(X_eval[0]), 3, 3), dtype=DTYPE)\n",
    "df_dr_ker_t = np.zeros((len(X_train[0]), 3, 3), dtype=DTYPE)\n",
    "df_dr_ker_e = np.zeros((len(X_eval[0]), 3, 3), dtype=DTYPE)\n",
    "hrs_t, hzs_t = rr_train[:,1,2]-rr_train[:,1,1], zz_train[:,2,1]-zz_train[:,1,1]\n",
    "hrs_e, hzs_e = rr_eval[:,1,2]-rr_eval[:,1,1], zz_eval[:,2,1]-zz_eval[:,1,1]\n",
    "for i in range(len(X_train[0])):\n",
    "    try:\n",
    "        laplace_ker_t[i,:,:], df_dr_ker_t[i,:,:] = calc_laplace_df_dr_ker(hrs_t[i], hzs_t[i])\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating laplace_ker_t for index {i}: {e}\")\n",
    "        # plot rr_train[i], zz_train[i]\n",
    "        plt.figure()\n",
    "        plt.scatter(rr_train[i], zz_train[i], marker='.')\n",
    "        plt.title(f\"rr_train[{i}]\")\n",
    "        plt.axis('equal')\n",
    "        plt.show() if HAS_SCREEN else plt.savefig(f'{DATA_DIR}/rr_train_{i}.png')\n",
    "        # plot rr_eval[i], zz_eval[i]\n",
    "        break\n",
    "\n",
    "for i in range(len(X_eval[0])):\n",
    "    laplace_ker_e[i,:,:], df_dr_ker_e[i,:,:] = calc_laplace_df_dr_ker(hrs_e[i], hzs_e[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dataset\n",
    "rows = 5\n",
    "idxs_train = np.random.randint(0, len(X_train[0]), rows)\n",
    "idxs_eval = np.random.randint(0, len(X_eval[0]), rows)\n",
    "fig,ax = plt.subplots(rows,6, figsize=(15,3*rows))\n",
    "box0 = get_box_from_grid(RRD, ZZD)\n",
    "for i, (it, ie)  in enumerate(zip(idxs_train, idxs_eval)):\n",
    "    # training\n",
    "    boxi = get_box_from_grid(rr_train[it], zz_train[it])\n",
    "    ax[i,0].plot(box0[:,0], box0[:,1])\n",
    "    ax[i,0].plot(boxi[:,0], boxi[:,1])\n",
    "    ax[i,0].set_aspect('equal')\n",
    "    ax[i,0].set_title(f\"Train {it}\")\n",
    "    a1 = ax[i,1].contourf(rr_train[it], zz_train[it], y_train[it], 20)\n",
    "    ax[i,1].plot(box0[:,0], box0[:,1])\n",
    "    ax[i,1].set_aspect('equal')\n",
    "    plt.colorbar(a1,ax=ax[i,1])\n",
    "    a2 = ax[i,2].contourf(rr_train[it], zz_train[it] ,-rhs_train[it], 20)\n",
    "    ax[i,2].plot(box0[:,0], box0[:,1])\n",
    "    ax[i,2].set_aspect('equal')\n",
    "    plt.colorbar(a2,ax=ax[i,2])\n",
    "    # evaluation\n",
    "    boxi = get_box_from_grid(rr_eval[ie], zz_eval[ie])\n",
    "    ax[i,3].plot(box0[:,0], box0[:,1])\n",
    "    ax[i,3].plot(boxi[:,0], boxi[:,1])\n",
    "    ax[i,3].set_aspect('equal')\n",
    "    ax[i,3].set_title(f\"Eval {ie}\")\n",
    "    a1 = ax[i,4].contourf(rr_eval[ie], zz_eval[ie], y_eval[ie], 20)\n",
    "    ax[i,4].plot(box0[:,0], box0[:,1])\n",
    "    ax[i,4].set_aspect('equal')\n",
    "    plt.colorbar(a1,ax=ax[i,4])\n",
    "    a2 = ax[i,5].contourf(rr_eval[ie], zz_eval[ie] ,-rhs_eval[ie], 20)\n",
    "    ax[i,5].plot(box0[:,0], box0[:,1])\n",
    "    ax[i,5].set_aspect('equal')\n",
    "    plt.colorbar(a2,ax=ax[i,5])\n",
    "plt.show() if HAS_SCREEN else plt.savefig(join(DATA_DIR, 'dataset_check.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import calc_gso, calc_gso_batch\n",
    "import torch\n",
    "n_plots = 7\n",
    "idxs = np.random.randint(0, len(X_train[0]), n_plots)\n",
    "psis, rhss = y_train[idxs], rhs_train[idxs]\n",
    "rrs, zzs = rr_train[idxs], zz_train[idxs]\n",
    "big_box = get_box_from_grid(RRD, ZZD)\n",
    "#batched version\n",
    "psist = torch.tensor(psis, dtype=torch.float32).view(n_plots, 1, N_GRID_Z, N_GRID_R)\n",
    "rrst = torch.tensor(rrs, dtype=torch.float32).view(n_plots, 1, N_GRID_Z, N_GRID_R)\n",
    "zzst = torch.tensor(zzs, dtype=torch.float32).view(n_plots, 1, N_GRID_Z, N_GRID_R)\n",
    "print(f'psi: {psist.shape}, rr: {rrst.shape}, zz: {zzst.shape}')\n",
    "gsos = calc_gso_batch(psist, rrst, zzst)\n",
    "print(f'gsos: {gsos.shape}')\n",
    "gsos = gsos.view(n_plots, N_GRID_Z, N_GRID_R).numpy()\n",
    "# single version\n",
    "for i in range(n_plots):\n",
    "    psi, rr, zz, rhs = psis[i], rrs[i], zzs[i], rhss[i]\n",
    "    box = get_box_from_grid(rr, zz)\n",
    "    gso = calc_gso(psi, rr, zz) # calculate the Grad-Shafranov operator\n",
    "    gso2 = gsos[i]\n",
    "    #plot error gso vs gso2\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "    im = ax.contourf(rr, zz, np.abs(gso-gso2), 20)\n",
    "    ax.plot(big_box[:,0], big_box[:,1])\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(f\"Error batch/no batch {i}\")\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    plt.show() if HAS_SCREEN else plt.savefig(join(DATA_DIR, f'gso_error_{i}.png'))\n",
    "    # NOTE: the error between the batched and non-batched version can be non-zero due to different\n",
    "    # implementations in gpu\n",
    "    print(f\"max error batch/no batch: {np.abs(gso-gso2).max()}\")\n",
    "    # assert np.allclose(gso, gso2, rtol=1e-2), f\"Error in the calculation of the Grad-Shafranov operator: \\ngso:\\n{gso}, \\ngso2:\\n{gso2}\"\n",
    "    # psi, gso, rhs = psi[1:-1,1:-1], gso[1:-1,1:-1], rhs[1:-1,1:-1]\n",
    "    # rr, zz = rr[1:-1,1:-1], zz[1:-1,1:-1] \n",
    "    fig,ax = plt.subplots(1,5, figsize=(20,5))\n",
    "    ax[0].plot(big_box[:,0], big_box[:,1])\n",
    "    ax[0].plot(box[:,0], box[:,1])\n",
    "    ax[0].set_aspect('equal')\n",
    "    ax[0].set_xticks([]), ax[0].set_yticks([])\n",
    "    ax[0].set_title(f\"Train {idxs}\")\n",
    "    im1 = ax[1].contourf(rr, zz, psi, 20)\n",
    "    ax[1].plot(big_box[:,0], big_box[:,1])\n",
    "    ax[1].set_aspect('equal')\n",
    "    ax[1].set_xticks([]), ax[1].set_yticks([])\n",
    "    ax[1].set_title(\"Ψ\")\n",
    "    im2 = ax[2].contourf(rr, zz, -gso, 20)\n",
    "    ax[2].plot(big_box[:,0], big_box[:,1])\n",
    "    ax[2].set_aspect('equal')\n",
    "    ax[2].set_xticks([]), ax[2].set_yticks([])\n",
    "    ax[2].set_title(\"GSO recalculated\")\n",
    "    im3 = ax[3].contourf(rr, zz, -rhs, 20)\n",
    "    ax[3].plot(big_box[:,0], big_box[:,1])\n",
    "    ax[3].set_aspect('equal')\n",
    "    ax[3].set_xticks([]), ax[3].set_yticks([])\n",
    "    ax[3].set_title(\"GSO from dataset\")\n",
    "    im4 = ax[4].contourf(rr, zz, np.abs(gso-rhs), 20)\n",
    "    ax[4].plot(big_box[:,0], big_box[:,1])\n",
    "    ax[4].set_aspect('equal')\n",
    "    ax[4].set_xticks([]), ax[4].set_yticks([])\n",
    "    ax[4].set_title(\"Absolute error\")\n",
    "    plt.colorbar(im1,ax=ax[1])\n",
    "    plt.colorbar(im2,ax=ax[2])\n",
    "    plt.colorbar(im3,ax=ax[3])\n",
    "    plt.colorbar(im4,ax=ax[4])\n",
    "    plt.show() if HAS_SCREEN else plt.savefig(join(DATA_DIR, f'gso_check_{i}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y_train.shape[1:] == (N_GRID_R, N_GRID_Z), f\"X_train shape mismatch: {X_train[0].shape[1:]} != ({N_GRID_R}, {N_GRID_Z})\"\n",
    "assert y_eval.shape[1:] == (N_GRID_R, N_GRID_Z), f\"X_eval shape mismatch: {X_eval[0].shape[1:]} != ({N_GRID_R}, {N_GRID_Z})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset anc calculate statistics\n",
    "curr = X_train[0] # coil currents\n",
    "mag = X_train[1] # magnetic probe measurements\n",
    "prof = X_train[2] # flux loop poloidal flux\n",
    "\n",
    "\n",
    "curr_mean_std = np.vstack([np.mean(curr, axis=0), np.std(curr, axis=0)])\n",
    "mag_mean_std = np.vstack([np.mean(mag, axis=0), np.std(mag, axis=0)])\n",
    "prof_mean_std = np.vstack([np.mean(prof, axis=0), np.std(prof, axis=0)])\n",
    "\n",
    "print(f'curr_mean_std: {curr_mean_std.shape}, mag_mean_std: {mag_mean_std.shape}, prof_mean_std: {prof_mean_std.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the datasets as mat files\n",
    "train_ds_path = join(DATA_DIR, f'train_ds_{N_SAMPLES}_{TRAIN_EVAL_SPLIT*100:.0f}_{FULL_SUBGRID_SPLIT*100:.0f}.mat')\n",
    "eval_ds_path = join(DATA_DIR, f'eval_ds_{N_SAMPLES}_{TRAIN_EVAL_SPLIT*100:.0f}_{FULL_SUBGRID_SPLIT*100:.0f}.mat')\n",
    "savemat(train_ds_path, {\n",
    "        'currs':curr, \n",
    "        'currs_mean_std':curr_mean_std,\n",
    "        'magnetic':mag, \n",
    "        'magnetic_mean_std':mag_mean_std,\n",
    "        'profiles':prof, \n",
    "        'profiles_mean_std':prof_mean_std,\n",
    "        'psi':y_train, \n",
    "        'rr':rr_train, \n",
    "        'zz':zz_train,\n",
    "        })\n",
    "savemat(eval_ds_path, {\n",
    "        'currs':X_eval[0],\n",
    "        'currs_mean_std':curr_mean_std,\n",
    "        'magnetic':X_eval[1], \n",
    "        'magnetic_mean_std':mag_mean_std,\n",
    "        'profiles':X_eval[2], \n",
    "        'profiles_mean_std':prof_mean_std,\n",
    "        'psi':y_eval, \n",
    "        'rr':rr_eval, \n",
    "        'zz':zz_eval\n",
    "        })\n",
    "print(\"Pytorch (.mat) dataset saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove this part\n",
    "os.system(f'cp {train_ds_path} dss/train_ds.mat')\n",
    "os.system(f'cp {eval_ds_path} dss/eval_ds.mat')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
