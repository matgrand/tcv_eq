{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Prepapre dataset with the prepare_dataset notebook, before running this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Module, Linear, Conv2d, MaxPool2d, BatchNorm2d, ReLU, Sequential, ConvTranspose2d\n",
    "from tqdm import tqdm\n",
    "import scipy.io as sio\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")  # adds seaborn style to charts, eg. grid\n",
    "plt.style.use(\"dark_background\")  # inverts colors to dark theme\n",
    "plt.rcParams['font.family'] = 'monospace'\n",
    "import os\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from utils import calc_gso_batch # gso/pinn calculation\n",
    "try: \n",
    "    JOBID = os.environ[\"SLURM_JOB_ID\"] # get job id from slurm, when training on cluster\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") # nvidia\n",
    "    HAS_SCREEN = False # for plotting or saving images\n",
    "except:\n",
    "    device = torch.device(\"mps\") # apple silicon\n",
    "    JOBID = \"local\"\n",
    "    HAS_SCREEN = True\n",
    "os.makedirs(f\"mg_data/{JOBID}\", exist_ok=True)\n",
    "print(f'device: {device}')\n",
    "\n",
    "# copy the python training to the directory (for cluster) (for local, it fails silently)\n",
    "os.system(f\"cp mg_train.py mg_data/{JOBID}/mg_train.py\")\n",
    "os.system(f\"cp utils.py mg_data/{JOBID}/utils.py\")\n",
    "\n",
    "def to_tensor(x, device=torch.device(\"cpu\")): return torch.tensor(x, dtype=torch.float32, device=device)\n",
    "\n",
    "SMALL, NORM, BIG = \"small\", \"norm\", \"big\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = f\"mg_data/{JOBID}\"  \n",
    "EPOCHS = 1000 # number of epochs\n",
    "BATCH_SIZE = 128 # 128 best\n",
    "\n",
    "# MODEL_NAME = SMALL\n",
    "# MODEL_NAME = NORM\n",
    "MODEL_NAME = BIG\n",
    "\n",
    "LOAD_PRETRAINED = None # Set it to None if you don't want to load pretrained model\n",
    "# LOAD_PRETRAINED = \"trained_models/pretrained_1809761.pth\" # norm model\n",
    "# LOAD_PRETRAINED = \"trained_models/pretrained_small_1810888.pth\" # small model\n",
    "# LOAD_PRETRAINED = \"trained_models/pretrained_big_1811142.pth\" # big model\n",
    "\n",
    "# LEARNING_RATE = 3e-4*np.linspace(1, 1e-2, EPOCHS)  # best\n",
    "LEARNING_RATE = 3e-4*np.logspace(0, -2, EPOCHS)\n",
    "# LEARNING_RATE = 1e-4*np.logspace(0, -2, EPOCHS)\n",
    "\n",
    "# GSO_LOSS_RATIO = np.linspace(0.4, 0.1, EPOCHS) # best\n",
    "# GSO_LOSS_RATIO = np.linspace(0.3, 0.1, EPOCHS) # best too\n",
    "# GSO_LOSS_RATIO = np.linspace(0.4, 0.0, EPOCHS) # best for big model pretrain start\n",
    "GSO_LOSS_RATIO = np.concatenate((np.linspace(0.4, 0.0, EPOCHS//2), np.linspace(0.0, 0.0, EPOCHS//2))) \n",
    "# GSO_LOSS_RATIO = 0.1*np.ones(EPOCHS) # not very good\n",
    "# GSO_LOSS_RATIO = (0.5+0.5*np.sin(np.linspace(0, 25*np.pi, EPOCHS)))*np.linspace(1, 0.1, EPOCHS) # crazy\n",
    "\n",
    "#activation functions\n",
    "\n",
    "\n",
    "USE_CURRENTS = True # usually True\n",
    "USE_PROFILES = True # false -> more realistic\n",
    "USE_MAGNETIC = True # usually True\n",
    "\n",
    "INPUT_SIZE = int(USE_CURRENTS)*14 + int(USE_PROFILES)*202 + int(USE_MAGNETIC)*187\n",
    "TRAIN_DS_PATH = \"data/train_ds.mat\" # generated from prepapre_dataset\n",
    "EVAL_DS_PATH = \"data/eval_ds.mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks\n",
    "assert USE_CURRENTS or USE_PROFILES or USE_MAGNETIC, \"At least one of the inputs should be used\"\n",
    "if LOAD_PRETRAINED is not None: assert os.path.exists(LOAD_PRETRAINED), \"Pretrained model does not exist\"\n",
    "assert os.path.exists(TRAIN_DS_PATH), \"Training dataset does not exist\"\n",
    "assert os.path.exists(EVAL_DS_PATH), \"Evaluation dataset does not exist\"\n",
    "assert os.path.exists(SAVE_DIR), \"Save directory does not exist\"\n",
    "assert len(LEARNING_RATE) == EPOCHS, \"Learning rate array length does not match epochs\"\n",
    "assert len(GSO_LOSS_RATIO) == EPOCHS, \"GSO loss ratio array length does not match epochs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best train runs: \n",
    "Note: all the metrics are on unseen, mixed (ful grid + subgrid) data. Evaluation on unseen full grid only is ~ 2x better (MSE halved).\n",
    "| ID       | NET | MSE    | GS0    | Pre | Notes |\n",
    "|----------|-----|--------|--------|-----|-------|\n",
    "| 1810019  | norm | 0.0072 | 0.0730 | / | / |\n",
    "| 1809989  | norm | 0.0082 | 0.3061 | / | / |\n",
    "| 1809986  | norm | 0.0050 | 0.0727 | / | / |\n",
    "| 1809768  | norm | 0.0030 | 0.0500 | / | / |\n",
    "| 1809761  | norm | 0.0060 | 0.0840 | / | / |\n",
    "| 1810294  | norm | 0.0026 | 0.0487 | 1809768 | pre-trained from 1809768 |\n",
    "| 1810825  | small | 0.0265 | 0.2606 | / | small model |\n",
    "| 1810888  | small | 0.0234 | 0.2236 | / | has potential for improvement |\n",
    "| 1810897  | small | 0.0388 | 0.2940 | / | start lr from 1e-4, bs 64| \n",
    "| 1810903  | norm | 0.0232 | 0.1345 | / | 0.1 const gso ratio (bad?) |\n",
    "| 1811116  | norm | 0.0141 | 0.0768 | / | 0.3 const gso ratio |\n",
    "| 1811117  | big | 0.0024 | 0.0601 | / | 0.1 const gso ratio, lr dec 3e-4 | \n",
    "| 1811142  | big | 0.0019 | 0.0535 | / | 0.1 const gso ratio, lr dec 3e-4 |\n",
    "| 1811143  | big | 0.0020 | 0.0609 | / | exact same as 1811142 |\n",
    "| 1811302 | big | 0.0063 | 0.0500 | / | gso .3 -> .1, lr dec 3e-4 |\n",
    "| 1811304 | big | 0.0013 | 0.0310 | 1811142 | gso .3 -> .1, lr dec 3e-4, first time train loss < eval |\n",
    "| 1814866 | big | 0.0012 | 0.0304 | 1811142 | gso .3 -> 0.0 \n",
    "| 1814867 | big | 0.0012 | **0.0277** | 1811142 | gso .3 -> 0.0, repeat 1814866 |\n",
    "| 1817256 | big | 0.0022 | 0.0512 | 1811142 | batch size 256 (bad) |\n",
    "| 1817333 | big | **0.0009** | 0.0333 | 1811142 | same as 1814866, but keep 0.0 from ep 500-1000 |\n",
    "| 1823444 | big | 0.0014 | 0.0579 | / | same as 1817333, but from scratch |\n",
    "| 1823543 | norm | 0.0099 | 0.9672 | / | testing best conf on norm | \n",
    "| 1823560 | big | 0.0025 | 0.1508 | / | NO PROFILES |\n",
    "| 1827440 | big | 0.0019 | 0.1501 | / | no prof, repeat 1823560 |\n",
    "| 1827437 | big | 0.0014 | 0.0516 | / | prof (a) & no prof (b) trained together see train2.ipynb, (a) metrics|\n",
    "| 1827432 | big | 0.0012 | 0.0455 | / | prof (a) & no prof (b) trained together see train2.ipynb, (a) metrics|\n",
    "| 1827441 | big | 0.1387 | 0.5608 | / | only currents (very bad performance, it needs at least magnetic measures)|\n",
    "| 1952987 | big | 0.0018 | 0.0673 | / | relu test (before testing swish) |\n",
    "| 1959172 | big | **0.0005** | 0.0454 | / | correct swish test, best so far |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot schedulers: lr + gso loss ratio\n",
    "assert MODEL_NAME in [SMALL, NORM, BIG], f\"Model name {MODEL_NAME} not found\"\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 3))\n",
    "ax[0].set_title(\"Learning Rate [Log]\")\n",
    "ax[0].plot(LEARNING_RATE, color=\"red\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Learning Rate\")\n",
    "ax[0].set_yscale(\"log\")\n",
    "ax[1].set_title(\"GSO Loss Ratio\")\n",
    "ax[1].plot(GSO_LOSS_RATIO, color=\"red\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"GSO Loss Ratio\")\n",
    "plt.tight_layout()\n",
    "plt.show() if HAS_SCREEN else plt.savefig(f\"mg_data/{JOBID}/schedulers.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Measurement    | Mean        | Standard Deviation |\n",
    "|----------------|-------------|--------------------|\n",
    "| Current        | -10183.76   | 34209.11           |\n",
    "| Magnetic       | -0.20       | 0.58               |\n",
    "| F Profile      | 33.13       | 0.28               |\n",
    "| P Profile      | 9654.42     | 8788.29            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlaNetDataset(Dataset):\n",
    "    def __init__(self, ds_mat_path):\n",
    "        ds_mat = sio.loadmat(ds_mat_path)\n",
    "        # output: magnetic flux, transposed (matlab is column-major)\n",
    "        self.psi = to_tensor(ds_mat[\"psi\"]).view(-1, 1, 64, 64)\n",
    "        # inputs: radial and vertical position of pixels (for plotting only rn) + currents + measurements + profiles \n",
    "        self.rr = to_tensor(ds_mat[\"rr\"]).view(-1,1,64,64) # radial position of pixels (64, 64)\n",
    "        self.zz = to_tensor(ds_mat[\"zz\"]).view(-1,1,64,64) # vertical position of pixels (64, 64)\n",
    "        self.currs = ds_mat[\"currs\"] # input currents (n, 14)\n",
    "        self.magnetic = ds_mat[\"magnetic\"] # input magnetic measurements (n, 187)\n",
    "        self.f_profile = ds_mat[\"f_profiles\"] # input profiles (n, 101)\n",
    "        self.p_profile = ds_mat[\"p_profiles\"] # input profiles (n, 101)\n",
    "        inputs = [] # add the normalized inputs to the list\n",
    "        if USE_CURRENTS: inputs.append((to_tensor(self.currs)+10183)/34209) # (n, 14) # normalized\n",
    "        if USE_MAGNETIC: inputs.append((to_tensor(self.magnetic)+0.2)/0.58) # (n, 187) # normalized\n",
    "        if USE_PROFILES: inputs.append(torch.cat(((to_tensor(self.f_profile)-33.13)/0.28, (to_tensor(self.p_profile)-9654)/8788), 1)) # (n, 202) # normalized\n",
    "        self.inputs = torch.cat(inputs, 1) # (n, 403)\n",
    "        #move to device (doable bc the dataset is fairly small, check memory usage)\n",
    "        self.psi, self.inputs, self.rr, self.zz = self.psi.to(device), self.inputs.to(device), self.rr.to(device), self.zz.to(device)\n",
    "        total_memory = sum([x.element_size()*x.nelement() for x in [self.psi, self.inputs, self.rr, self.zz]])\n",
    "        print(f\"Dataset: {len(self)}, memory: {total_memory/1024**2:.0f} MB\")\n",
    "    def __len__(self): return len(self.psi)\n",
    "    def __getitem__(self, idx): return self.inputs[idx], self.psi[idx], self.rr[idx], self.zz[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset\n",
    "ds = PlaNetDataset(EVAL_DS_PATH)\n",
    "print(f\"Dataset length: {len(ds)}\")\n",
    "print(f\"Input shape: {ds[0][0].shape}\")\n",
    "print(f\"Output shape: {ds[0][1].shape}\")\n",
    "n_plot = 10\n",
    "print(len(ds))\n",
    "fig, axs = plt.subplots(1, n_plot, figsize=(3*n_plot, 5))\n",
    "for i, j in enumerate(np.random.randint(0, len(ds), n_plot)):\n",
    "    psi, rr, zz = ds[j][1].cpu().numpy().squeeze(), ds[j][2].cpu().numpy().squeeze(), ds[j][3].cpu().numpy().squeeze()\n",
    "    axs[i].contourf(rr, zz, psi, 100, cmap=\"inferno\")\n",
    "    axs[i].contour(rr, zz, -psi, 20, colors=\"black\", linestyles=\"dotted\")\n",
    "    axs[i].axis(\"off\")\n",
    "    axs[i].set_aspect(\"equal\")\n",
    "plt.show() if HAS_SCREEN else plt.savefig(f\"mg_data/{JOBID}/dataset.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions\n",
    "# custom trainable swish\n",
    "class Swish(Module):\n",
    "    def __init__(self, β=1.0): \n",
    "        super(Swish, self).__init__()\n",
    "        # self.β = torch.nn.Parameter(torch.tensor(β, device=device), requires_grad=True)\n",
    "        self.β = torch.nn.Parameter(torch.tensor(β), requires_grad=True)\n",
    "    def forward(self, x): \n",
    "        return x*torch.sigmoid(self.β*x)\n",
    "    def to(self, device): \n",
    "        self.β = self.β.to(device)\n",
    "        return super().to(device)\n",
    "\n",
    "# Λ = ReLU() # ReLU activation function\n",
    "# Λ = Swish() # Swish activation function\n",
    "\n",
    "# class Λ(Module): # relu\n",
    "#     def __init__(self): super(Λ, self).__init__()\n",
    "#     def forward(self, x): return torch.relu(x)\n",
    "\n",
    "class Λ(Module): # swish\n",
    "    def __init__(self): \n",
    "        super(Λ, self).__init__()\n",
    "        self.β = torch.nn.Parameter(torch.tensor(1.0), requires_grad=True)\n",
    "    def forward(self, x): return x*torch.sigmoid(self.β*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL: PlaNet: # Paper net: branch + trunk conenction and everything \n",
    "if MODEL_NAME == NORM:\n",
    "    class PlaNet(Module): # Paper net: branch + trunk conenction and everything\n",
    "        def __init__(self):\n",
    "            super(PlaNet, self).__init__()\n",
    "            #branch\n",
    "            self.branch = Sequential(\n",
    "                Linear(INPUT_SIZE, 256), Λ(),\n",
    "                Linear(256, 128), Λ(),\n",
    "                Linear(128, 64), Λ(),\n",
    "            )\n",
    "            #trunk\n",
    "            # def trunk_block(): # faster\n",
    "            #     return  Sequential(\n",
    "            #         Conv2d(1, 8, kernel_size=3, stride=2, padding=1), BatchNorm2d(8), Λ(),\n",
    "            #         Conv2d(8, 16, kernel_size=3, stride=2, padding=1), BatchNorm2d(16), Λ(),\n",
    "            #         Conv2d(16, 32, kernel_size=3, stride=2, padding=1), BatchNorm2d(32), Λ(),\n",
    "            #     )\n",
    "            def trunk_block(): \n",
    "                return  Sequential(\n",
    "                    Conv2d(1, 8, kernel_size=3, stride=1, padding=1), BatchNorm2d(8), Λ(), MaxPool2d(2),\n",
    "                    Conv2d(8, 16, kernel_size=3, stride=1, padding=1), BatchNorm2d(16), Λ(), MaxPool2d(2),\n",
    "                    Conv2d(16, 32, kernel_size=3, stride=1, padding=1), BatchNorm2d(32), Λ(), MaxPool2d(2),\n",
    "                )\n",
    "            self.trunk_r, self.trunk_z = trunk_block(), trunk_block()\n",
    "            self.trunk_fc = Sequential(\n",
    "                Linear(2*32*8*8, 128), Λ(),\n",
    "                Linear(128, 64), Λ(),\n",
    "                Linear(64, 64), Λ(),\n",
    "            )\n",
    "            # head\n",
    "            self.fc = Sequential(Linear(64, 2048), Λ)\n",
    "            self.anti_conv = Sequential( # U-Net style\n",
    "                ConvTranspose2d(32, 32, kernel_size=2, stride=2), \n",
    "                Conv2d(32, 32, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(32, 32, kernel_size=3, padding=0), Λ(),\n",
    "                ConvTranspose2d(32, 16, kernel_size=2, stride=2),\n",
    "                Conv2d(16, 16, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(16, 16, kernel_size=3, padding=0), Λ(),\n",
    "                ConvTranspose2d(16, 8, kernel_size=2, stride=2),\n",
    "                Conv2d(8, 8, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(8, 8, kernel_size=3, padding=0), Λ(),\n",
    "                ConvTranspose2d(8, 4, kernel_size=2, stride=2),\n",
    "                Conv2d(4, 2, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(2, 1, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(1, 1, kernel_size=5, padding=0),\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            xb, r, z = x\n",
    "            #branch net\n",
    "            xb = self.branch(xb)\n",
    "            #trunk net\n",
    "            r, z = self.trunk_r(r), self.trunk_z(z) # convolutions\n",
    "            r, z = r.view(-1, 32*8*8), z.view(-1, 32*8*8) # flatten\n",
    "            xt = torch.cat((r, z), 1) # concatenate\n",
    "            xt = self.trunk_fc(xt) # fully connected\n",
    "            # multiply trunk and branch\n",
    "            x = xt * xb\n",
    "            #head net\n",
    "            x = self.fc(x)\n",
    "            x = x.view(-1, 32, 8, 8)\n",
    "            x = self.anti_conv(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL: VERY BIG PlaNet: # SAME NET, BUT MORE NEURONS\n",
    "if MODEL_NAME == BIG:\n",
    "    class PlaNet(Module): # Paper net: branch + trunk conenction and everything\n",
    "        def __init__(self):\n",
    "            super(PlaNet, self).__init__()\n",
    "            #branch\n",
    "            self.branch = Sequential(\n",
    "                Linear(INPUT_SIZE, 256), Λ(),\n",
    "                Linear(256, 128), Λ(),\n",
    "                Linear(128, 64), Λ(),\n",
    "            )\n",
    "            #trunk\n",
    "            def trunk_block(): \n",
    "                return  Sequential(\n",
    "                    Conv2d(1, 8, kernel_size=3, stride=1, padding=1), BatchNorm2d(8), Λ(), MaxPool2d(2),\n",
    "                    Conv2d(8, 16, kernel_size=3, stride=1, padding=1), BatchNorm2d(16), Λ(), MaxPool2d(2),\n",
    "                    Conv2d(16, 32, kernel_size=3, stride=1, padding=1), BatchNorm2d(32), Λ(), MaxPool2d(2),\n",
    "                )\n",
    "            self.trunk_r, self.trunk_z = trunk_block(), trunk_block()\n",
    "            self.trunk_fc = Sequential(\n",
    "                Linear(2*32*8*8, 128), Λ(),\n",
    "                Linear(128, 64), Λ(),\n",
    "                Linear(64, 64), Λ(), \n",
    "            )\n",
    "            # head\n",
    "            self.fc = Sequential(Linear(64, 4096), Λ())\n",
    "            self.anti_conv = Sequential( # U-Net style\n",
    "                ConvTranspose2d(64, 64, kernel_size=2, stride=2), \n",
    "                Conv2d(64, 64, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(64, 64, kernel_size=3, padding=0), Λ(),\n",
    "                ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n",
    "                Conv2d(32, 32, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(32, 32, kernel_size=3, padding=0), Λ(),\n",
    "                ConvTranspose2d(32, 16, kernel_size=2, stride=2),\n",
    "                Conv2d(16, 16, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(16, 16, kernel_size=3, padding=0), Λ(),\n",
    "                ConvTranspose2d(16, 8, kernel_size=2, stride=2),\n",
    "                Conv2d(8, 4, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(4, 2, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(2, 1, kernel_size=5, padding=0),\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            xb, r, z = x\n",
    "            #branch net\n",
    "            xb = self.branch(xb)\n",
    "            #trunk net\n",
    "            r, z = self.trunk_r(r), self.trunk_z(z) # convolutions\n",
    "            r, z = r.view(-1, 32*8*8), z.view(-1, 32*8*8) # flatten\n",
    "            xt = torch.cat((r, z), 1) # concatenate\n",
    "            xt = self.trunk_fc(xt) # fully connected\n",
    "            # multiply trunk and branch\n",
    "            x = xt * xb\n",
    "            #head net\n",
    "            x = self.fc(x)\n",
    "            x = x.view(-1, 64, 8, 8)\n",
    "            x = self.anti_conv(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL: SMALL PlaNet: # Paper net: branch + trunk conenction and everything BUT SMALLER\n",
    "if MODEL_NAME == SMALL:\n",
    "    class PlaNet(Module): # Paper net: branch + trunk conenction and everything\n",
    "        def __init__(self):\n",
    "            super(PlaNet, self).__init__()\n",
    "            #branch\n",
    "            self.branch = Sequential(\n",
    "                Linear(INPUT_SIZE, 128), Λ(),\n",
    "                Linear(128, 64), Λ(),\n",
    "                Linear(64, 32), Λ(),\n",
    "            )\n",
    "            def trunk_block(): \n",
    "                return  Sequential(\n",
    "                    Conv2d(1, 4, kernel_size=3, stride=1, padding=1), BatchNorm2d(4), Λ(), MaxPool2d(2),\n",
    "                    Conv2d(4, 8, kernel_size=3, stride=1, padding=1), BatchNorm2d(8), Λ(), MaxPool2d(2),\n",
    "                    Conv2d(8, 16, kernel_size=3, stride=1, padding=1), BatchNorm2d(16), Λ(), MaxPool2d(2),\n",
    "                )\n",
    "            self.trunk_r, self.trunk_z = trunk_block(), trunk_block()\n",
    "            self.trunk_fc = Sequential(\n",
    "                Linear(2*16*8*8, 64), Λ(),\n",
    "                Linear(64, 32), Λ(),\n",
    "                Linear(32, 32), Λ(),\n",
    "            )\n",
    "            # head\n",
    "            self.fc = Sequential(Linear(32, 1024), Λ)\n",
    "            self.anti_conv = Sequential( # U-Net style\n",
    "                ConvTranspose2d(16, 16, kernel_size=2, stride=2), \n",
    "                Conv2d(16, 16, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(16, 16, kernel_size=3, padding=0), Λ(),\n",
    "                ConvTranspose2d(16, 8, kernel_size=2, stride=2),\n",
    "                Conv2d(8, 8, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(8, 8, kernel_size=3, padding=0), Λ(),\n",
    "                ConvTranspose2d(8, 4, kernel_size=2, stride=2),\n",
    "                Conv2d(4, 4, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(4, 4, kernel_size=3, padding=0), Λ(),\n",
    "                ConvTranspose2d(4, 2, kernel_size=2, stride=2),\n",
    "                Conv2d(2, 2, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(2, 1, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(1, 1, kernel_size=5, padding=0),\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            xb, r, z = x\n",
    "            #branch net\n",
    "            xb = self.branch(xb)\n",
    "            #trunk net\n",
    "            r, z = self.trunk_r(r), self.trunk_z(z) # convolutions\n",
    "            r, z = r.view(-1, 16*8*8), z.view(-1, 16*8*8) # flatten\n",
    "            xt = torch.cat((r, z), 1) # concatenate\n",
    "            xt = self.trunk_fc(xt) # fully connected\n",
    "            # multiply trunk and branch\n",
    "            x = xt * xb\n",
    "            #head net\n",
    "            x = self.fc(x)\n",
    "            x = x.view(-1, 16, 8, 8)\n",
    "            x = self.anti_conv(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model inputs / outputs\n",
    "x = (torch.rand(1, INPUT_SIZE), torch.rand(1, 1, 64, 64), torch.rand(1, 1, 64, 64))\n",
    "net = PlaNet()\n",
    "y = net(x)\n",
    "print(f\"in: {[x.shape for x in x]}, out: {y.shape}\")\n",
    "n_sampl = 7\n",
    "nx = (torch.rand(n_sampl, INPUT_SIZE), torch.rand(n_sampl, 1, 64, 64), torch.rand(n_sampl, 1, 64, 64))\n",
    "ny = net(nx)\n",
    "print(f\"in: {[x.shape for x in nx]}, out: {ny.shape}\")\n",
    "assert ny.shape == (n_sampl, 1, 64, 64), f\"Wrong output shape: {ny.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_ds, val_ds = PlaNetDataset(TRAIN_DS_PATH), PlaNetDataset(EVAL_DS_PATH) # initialize datasets\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True) # initialize DataLoader\n",
    "    val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)  \n",
    "    model = PlaNet()  # instantiate model\n",
    "    if LOAD_PRETRAINED is not None: # load pretrained model\n",
    "        model.load_state_dict(torch.load(LOAD_PRETRAINED, map_location=torch.device(\"cpu\"))) # load pretrained model\n",
    "        print(f\"Pretrained model loaded: {LOAD_PRETRAINED}\")\n",
    "    model.to(device) # move model to device\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE[0])\n",
    "    loss_fn = torch.nn.MSELoss() # Mean Squared Error Loss\n",
    "    tlog_tot, tlog_mse, tlog_gso, elog_tot, elog_mse, elog_gso = [], [], [], [], [], [] # logs for losses\n",
    "    start_time = time() # start time\n",
    "    for ep in range(EPOCHS): # epochs\n",
    "        epoch_time = time()\n",
    "        for pg in optimizer.param_groups: pg['lr'] = LEARNING_RATE[ep] # update learning rate\n",
    "        model.train()\n",
    "        trainloss, evalloss = [], []\n",
    "        for input, psi, rr, zz in train_dl:\n",
    "            optimizer.zero_grad() # zero gradients\n",
    "            psi_pred = model((input, rr, zz)) # forward pass\n",
    "            gso, gso_pred = calc_gso_batch(psi, rr, zz, dev=device), calc_gso_batch(psi_pred, rr, zz, dev=device) # calculate grad shafranov\n",
    "            mse_loss = loss_fn(psi_pred, psi) # mean squared error loss on psi\n",
    "            gso_loss = loss_fn(gso_pred, gso) # PINN loss on grad shafranov\n",
    "            loss = (1-GSO_LOSS_RATIO[ep])*mse_loss + GSO_LOSS_RATIO[ep]*gso_loss # total loss\n",
    "            loss.backward() # backprop\n",
    "            optimizer.step() # update weights\n",
    "            trainloss.append((loss.item(), mse_loss.item(), gso_loss.item())) # save batch losses\n",
    "        model.eval() # evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for input, psi, rr, zz in val_dl:\n",
    "                psi_pred = model((input, rr, zz))\n",
    "                gso, gso_pred = calc_gso_batch(psi, rr, zz, dev=device), calc_gso_batch(psi_pred, rr, zz, dev=device)\n",
    "                mse_loss = loss_fn(psi_pred, psi)\n",
    "                gso_loss = loss_fn(gso_pred, gso)\n",
    "                loss = (1-GSO_LOSS_RATIO[ep])*mse_loss + GSO_LOSS_RATIO[ep]*gso_loss # total loss\n",
    "                evalloss.append((loss.item(), mse_loss.item(), gso_loss.item()))\n",
    "        tloss_tot, tloss_mse, tloss_gso = map(lambda x: sum(x)/len(x), zip(*trainloss))\n",
    "        eloss_tot, eloss_mse, eloss_gso = map(lambda x: sum(x)/len(x), zip(*evalloss))\n",
    "        # save model if improved        \n",
    "        endp = \"\\n\" \n",
    "        if eloss_tot <= min(elog_tot, default=eloss_tot): \n",
    "            torch.save(model.state_dict(), f\"{SAVE_DIR}/mg_planet_tot.pth\"); endp=\" *tot\\n\"\n",
    "        if eloss_mse <= min(elog_mse, default=eloss_mse):\n",
    "            torch.save(model.state_dict(), f\"{SAVE_DIR}/mg_planet_mse.pth\"); endp=\" *mse\\n\"\n",
    "        if eloss_gso <= min(elog_gso, default=eloss_gso):\n",
    "            torch.save(model.state_dict(), f\"{SAVE_DIR}/mg_planet_gso.pth\"); endp=\" *gso\\n\"\n",
    "        tlog_tot.append(tloss_tot); tlog_mse.append(tloss_mse); tlog_gso.append(tloss_gso)\n",
    "        elog_tot.append(eloss_tot); elog_mse.append(eloss_mse); elog_gso.append(eloss_gso) \n",
    "        print(f\"{ep+1}/{EPOCHS}: \"\n",
    "            f\"Eval: tot {eloss_tot:.4f}, mse {eloss_mse:.4f}, gso {eloss_gso:.4f} | \" + \n",
    "            f\"lr:{LEARNING_RATE[ep]:.1e}, r:{GSO_LOSS_RATIO[ep]:.2f} | {time()-epoch_time:.0f}s, eta:{(time()-start_time)*(EPOCHS-ep)/(ep+1)/60:.0f}m |\", end=endp,  flush=True)\n",
    "        if ep >= 10 and ((eloss_gso > 30.0 and GSO_LOSS_RATIO[ep] > 0.01) or eloss_mse > 11.0): return False, () # stop training, if not converging, try again\n",
    "    print(f\"Training time: {(time()-start_time)/60:.0f}mins\")\n",
    "    print(f\"Best losses: tot {min(elog_tot):.4f}, mse {min(elog_mse):.4f}, gso {min(elog_gso):.4f}\")\n",
    "    for l, n in zip([tlog_tot, tlog_mse, tlog_gso], [\"tot\", \"mse\", \"gso\"]): np.save(f\"{SAVE_DIR}/train_{n}_losses.npy\", l) # save losses\n",
    "    for l, n in zip([elog_tot, elog_mse, elog_gso], [\"tot\", \"mse\", \"gso\"]): np.save(f\"{SAVE_DIR}/eval_{n}_losses.npy\", l) # save losses\n",
    "    return True, (tlog_tot, tlog_mse, tlog_gso, elog_tot, elog_mse, elog_gso)\n",
    "\n",
    "# train the model (multiple attempts)\n",
    "for i in range(10): \n",
    "    success, logs = train()\n",
    "    if success: tlog_tot, tlog_mse, tlog_gso, elog_tot, elog_mse, elog_gso = logs; break\n",
    "    else: print(f\"Convergence failed, retrying... {i+1}/10\")\n",
    "assert success, \"Training failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "fig, ax = plt.subplots(2, 3, figsize=(12, 6))\n",
    "ce, ct = \"yellow\", \"red\"\n",
    "lw = 1.0\n",
    "ax[0,0].set_title(\"TOT Loss\")\n",
    "ax[0,0].plot(tlog_tot, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[0,0].plot(elog_tot, color=ce, label=\"eval\", linewidth=lw)\n",
    "ax[0,1].set_title(\"MSE Loss\")\n",
    "ax[0,1].plot(tlog_mse, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[0,1].plot(elog_mse, color=ce, label=\"eval\", linewidth=lw)\n",
    "ax[0,2].set_title(\"GSO Loss\")\n",
    "ax[0,2].plot(tlog_gso, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[0,2].plot(elog_gso, color=ce, label=\"eval\", linewidth=lw)\n",
    "#now the same but with log scale\n",
    "ax[1,0].set_title(\"TOT Loss (log)\")\n",
    "ax[1,0].plot(tlog_tot, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[1,0].plot(elog_tot, color=ce, label=\"eval\", linewidth=lw)\n",
    "ax[1,0].set_yscale(\"log\")\n",
    "ax[1,0].grid(True, which=\"both\", axis=\"y\")\n",
    "\n",
    "ax[1,1].set_title(\"MSE Loss (log)\")\n",
    "ax[1,1].plot(tlog_mse, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[1,1].plot(elog_mse, color=ce, label=\"eval\", linewidth=lw)\n",
    "ax[1,1].set_yscale(\"log\")\n",
    "ax[1,1].grid(True, which=\"both\", axis=\"y\")\n",
    "\n",
    "ax[1,2].set_title(\"GSO Loss (log)\")\n",
    "ax[1,2].plot(tlog_gso, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[1,2].plot(elog_gso, color=ce, label=\"eval\", linewidth=lw)\n",
    "ax[1,2].set_yscale(\"log\")\n",
    "ax[1,2].grid(True, which=\"both\", axis=\"y\")\n",
    "for a in ax.flatten(): a.legend(); a.set_xlabel(\"Epoch\"); a.set_ylabel(\"Loss\")\n",
    "plt.tight_layout()\n",
    "plt.show() if HAS_SCREEN else plt.savefig(f\"mg_data/{JOBID}/losses.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing network output\n",
    "for titl, best_model_path in zip([\"TOT\",\"MSE\", \"GSO\"], [\"mg_planet_tot.pth\", \"mg_planet_mse.pth\", \"mg_planet_gso.pth\"]):\n",
    "    model = PlaNet()\n",
    "    model.load_state_dict(torch.load(f\"{SAVE_DIR}/{best_model_path}\"))\n",
    "    model.eval()\n",
    "    ds = PlaNetDataset(EVAL_DS_PATH)\n",
    "    # ds = PlaNetDataset(TRAIN_DS_PATH)\n",
    "    os.makedirs(f\"mg_data/{JOBID}/imgs\", exist_ok=True)\n",
    "    N_PLOTS = 2 if HAS_SCREEN else 50\n",
    "    for i in np.random.randint(0, len(ds), N_PLOTS):  \n",
    "        fig, axs = plt.subplots(2, 5, figsize=(15, 9))\n",
    "        input, psi_ds, rr, zz = ds[i]\n",
    "        input, psi_ds, rr, zz = input.to('cpu'), psi_ds.to('cpu'), rr.to('cpu'), zz.to('cpu')\n",
    "        input, psi_ds, rr, zz = input.view(1,-1), psi_ds.view(1,1,64,64), rr.view(1,1,64,64), zz.view(1,1,64,64)\n",
    "        psi_pred = model((input, rr, zz))\n",
    "        gso, gso_pred = calc_gso_batch(psi_ds, rr, zz), calc_gso_batch(psi_pred, rr, zz)\n",
    "        gso, gso_pred = gso.detach().numpy().reshape(64, 64), gso_pred.detach().numpy().reshape(64, 64)\n",
    "        gso_range = (gso.max(), gso.min())\n",
    "        gso_levels = np.linspace(gso_range[1], gso_range[0], 12)\n",
    "        gso_pred = np.clip(gso_pred, gso_range[1], gso_range[0]) # clip to gso range\n",
    "        \n",
    "        psi_pred = psi_pred.detach().numpy().reshape(64, 64)\n",
    "        psi_ds = psi_ds.detach().numpy().reshape(64, 64)\n",
    "        rr, zz = rr.view(64, 64).detach().numpy(), zz.view(64, 64).detach().numpy()\n",
    "        ext = [ds.rr.min(), ds.rr.max(), ds.zz.min(), ds.zz.max()]\n",
    "        bmin, bmax = np.min([psi_ds, psi_pred]), np.max([psi_ds, psi_pred]) # min max psi\n",
    "        blevels = np.linspace(bmin, bmax, 13, endpoint=True)\n",
    "        ψ_mse = (psi_ds - psi_pred)**2\n",
    "        gso_mse = (gso - gso_pred)**2\n",
    "        mse_levels1 = np.linspace(0, 0.5, 13, endpoint=True)\n",
    "        mse_levels2 = np.linspace(0, 0.05, 13, endpoint=True)\n",
    "\n",
    "        im00 = axs[0,0].contourf(rr, zz, psi_ds, blevels, cmap=\"inferno\")\n",
    "        axs[0,0].set_title(\"Actual\")\n",
    "        axs[0,0].set_aspect('equal')\n",
    "        axs[0,0].set_ylabel(\"ψ\")\n",
    "        fig.colorbar(im00, ax=axs[0,0]) \n",
    "        im01 = axs[0,1].contourf(rr, zz, psi_pred, blevels, cmap=\"inferno\")\n",
    "        axs[0,1].set_title(\"Predicted\")\n",
    "        fig.colorbar(im01, ax=axs[0,1])\n",
    "        im02 = axs[0,2].contour(rr, zz, psi_ds, blevels, linestyles='dashed', cmap=\"inferno\")\n",
    "        axs[0,2].contour(rr, zz, psi_pred, blevels, cmap=\"inferno\")\n",
    "        axs[0,2].set_title(\"Contours\")\n",
    "        fig.colorbar(im02, ax=axs[0,2])\n",
    "        im03 = axs[0,3].contourf(rr, zz, np.clip(ψ_mse, 0, 0.5), mse_levels1, cmap=\"inferno\")\n",
    "        axs[0,3].set_title(\"MSE 0.5\")\n",
    "        fig.colorbar(im03, ax=axs[0,3])\n",
    "        im04 = axs[0,4].contourf(rr, zz, np.clip(ψ_mse, 0.00001, 0.04999), mse_levels2, cmap=\"inferno\")\n",
    "        axs[0,4].set_title(\"MSE 0.05\")\n",
    "        fig.colorbar(im04, ax=axs[0,4])\n",
    "        im10 = axs[1,0].contourf(rr, zz, gso, gso_levels, cmap=\"inferno\")\n",
    "        axs[1,0].set_ylabel(\"GSO\")\n",
    "        fig.colorbar(im10, ax=axs[1,0])\n",
    "        im6 = axs[1,1].contourf(rr, zz, gso_pred, gso_levels, cmap=\"inferno\")\n",
    "        fig.colorbar(im6, ax=axs[1,1])\n",
    "        im12 = axs[1,2].contour(rr, zz, gso, gso_levels, linestyles='dashed', cmap=\"inferno\")\n",
    "        axs[1,2].contour(rr, zz, gso_pred, gso_levels, cmap=\"inferno\")\n",
    "        fig.colorbar(im12, ax=axs[1,2])\n",
    "        im13 = axs[1,3].contourf(rr, zz, np.clip(gso_mse, 0, 0.5), mse_levels1, cmap=\"inferno\")\n",
    "        fig.colorbar(im13, ax=axs[1,3])\n",
    "        im14 = axs[1,4].contourf(rr, zz, np.clip(gso_mse, 0.00001, 0.04999), mse_levels2, cmap=\"inferno\")\n",
    "        fig.colorbar(im14, ax=axs[1,4])\n",
    "\n",
    "        for ax in axs.flatten(): ax.grid(False), ax.set_xticks([]), ax.set_yticks([]), ax.set_aspect(\"equal\")\n",
    "\n",
    "        #suptitle\n",
    "        plt.suptitle(f\"PlaNet: {titl} {i}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show() if HAS_SCREEN else plt.savefig(f\"mg_data/{JOBID}/imgs/planet_{titl}_{i}.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test inference speed\n",
    "model = PlaNet()\n",
    "model.load_state_dict(torch.load(f\"{SAVE_DIR}/{best_model_path}\"))\n",
    "model.eval()\n",
    "ds = PlaNetDataset(EVAL_DS_PATH)\n",
    "n_samples = 100\n",
    "random_idxs = np.random.choice(n_samples, len(ds))\n",
    "#cpu\n",
    "cpu_times = []\n",
    "for i in random_idxs:\n",
    "    start_t = time()\n",
    "    input, psi_ds, rr, zz = ds[i]\n",
    "    input, psi_ds, rr, zz = input.to('cpu'), psi_ds.to('cpu'), rr.to('cpu'), zz.to('cpu')\n",
    "    input, psi_ds, rr, zz = input.view(1,-1), psi_ds.view(1,1,64,64), rr.view(1,1,64,64), zz.view(1,1,64,64)\n",
    "    psi_pred = model((input, rr, zz))\n",
    "    end_t = time()\n",
    "    cpu_times.append(end_t - start_t) \n",
    "# device\n",
    "model.to(device)\n",
    "dev_times = []\n",
    "for i in random_idxs:\n",
    "    input, psi_ds, rr, zz = ds[i]\n",
    "    input, psi_ds, rr, zz = input.view(1,-1), psi_ds.view(1,1,64,64), rr.view(1,1,64,64), zz.view(1,1,64,64)\n",
    "    start_t = time()\n",
    "    psi_pred = model((input, rr, zz))\n",
    "    end_t = time()\n",
    "    dev_times.append(end_t - start_t)    \n",
    "cpu_times, dev_times = np.array(cpu_times), np.array(dev_times)\n",
    "print(f\"cpu: inference time: {cpu_times.mean():.5f}s, std: {cpu_times.std():.5f}\")\n",
    "print(f\"dev: inference time: {dev_times.mean():.5f}s, std: {dev_times.std():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\", flush=True)\n",
    "if not HAS_SCREEN: sleep(30) # wait for files to update (for cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy the log file to the folder\n",
    "os.system(f\"cp jobs/{JOBID}.txt mg_data/{JOBID}/log.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not HAS_SCREEN: exit(0) # skip this test sections on cluster\n",
    "# testing network output on full frame dataset (not subgrids)\n",
    "TESTS_DIR = \"mg_data_deicluster/1817333\"\n",
    "for titl, best_model_path in zip([\"TOT\",\"MSE\", \"GSO\"], [\"mg_planet_tot.pth\", \"mg_planet_mse.pth\", \"mg_planet_gso.pth\"]):\n",
    "    model = PlaNet()\n",
    "    model.load_state_dict(torch.load(f\"{TESTS_DIR}/{best_model_path}\", map_location=torch.device(\"cpu\")))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # ds = PlaNetDataset(\"data/train_ds_10000_80_100.mat\")\n",
    "    ds = PlaNetDataset(\"data/eval_ds_10000_80_100.mat\")\n",
    "\n",
    "    #run an evaluation on all the dataset\n",
    "    dl = DataLoader(ds, batch_size=1000, shuffle=False)\n",
    "    mses, gsos = [],[]\n",
    "    for input, psi_ds, rr, zz in dl:\n",
    "        psi_pred = model((input, rr, zz))\n",
    "        gso, gso_pred = calc_gso_batch(psi_ds, rr, zz, device), calc_gso_batch(psi_pred, rr, zz, device)\n",
    "        loss_gso = torch.nn.MSELoss()(gso, gso_pred)\n",
    "        loss_psi = torch.nn.MSELoss()(psi_ds, psi_pred)\n",
    "        mses.append(loss_psi.item())\n",
    "        gsos.append(loss_gso.item())\n",
    "    print(f\"Model {titl} MSE: {np.mean(mses):.5f}, GSO: {np.mean(gsos):.5f}\")\n",
    "\n",
    "    os.makedirs(f\"mg_data/{JOBID}/imgs\", exist_ok=True)\n",
    "    N_PLOTS = 2 if HAS_SCREEN else 50\n",
    "    model.to('cpu')\n",
    "    for i in np.random.randint(0, len(ds), N_PLOTS):  \n",
    "        fig, axs = plt.subplots(2, 5, figsize=(15, 9))\n",
    "        input, psi_ds, rr, zz = ds[i]\n",
    "        input, psi_ds, rr, zz = input.to('cpu'), psi_ds.to('cpu'), rr.to('cpu'), zz.to('cpu')\n",
    "        input, psi_ds, rr, zz = input.view(1,-1), psi_ds.view(1,1,64,64), rr.view(1,1,64,64), zz.view(1,1,64,64)\n",
    "        psi_pred = model((input, rr, zz))\n",
    "        gso, gso_pred = calc_gso_batch(psi_ds, rr, zz), calc_gso_batch(psi_pred, rr, zz)\n",
    "        gso, gso_pred = gso.detach().numpy().reshape(64, 64), gso_pred.detach().numpy().reshape(64, 64)\n",
    "        gso_range = (gso.max(), gso.min())\n",
    "        gso_levels = np.linspace(gso_range[1], gso_range[0], 12)\n",
    "        gso_pred = np.clip(gso_pred, gso_range[1], gso_range[0]) # clip to gso range\n",
    "        \n",
    "        psi_pred = psi_pred.detach().numpy().reshape(64, 64)\n",
    "        psi_ds = psi_ds.detach().numpy().reshape(64, 64)\n",
    "        rr, zz = rr.view(64, 64).detach().numpy(), zz.view(64, 64).detach().numpy()\n",
    "        ext = [ds.rr.min(), ds.rr.max(), ds.zz.min(), ds.zz.max()]\n",
    "        bmin, bmax = np.min([psi_ds, psi_pred]), np.max([psi_ds, psi_pred]) # min max psi\n",
    "        blevels = np.linspace(bmin, bmax, 13, endpoint=True)\n",
    "        ψ_mse = (psi_ds - psi_pred)**2\n",
    "        gso_mse = (gso - gso_pred)**2\n",
    "        mse_levels1 = np.linspace(0, 0.5, 13, endpoint=True)\n",
    "        mse_levels2 = np.linspace(0, 0.05, 13, endpoint=True)\n",
    "\n",
    "        im00 = axs[0,0].contourf(rr, zz, psi_ds, blevels, cmap=\"inferno\")\n",
    "        axs[0,0].set_title(\"Actual\")\n",
    "        axs[0,0].set_aspect('equal')\n",
    "        axs[0,0].set_ylabel(\"ψ\")\n",
    "        fig.colorbar(im00, ax=axs[0,0]) \n",
    "        im01 = axs[0,1].contourf(rr, zz, psi_pred, blevels, cmap=\"inferno\")\n",
    "        axs[0,1].set_title(\"Predicted\")\n",
    "        fig.colorbar(im01, ax=axs[0,1])\n",
    "        im02 = axs[0,2].contour(rr, zz, psi_ds, blevels, linestyles='dashed', cmap=\"inferno\")\n",
    "        axs[0,2].contour(rr, zz, psi_pred, blevels, cmap=\"inferno\")\n",
    "        axs[0,2].set_title(\"Contours\")\n",
    "        fig.colorbar(im02, ax=axs[0,2])\n",
    "        im03 = axs[0,3].contourf(rr, zz, np.clip(ψ_mse, 0, 0.5), mse_levels1, cmap=\"inferno\")\n",
    "        axs[0,3].set_title(\"MSE 0.5\")\n",
    "        fig.colorbar(im03, ax=axs[0,3])\n",
    "        im04 = axs[0,4].contourf(rr, zz, np.clip(ψ_mse, 0.00001, 0.04999), mse_levels2, cmap=\"inferno\")\n",
    "        axs[0,4].set_title(\"MSE 0.05\")\n",
    "        fig.colorbar(im04, ax=axs[0,4])\n",
    "        im10 = axs[1,0].contourf(rr, zz, gso, gso_levels, cmap=\"inferno\")\n",
    "        axs[1,0].set_ylabel(\"GSO\")\n",
    "        fig.colorbar(im10, ax=axs[1,0])\n",
    "        im6 = axs[1,1].contourf(rr, zz, gso_pred, gso_levels, cmap=\"inferno\")\n",
    "        fig.colorbar(im6, ax=axs[1,1])\n",
    "        im12 = axs[1,2].contour(rr, zz, gso, gso_levels, linestyles='dashed', cmap=\"inferno\")\n",
    "        axs[1,2].contour(rr, zz, gso_pred, gso_levels, cmap=\"inferno\")\n",
    "        fig.colorbar(im12, ax=axs[1,2])\n",
    "        im13 = axs[1,3].contourf(rr, zz, np.clip(gso_mse, 0, 0.5), mse_levels1, cmap=\"inferno\")\n",
    "        fig.colorbar(im13, ax=axs[1,3])\n",
    "        im14 = axs[1,4].contourf(rr, zz, np.clip(gso_mse, 0.00001, 0.04999), mse_levels2, cmap=\"inferno\")\n",
    "        fig.colorbar(im14, ax=axs[1,4])\n",
    "\n",
    "        for ax in axs.flatten(): ax.grid(False), ax.set_xticks([]), ax.set_yticks([]), ax.set_aspect(\"equal\")\n",
    "\n",
    "        #suptitle\n",
    "        plt.suptitle(f\"PlaNet: {titl} {i}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show() if HAS_SCREEN else plt.savefig(f\"mg_data/{JOBID}/imgs/planet_{titl}_{i}.png\")\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
