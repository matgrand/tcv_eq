{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Prepapre dataset with the prepare_dataset notebook, before running this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cp: cannot stat 'mg_train.py': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Module, Linear, Conv2d, MaxPool2d, BatchNorm2d, ReLU, Sequential, ConvTranspose2d\n",
    "from tqdm import tqdm\n",
    "import scipy.io as sio\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from utils import *\n",
    "try: \n",
    "    JOBID = os.environ[\"SLURM_JOB_ID\"] # get job id from slurm, when training on cluster\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") # nvidia\n",
    "    HAS_SCREEN = False # for plotting or saving images\n",
    "except:\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\") # apple silicon / cpu\n",
    "    JOBID = \"local\"\n",
    "    HAS_SCREEN = True\n",
    "os.makedirs(f\"mg_data/{JOBID}\", exist_ok=True)\n",
    "print(f'device: {device}')\n",
    "\n",
    "# copy the python training to the directory (for cluster) (for local, it fails silently)\n",
    "os.system(f\"cp mg_train.py mg_data/{JOBID}/mg_train.py\")\n",
    "os.system(f\"cp utils.py mg_data/{JOBID}/utils.py\")\n",
    "\n",
    "def to_tensor(x, device=torch.device(\"cpu\")): return torch.tensor(x, dtype=torch.float32, device=device)\n",
    "\n",
    "SMALL, NORM, BIG = \"small\", \"norm\", \"big\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = f\"mg_data/{JOBID}\"  \n",
    "EPOCHS = 1000 # number of epochs\n",
    "BATCH_SIZE = 128 # 128 best\n",
    "\n",
    "# MODEL_NAME = SMALL\n",
    "# MODEL_NAME = NORM\n",
    "MODEL_NAME = BIG\n",
    "\n",
    "LOAD_PRETRAINED = None # Set it to None if you don't want to load pretrained model\n",
    "# LOAD_PRETRAINED = \"trained_models/pretrained_1809761.pth\" # norm model\n",
    "# LOAD_PRETRAINED = \"trained_models/pretrained_small_1810888.pth\" # small model\n",
    "# LOAD_PRETRAINED = \"trained_models/pretrained_big_1811142.pth\" # big model\n",
    "\n",
    "# LEARNING_RATE = 3e-4*np.linspace(1, 1e-2, EPOCHS)  # best\n",
    "LEARNING_RATE = 3e-4*np.logspace(0, -2, EPOCHS)\n",
    "# LEARNING_RATE = 1e-4*np.logspace(0, -2, EPOCHS)\n",
    "\n",
    "# GSO_LOSS_RATIO = np.linspace(0.4, 0.1, EPOCHS) # best\n",
    "# GSO_LOSS_RATIO = np.linspace(0.3, 0.1, EPOCHS) # best too\n",
    "# GSO_LOSS_RATIO = np.linspace(0.4, 0.0, EPOCHS) # best for big model pretrain start\n",
    "GSO_LOSS_RATIO = np.concatenate((np.linspace(0.4, 0.0, EPOCHS//2), np.linspace(0.0, 0.0, EPOCHS//2))) \n",
    "# GSO_LOSS_RATIO = 0.1*np.ones(EPOCHS) # not very good\n",
    "# GSO_LOSS_RATIO = (0.5+0.5*np.sin(np.linspace(0, 25*np.pi, EPOCHS)))*np.linspace(1, 0.1, EPOCHS) # crazy\n",
    "\n",
    "#activation functions\n",
    "\n",
    "\n",
    "USE_CURRENTS = True # usually True\n",
    "USE_PROFILES = True # false -> more realistic\n",
    "USE_MAGNETIC = True # usually True\n",
    "\n",
    "INPUT_SIZE = int(USE_CURRENTS)*14 + int(USE_PROFILES)*202 + int(USE_MAGNETIC)*187\n",
    "TRAIN_DS_PATH = \"dss/train_ds.mat\" # generated from prepapre_dataset\n",
    "EVAL_DS_PATH = \"dss/eval_ds.mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks\n",
    "assert USE_CURRENTS or USE_PROFILES or USE_MAGNETIC, \"At least one of the inputs should be used\"\n",
    "if LOAD_PRETRAINED is not None: assert os.path.exists(LOAD_PRETRAINED), \"Pretrained model does not exist\"\n",
    "assert os.path.exists(TRAIN_DS_PATH), \"Training dataset does not exist\"\n",
    "assert os.path.exists(EVAL_DS_PATH), \"Evaluation dataset does not exist\"\n",
    "assert os.path.exists(SAVE_DIR), \"Save directory does not exist\"\n",
    "assert len(LEARNING_RATE) == EPOCHS, \"Learning rate array length does not match epochs\"\n",
    "assert len(GSO_LOSS_RATIO) == EPOCHS, \"GSO loss ratio array length does not match epochs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best train runs: \n",
    "Note: all the metrics are on unseen, mixed (ful grid + subgrid) data. Evaluation on unseen full grid\n",
    "only is ~ 2x better (MSE halved).\n",
    "### Old runs with ITER ds\n",
    "| ID       | NET | MSE    | GS0    | Pre | Notes |\n",
    "|----------|-----|--------|--------|-----|-------|\n",
    "| 1810019  | norm | 0.0072 | 0.0730 | / | / |\n",
    "| 1814867 | big | 0.0012 | **0.0277** | 1811142 | gso .3 -> 0.0, repeat 1814866 |\n",
    "| 1817333 | big | **0.0009** | 0.0333 | 1811142 | same as 1814866, but keep 0.0 from ep 500-1000 |\n",
    "| 1959172 | big | **0.0005** | 0.0454 | / | correct swish test, best so far |\n",
    "### New runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot schedulers: lr + gso loss ratio\n",
    "assert MODEL_NAME in [SMALL, NORM, BIG], f\"Model name {MODEL_NAME} not found\"\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 3))\n",
    "ax[0].set_title(\"Learning Rate [Log]\")\n",
    "ax[0].plot(LEARNING_RATE, color=\"red\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Learning Rate\")\n",
    "ax[0].set_yscale(\"log\")\n",
    "ax[1].set_title(\"GSO Loss Ratio\")\n",
    "ax[1].plot(GSO_LOSS_RATIO, color=\"red\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"GSO Loss Ratio\")\n",
    "plt.tight_layout()\n",
    "plt.show() if HAS_SCREEN else plt.savefig(f\"mg_data/{JOBID}/schedulers.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Measurement    | Mean        | Standard Deviation |\n",
    "|----------------|-------------|--------------------|\n",
    "| Current        | -10183.76   | 34209.11           |\n",
    "| Magnetic       | -0.20       | 0.58               |\n",
    "| F Profile      | 33.13       | 0.28               |\n",
    "| P Profile      | 9654.42     | 8788.29            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlaNetDataset(Dataset):\n",
    "    def __init__(self, ds_mat_path):\n",
    "        ds_mat = sio.loadmat(ds_mat_path)\n",
    "        # output: magnetic flux, transposed (matlab is column-major)\n",
    "        self.psi = to_tensor(ds_mat[\"psi\"]).view(-1, 1, 64, 64)\n",
    "        # inputs: radial and vertical position of pixels (for plotting only rn) + currents + measurements + profiles \n",
    "        self.rr = to_tensor(ds_mat[\"rr\"]).view(-1,1,64,64) # radial position of pixels (64, 64)\n",
    "        self.zz = to_tensor(ds_mat[\"zz\"]).view(-1,1,64,64) # vertical position of pixels (64, 64)\n",
    "        self.currs = ds_mat[\"currs\"] # input currents (n, 14)\n",
    "        self.magnetic = ds_mat[\"magnetic\"] # input magnetic measurements (n, 187)\n",
    "        self.profiles = ds_mat[\"profiles\"] # input profiles (n, 101)\n",
    "        inputs = [] # add the normalized inputs to the list\n",
    "        if USE_CURRENTS: inputs.append((to_tensor(self.currs)+10183)/34209) # (n, 14) # normalized\n",
    "        if USE_MAGNETIC: inputs.append((to_tensor(self.magnetic)+0.2)/0.58) # (n, 187) # normalized\n",
    "        if USE_PROFILES: inputs.append(torch.cat(((to_tensor(self.profiles)-33.13)/0.28, (to_tensor(self.p_profile)-9654)/8788), 1)) # (n, 202) # normalized\n",
    "        self.inputs = torch.cat(inputs, 1) # (n, 403)\n",
    "        #move to device (doable bc the dataset is fairly small, check memory usage)\n",
    "        self.psi, self.inputs, self.rr, self.zz = self.psi.to(device), self.inputs.to(device), self.rr.to(device), self.zz.to(device)\n",
    "        total_memory = sum([x.element_size()*x.nelement() for x in [self.psi, self.inputs, self.rr, self.zz]])\n",
    "        print(f\"Dataset: {len(self)}, memory: {total_memory/1024**2:.0f} MB\")\n",
    "    def __len__(self): return len(self.psi)\n",
    "    def __getitem__(self, idx): return self.inputs[idx], self.psi[idx], self.rr[idx], self.zz[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 1, 64, 64]' is invalid for input of size 364000",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# test dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m ds = \u001b[43mPlaNetDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEVAL_DS_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m].shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mPlaNetDataset.__init__\u001b[39m\u001b[34m(self, ds_mat_path)\u001b[39m\n\u001b[32m      3\u001b[39m ds_mat = sio.loadmat(ds_mat_path)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# output: magnetic flux, transposed (matlab is column-major)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28mself\u001b[39m.psi = \u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_mat\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpsi\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# inputs: radial and vertical position of pixels (for plotting only rn) + currents + measurements + profiles \u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mself\u001b[39m.rr = to_tensor(ds_mat[\u001b[33m\"\u001b[39m\u001b[33mrr\u001b[39m\u001b[33m\"\u001b[39m]).view(-\u001b[32m1\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m64\u001b[39m,\u001b[32m64\u001b[39m) \u001b[38;5;66;03m# radial position of pixels (64, 64)\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: shape '[-1, 1, 64, 64]' is invalid for input of size 364000"
     ]
    }
   ],
   "source": [
    "# test dataset\n",
    "ds = PlaNetDataset(EVAL_DS_PATH)\n",
    "print(f\"Dataset length: {len(ds)}\")\n",
    "print(f\"Input shape: {ds[0][0].shape}\")\n",
    "print(f\"Output shape: {ds[0][1].shape}\")\n",
    "n_plot = 10\n",
    "print(len(ds))\n",
    "fig, axs = plt.subplots(1, n_plot, figsize=(3*n_plot, 5))\n",
    "for i, j in enumerate(np.random.randint(0, len(ds), n_plot)):\n",
    "    psi, rr, zz = ds[j][1].cpu().numpy().squeeze(), ds[j][2].cpu().numpy().squeeze(), ds[j][3].cpu().numpy().squeeze()\n",
    "    axs[i].contourf(rr, zz, psi, 100, cmap=\"inferno\")\n",
    "    axs[i].contour(rr, zz, -psi, 20, colors=\"black\", linestyles=\"dotted\")\n",
    "    axs[i].axis(\"off\")\n",
    "    axs[i].set_aspect(\"equal\")\n",
    "plt.show() if HAS_SCREEN else plt.savefig(f\"mg_data/{JOBID}/dataset.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions\n",
    "# custom trainable swish\n",
    "class Swish(Module):\n",
    "    def __init__(self, β=1.0): \n",
    "        super(Swish, self).__init__()\n",
    "        # self.β = torch.nn.Parameter(torch.tensor(β, device=device), requires_grad=True)\n",
    "        self.β = torch.nn.Parameter(torch.tensor(β), requires_grad=True)\n",
    "    def forward(self, x): \n",
    "        return x*torch.sigmoid(self.β*x)\n",
    "    def to(self, device): \n",
    "        self.β = self.β.to(device)\n",
    "        return super().to(device)\n",
    "\n",
    "# Λ = ReLU() # ReLU activation function\n",
    "# Λ = Swish() # Swish activation function\n",
    "\n",
    "# class Λ(Module): # relu\n",
    "#     def __init__(self): super(Λ, self).__init__()\n",
    "#     def forward(self, x): return torch.relu(x)\n",
    "\n",
    "class Λ(Module): # swish\n",
    "    def __init__(self): \n",
    "        super(Λ, self).__init__()\n",
    "        self.β = torch.nn.Parameter(torch.tensor(1.0), requires_grad=True)\n",
    "    def forward(self, x): return x*torch.sigmoid(self.β*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL: PlaNet: # Paper net: branch + trunk conenction and everything \n",
    "if MODEL_NAME == NORM:\n",
    "    class PlaNet(Module): # Paper net: branch + trunk conenction and everything\n",
    "        def __init__(self):\n",
    "            super(PlaNet, self).__init__()\n",
    "            #branch\n",
    "            self.branch = Sequential(\n",
    "                Linear(INPUT_SIZE, 256), Λ(),\n",
    "                Linear(256, 128), Λ(),\n",
    "                Linear(128, 64), Λ(),\n",
    "            )\n",
    "            #trunk\n",
    "            # def trunk_block(): # faster\n",
    "            #     return  Sequential(\n",
    "            #         Conv2d(1, 8, kernel_size=3, stride=2, padding=1), BatchNorm2d(8), Λ(),\n",
    "            #         Conv2d(8, 16, kernel_size=3, stride=2, padding=1), BatchNorm2d(16), Λ(),\n",
    "            #         Conv2d(16, 32, kernel_size=3, stride=2, padding=1), BatchNorm2d(32), Λ(),\n",
    "            #     )\n",
    "            def trunk_block(): \n",
    "                return  Sequential(\n",
    "                    Conv2d(1, 8, kernel_size=3, stride=1, padding=1), BatchNorm2d(8), Λ(), MaxPool2d(2),\n",
    "                    Conv2d(8, 16, kernel_size=3, stride=1, padding=1), BatchNorm2d(16), Λ(), MaxPool2d(2),\n",
    "                    Conv2d(16, 32, kernel_size=3, stride=1, padding=1), BatchNorm2d(32), Λ(), MaxPool2d(2),\n",
    "                )\n",
    "            self.trunk_r, self.trunk_z = trunk_block(), trunk_block()\n",
    "            self.trunk_fc = Sequential(\n",
    "                Linear(2*32*8*8, 128), Λ(),\n",
    "                Linear(128, 64), Λ(),\n",
    "                Linear(64, 64), Λ(),\n",
    "            )\n",
    "            # head\n",
    "            self.fc = Sequential(Linear(64, 2048), Λ)\n",
    "            self.anti_conv = Sequential( # U-Net style\n",
    "                ConvTranspose2d(32, 32, kernel_size=2, stride=2), \n",
    "                Conv2d(32, 32, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(32, 32, kernel_size=3, padding=0), Λ(),\n",
    "                ConvTranspose2d(32, 16, kernel_size=2, stride=2),\n",
    "                Conv2d(16, 16, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(16, 16, kernel_size=3, padding=0), Λ(),\n",
    "                ConvTranspose2d(16, 8, kernel_size=2, stride=2),\n",
    "                Conv2d(8, 8, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(8, 8, kernel_size=3, padding=0), Λ(),\n",
    "                ConvTranspose2d(8, 4, kernel_size=2, stride=2),\n",
    "                Conv2d(4, 2, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(2, 1, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(1, 1, kernel_size=5, padding=0),\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            xb, r, z = x\n",
    "            #branch net\n",
    "            xb = self.branch(xb)\n",
    "            #trunk net\n",
    "            r, z = self.trunk_r(r), self.trunk_z(z) # convolutions\n",
    "            r, z = r.view(-1, 32*8*8), z.view(-1, 32*8*8) # flatten\n",
    "            xt = torch.cat((r, z), 1) # concatenate\n",
    "            xt = self.trunk_fc(xt) # fully connected\n",
    "            # multiply trunk and branch\n",
    "            x = xt * xb\n",
    "            #head net\n",
    "            x = self.fc(x)\n",
    "            x = x.view(-1, 32, 8, 8)\n",
    "            x = self.anti_conv(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL: VERY BIG PlaNet: # SAME NET, BUT MORE NEURONS\n",
    "if MODEL_NAME == BIG:\n",
    "    class PlaNet(Module): # Paper net: branch + trunk conenction and everything\n",
    "        def __init__(self):\n",
    "            super(PlaNet, self).__init__()\n",
    "            #branch\n",
    "            self.branch = Sequential(\n",
    "                Linear(INPUT_SIZE, 256), Λ(),\n",
    "                Linear(256, 128), Λ(),\n",
    "                Linear(128, 64), Λ(),\n",
    "            )\n",
    "            #trunk\n",
    "            def trunk_block(): \n",
    "                return  Sequential(\n",
    "                    Conv2d(1, 8, kernel_size=3, stride=1, padding=1), BatchNorm2d(8), Λ(), MaxPool2d(2),\n",
    "                    Conv2d(8, 16, kernel_size=3, stride=1, padding=1), BatchNorm2d(16), Λ(), MaxPool2d(2),\n",
    "                    Conv2d(16, 32, kernel_size=3, stride=1, padding=1), BatchNorm2d(32), Λ(), MaxPool2d(2),\n",
    "                )\n",
    "            self.trunk_r, self.trunk_z = trunk_block(), trunk_block()\n",
    "            self.trunk_fc = Sequential(\n",
    "                Linear(2*32*8*8, 128), Λ(),\n",
    "                Linear(128, 64), Λ(),\n",
    "                Linear(64, 64), Λ(), \n",
    "            )\n",
    "            # head\n",
    "            self.fc = Sequential(Linear(64, 4096), Λ())\n",
    "            self.anti_conv = Sequential( # U-Net style\n",
    "                ConvTranspose2d(64, 64, kernel_size=2, stride=2), \n",
    "                Conv2d(64, 64, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(64, 64, kernel_size=3, padding=0), Λ(),\n",
    "                ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n",
    "                Conv2d(32, 32, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(32, 32, kernel_size=3, padding=0), Λ(),\n",
    "                ConvTranspose2d(32, 16, kernel_size=2, stride=2),\n",
    "                Conv2d(16, 16, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(16, 16, kernel_size=3, padding=0), Λ(),\n",
    "                ConvTranspose2d(16, 8, kernel_size=2, stride=2),\n",
    "                Conv2d(8, 4, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(4, 2, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(2, 1, kernel_size=5, padding=0),\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            xb, r, z = x\n",
    "            #branch net\n",
    "            xb = self.branch(xb)\n",
    "            #trunk net\n",
    "            r, z = self.trunk_r(r), self.trunk_z(z) # convolutions\n",
    "            r, z = r.view(-1, 32*8*8), z.view(-1, 32*8*8) # flatten\n",
    "            xt = torch.cat((r, z), 1) # concatenate\n",
    "            xt = self.trunk_fc(xt) # fully connected\n",
    "            # multiply trunk and branch\n",
    "            x = xt * xb\n",
    "            #head net\n",
    "            x = self.fc(x)\n",
    "            x = x.view(-1, 64, 8, 8)\n",
    "            x = self.anti_conv(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL: SMALL PlaNet: # Paper net: branch + trunk conenction and everything BUT SMALLER\n",
    "if MODEL_NAME == SMALL:\n",
    "    class PlaNet(Module): # Paper net: branch + trunk conenction and everything\n",
    "        def __init__(self):\n",
    "            super(PlaNet, self).__init__()\n",
    "            #branch\n",
    "            self.branch = Sequential(\n",
    "                Linear(INPUT_SIZE, 128), Λ(),\n",
    "                Linear(128, 64), Λ(),\n",
    "                Linear(64, 32), Λ(),\n",
    "            )\n",
    "            def trunk_block(): \n",
    "                return  Sequential(\n",
    "                    Conv2d(1, 4, kernel_size=3, stride=1, padding=1), BatchNorm2d(4), Λ(), MaxPool2d(2),\n",
    "                    Conv2d(4, 8, kernel_size=3, stride=1, padding=1), BatchNorm2d(8), Λ(), MaxPool2d(2),\n",
    "                    Conv2d(8, 16, kernel_size=3, stride=1, padding=1), BatchNorm2d(16), Λ(), MaxPool2d(2),\n",
    "                )\n",
    "            self.trunk_r, self.trunk_z = trunk_block(), trunk_block()\n",
    "            self.trunk_fc = Sequential(\n",
    "                Linear(2*16*8*8, 64), Λ(),\n",
    "                Linear(64, 32), Λ(),\n",
    "                Linear(32, 32), Λ(),\n",
    "            )\n",
    "            # head\n",
    "            self.fc = Sequential(Linear(32, 1024), Λ)\n",
    "            self.anti_conv = Sequential( # U-Net style\n",
    "                ConvTranspose2d(16, 16, kernel_size=2, stride=2), \n",
    "                Conv2d(16, 16, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(16, 16, kernel_size=3, padding=0), Λ(),\n",
    "                ConvTranspose2d(16, 8, kernel_size=2, stride=2),\n",
    "                Conv2d(8, 8, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(8, 8, kernel_size=3, padding=0), Λ(),\n",
    "                ConvTranspose2d(8, 4, kernel_size=2, stride=2),\n",
    "                Conv2d(4, 4, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(4, 4, kernel_size=3, padding=0), Λ(),\n",
    "                ConvTranspose2d(4, 2, kernel_size=2, stride=2),\n",
    "                Conv2d(2, 2, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(2, 1, kernel_size=3, padding=0), Λ(),\n",
    "                Conv2d(1, 1, kernel_size=5, padding=0),\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            xb, r, z = x\n",
    "            #branch net\n",
    "            xb = self.branch(xb)\n",
    "            #trunk net\n",
    "            r, z = self.trunk_r(r), self.trunk_z(z) # convolutions\n",
    "            r, z = r.view(-1, 16*8*8), z.view(-1, 16*8*8) # flatten\n",
    "            xt = torch.cat((r, z), 1) # concatenate\n",
    "            xt = self.trunk_fc(xt) # fully connected\n",
    "            # multiply trunk and branch\n",
    "            x = xt * xb\n",
    "            #head net\n",
    "            x = self.fc(x)\n",
    "            x = x.view(-1, 16, 8, 8)\n",
    "            x = self.anti_conv(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model inputs / outputs\n",
    "x = (torch.rand(1, INPUT_SIZE), torch.rand(1, 1, 64, 64), torch.rand(1, 1, 64, 64))\n",
    "net = PlaNet()\n",
    "y = net(x)\n",
    "print(f\"in: {[x.shape for x in x]}, out: {y.shape}\")\n",
    "n_sampl = 7\n",
    "nx = (torch.rand(n_sampl, INPUT_SIZE), torch.rand(n_sampl, 1, 64, 64), torch.rand(n_sampl, 1, 64, 64))\n",
    "ny = net(nx)\n",
    "print(f\"in: {[x.shape for x in nx]}, out: {ny.shape}\")\n",
    "assert ny.shape == (n_sampl, 1, 64, 64), f\"Wrong output shape: {ny.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_ds, val_ds = PlaNetDataset(TRAIN_DS_PATH), PlaNetDataset(EVAL_DS_PATH) # initialize datasets\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True) # initialize DataLoader\n",
    "    val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)  \n",
    "    model = PlaNet()  # instantiate model\n",
    "    if LOAD_PRETRAINED is not None: # load pretrained model\n",
    "        model.load_state_dict(torch.load(LOAD_PRETRAINED, map_location=torch.device(\"cpu\"))) # load pretrained model\n",
    "        print(f\"Pretrained model loaded: {LOAD_PRETRAINED}\")\n",
    "    model.to(device) # move model to device\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE[0])\n",
    "    loss_fn = torch.nn.MSELoss() # Mean Squared Error Loss\n",
    "    tlog_tot, tlog_mse, tlog_gso, elog_tot, elog_mse, elog_gso = [], [], [], [], [], [] # logs for losses\n",
    "    start_time = time() # start time\n",
    "    for ep in range(EPOCHS): # epochs\n",
    "        epoch_time = time()\n",
    "        for pg in optimizer.param_groups: pg['lr'] = LEARNING_RATE[ep] # update learning rate\n",
    "        model.train()\n",
    "        trainloss, evalloss = [], []\n",
    "        for input, psi, rr, zz in train_dl:\n",
    "            optimizer.zero_grad() # zero gradients\n",
    "            psi_pred = model((input, rr, zz)) # forward pass\n",
    "            gso, gso_pred = calc_gso_batch(psi, rr, zz, dev=device), calc_gso_batch(psi_pred, rr, zz, dev=device) # calculate grad shafranov\n",
    "            mse_loss = loss_fn(psi_pred, psi) # mean squared error loss on psi\n",
    "            gso_loss = loss_fn(gso_pred, gso) # PINN loss on grad shafranov\n",
    "            loss = (1-GSO_LOSS_RATIO[ep])*mse_loss + GSO_LOSS_RATIO[ep]*gso_loss # total loss\n",
    "            loss.backward() # backprop\n",
    "            optimizer.step() # update weights\n",
    "            trainloss.append((loss.item(), mse_loss.item(), gso_loss.item())) # save batch losses\n",
    "        model.eval() # evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for input, psi, rr, zz in val_dl:\n",
    "                psi_pred = model((input, rr, zz))\n",
    "                gso, gso_pred = calc_gso_batch(psi, rr, zz, dev=device), calc_gso_batch(psi_pred, rr, zz, dev=device)\n",
    "                mse_loss = loss_fn(psi_pred, psi)\n",
    "                gso_loss = loss_fn(gso_pred, gso)\n",
    "                loss = (1-GSO_LOSS_RATIO[ep])*mse_loss + GSO_LOSS_RATIO[ep]*gso_loss # total loss\n",
    "                evalloss.append((loss.item(), mse_loss.item(), gso_loss.item()))\n",
    "        tloss_tot, tloss_mse, tloss_gso = map(lambda x: sum(x)/len(x), zip(*trainloss))\n",
    "        eloss_tot, eloss_mse, eloss_gso = map(lambda x: sum(x)/len(x), zip(*evalloss))\n",
    "        # save model if improved        \n",
    "        endp = \"\\n\" \n",
    "        if eloss_tot <= min(elog_tot, default=eloss_tot): \n",
    "            torch.save(model.state_dict(), f\"{SAVE_DIR}/mg_planet_tot.pth\"); endp=\" *tot\\n\"\n",
    "        if eloss_mse <= min(elog_mse, default=eloss_mse):\n",
    "            torch.save(model.state_dict(), f\"{SAVE_DIR}/mg_planet_mse.pth\"); endp=\" *mse\\n\"\n",
    "        if eloss_gso <= min(elog_gso, default=eloss_gso):\n",
    "            torch.save(model.state_dict(), f\"{SAVE_DIR}/mg_planet_gso.pth\"); endp=\" *gso\\n\"\n",
    "        tlog_tot.append(tloss_tot); tlog_mse.append(tloss_mse); tlog_gso.append(tloss_gso)\n",
    "        elog_tot.append(eloss_tot); elog_mse.append(eloss_mse); elog_gso.append(eloss_gso) \n",
    "        print(f\"{ep+1}/{EPOCHS}: \"\n",
    "            f\"Eval: tot {eloss_tot:.4f}, mse {eloss_mse:.4f}, gso {eloss_gso:.4f} | \" + \n",
    "            f\"lr:{LEARNING_RATE[ep]:.1e}, r:{GSO_LOSS_RATIO[ep]:.2f} | {time()-epoch_time:.0f}s, eta:{(time()-start_time)*(EPOCHS-ep)/(ep+1)/60:.0f}m |\", end=endp,  flush=True)\n",
    "        if ep >= 10 and ((eloss_gso > 30.0 and GSO_LOSS_RATIO[ep] > 0.01) or eloss_mse > 11.0): return False, () # stop training, if not converging, try again\n",
    "    print(f\"Training time: {(time()-start_time)/60:.0f}mins\")\n",
    "    print(f\"Best losses: tot {min(elog_tot):.4f}, mse {min(elog_mse):.4f}, gso {min(elog_gso):.4f}\")\n",
    "    for l, n in zip([tlog_tot, tlog_mse, tlog_gso], [\"tot\", \"mse\", \"gso\"]): np.save(f\"{SAVE_DIR}/train_{n}_losses.npy\", l) # save losses\n",
    "    for l, n in zip([elog_tot, elog_mse, elog_gso], [\"tot\", \"mse\", \"gso\"]): np.save(f\"{SAVE_DIR}/eval_{n}_losses.npy\", l) # save losses\n",
    "    return True, (tlog_tot, tlog_mse, tlog_gso, elog_tot, elog_mse, elog_gso)\n",
    "\n",
    "# train the model (multiple attempts)\n",
    "for i in range(10): \n",
    "    success, logs = train()\n",
    "    if success: tlog_tot, tlog_mse, tlog_gso, elog_tot, elog_mse, elog_gso = logs; break\n",
    "    else: print(f\"Convergence failed, retrying... {i+1}/10\")\n",
    "assert success, \"Training failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "fig, ax = plt.subplots(2, 3, figsize=(12, 6))\n",
    "ce, ct = \"yellow\", \"red\"\n",
    "lw = 1.0\n",
    "ax[0,0].set_title(\"TOT Loss\")\n",
    "ax[0,0].plot(tlog_tot, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[0,0].plot(elog_tot, color=ce, label=\"eval\", linewidth=lw)\n",
    "ax[0,1].set_title(\"MSE Loss\")\n",
    "ax[0,1].plot(tlog_mse, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[0,1].plot(elog_mse, color=ce, label=\"eval\", linewidth=lw)\n",
    "ax[0,2].set_title(\"GSO Loss\")\n",
    "ax[0,2].plot(tlog_gso, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[0,2].plot(elog_gso, color=ce, label=\"eval\", linewidth=lw)\n",
    "#now the same but with log scale\n",
    "ax[1,0].set_title(\"TOT Loss (log)\")\n",
    "ax[1,0].plot(tlog_tot, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[1,0].plot(elog_tot, color=ce, label=\"eval\", linewidth=lw)\n",
    "ax[1,0].set_yscale(\"log\")\n",
    "ax[1,0].grid(True, which=\"both\", axis=\"y\")\n",
    "\n",
    "ax[1,1].set_title(\"MSE Loss (log)\")\n",
    "ax[1,1].plot(tlog_mse, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[1,1].plot(elog_mse, color=ce, label=\"eval\", linewidth=lw)\n",
    "ax[1,1].set_yscale(\"log\")\n",
    "ax[1,1].grid(True, which=\"both\", axis=\"y\")\n",
    "\n",
    "ax[1,2].set_title(\"GSO Loss (log)\")\n",
    "ax[1,2].plot(tlog_gso, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[1,2].plot(elog_gso, color=ce, label=\"eval\", linewidth=lw)\n",
    "ax[1,2].set_yscale(\"log\")\n",
    "ax[1,2].grid(True, which=\"both\", axis=\"y\")\n",
    "for a in ax.flatten(): a.legend(); a.set_xlabel(\"Epoch\"); a.set_ylabel(\"Loss\")\n",
    "plt.tight_layout()\n",
    "plt.show() if HAS_SCREEN else plt.savefig(f\"mg_data/{JOBID}/losses.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing network output\n",
    "for titl, best_model_path in zip([\"TOT\",\"MSE\", \"GSO\"], [\"mg_planet_tot.pth\", \"mg_planet_mse.pth\", \"mg_planet_gso.pth\"]):\n",
    "    model = PlaNet()\n",
    "    model.load_state_dict(torch.load(f\"{SAVE_DIR}/{best_model_path}\"))\n",
    "    model.eval()\n",
    "    ds = PlaNetDataset(EVAL_DS_PATH)\n",
    "    # ds = PlaNetDataset(TRAIN_DS_PATH)\n",
    "    os.makedirs(f\"mg_data/{JOBID}/imgs\", exist_ok=True)\n",
    "    N_PLOTS = 2 if HAS_SCREEN else 50\n",
    "    for i in np.random.randint(0, len(ds), N_PLOTS):  \n",
    "        fig, axs = plt.subplots(2, 5, figsize=(15, 9))\n",
    "        input, psi_ds, rr, zz = ds[i]\n",
    "        input, psi_ds, rr, zz = input.to('cpu'), psi_ds.to('cpu'), rr.to('cpu'), zz.to('cpu')\n",
    "        input, psi_ds, rr, zz = input.view(1,-1), psi_ds.view(1,1,64,64), rr.view(1,1,64,64), zz.view(1,1,64,64)\n",
    "        psi_pred = model((input, rr, zz))\n",
    "        gso, gso_pred = calc_gso_batch(psi_ds, rr, zz), calc_gso_batch(psi_pred, rr, zz)\n",
    "        gso, gso_pred = gso.detach().numpy().reshape(64, 64), gso_pred.detach().numpy().reshape(64, 64)\n",
    "        gso_range = (gso.max(), gso.min())\n",
    "        gso_levels = np.linspace(gso_range[1], gso_range[0], 12)\n",
    "        gso_pred = np.clip(gso_pred, gso_range[1], gso_range[0]) # clip to gso range\n",
    "        \n",
    "        psi_pred = psi_pred.detach().numpy().reshape(64, 64)\n",
    "        psi_ds = psi_ds.detach().numpy().reshape(64, 64)\n",
    "        rr, zz = rr.view(64, 64).detach().numpy(), zz.view(64, 64).detach().numpy()\n",
    "        ext = [ds.rr.min(), ds.rr.max(), ds.zz.min(), ds.zz.max()]\n",
    "        bmin, bmax = np.min([psi_ds, psi_pred]), np.max([psi_ds, psi_pred]) # min max psi\n",
    "        blevels = np.linspace(bmin, bmax, 13, endpoint=True)\n",
    "        ψ_mse = (psi_ds - psi_pred)**2\n",
    "        gso_mse = (gso - gso_pred)**2\n",
    "        mse_levels1 = np.linspace(0, 0.5, 13, endpoint=True)\n",
    "        mse_levels2 = np.linspace(0, 0.05, 13, endpoint=True)\n",
    "\n",
    "        im00 = axs[0,0].contourf(rr, zz, psi_ds, blevels, cmap=\"inferno\")\n",
    "        axs[0,0].set_title(\"Actual\")\n",
    "        axs[0,0].set_aspect('equal')\n",
    "        axs[0,0].set_ylabel(\"ψ\")\n",
    "        fig.colorbar(im00, ax=axs[0,0]) \n",
    "        im01 = axs[0,1].contourf(rr, zz, psi_pred, blevels, cmap=\"inferno\")\n",
    "        axs[0,1].set_title(\"Predicted\")\n",
    "        fig.colorbar(im01, ax=axs[0,1])\n",
    "        im02 = axs[0,2].contour(rr, zz, psi_ds, blevels, linestyles='dashed', cmap=\"inferno\")\n",
    "        axs[0,2].contour(rr, zz, psi_pred, blevels, cmap=\"inferno\")\n",
    "        axs[0,2].set_title(\"Contours\")\n",
    "        fig.colorbar(im02, ax=axs[0,2])\n",
    "        im03 = axs[0,3].contourf(rr, zz, np.clip(ψ_mse, 0, 0.5), mse_levels1, cmap=\"inferno\")\n",
    "        axs[0,3].set_title(\"MSE 0.5\")\n",
    "        fig.colorbar(im03, ax=axs[0,3])\n",
    "        im04 = axs[0,4].contourf(rr, zz, np.clip(ψ_mse, 0.00001, 0.04999), mse_levels2, cmap=\"inferno\")\n",
    "        axs[0,4].set_title(\"MSE 0.05\")\n",
    "        fig.colorbar(im04, ax=axs[0,4])\n",
    "        im10 = axs[1,0].contourf(rr, zz, gso, gso_levels, cmap=\"inferno\")\n",
    "        axs[1,0].set_ylabel(\"GSO\")\n",
    "        fig.colorbar(im10, ax=axs[1,0])\n",
    "        im6 = axs[1,1].contourf(rr, zz, gso_pred, gso_levels, cmap=\"inferno\")\n",
    "        fig.colorbar(im6, ax=axs[1,1])\n",
    "        im12 = axs[1,2].contour(rr, zz, gso, gso_levels, linestyles='dashed', cmap=\"inferno\")\n",
    "        axs[1,2].contour(rr, zz, gso_pred, gso_levels, cmap=\"inferno\")\n",
    "        fig.colorbar(im12, ax=axs[1,2])\n",
    "        im13 = axs[1,3].contourf(rr, zz, np.clip(gso_mse, 0, 0.5), mse_levels1, cmap=\"inferno\")\n",
    "        fig.colorbar(im13, ax=axs[1,3])\n",
    "        im14 = axs[1,4].contourf(rr, zz, np.clip(gso_mse, 0.00001, 0.04999), mse_levels2, cmap=\"inferno\")\n",
    "        fig.colorbar(im14, ax=axs[1,4])\n",
    "\n",
    "        for ax in axs.flatten(): ax.grid(False), ax.set_xticks([]), ax.set_yticks([]), ax.set_aspect(\"equal\")\n",
    "\n",
    "        #suptitle\n",
    "        plt.suptitle(f\"PlaNet: {titl} {i}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show() if HAS_SCREEN else plt.savefig(f\"mg_data/{JOBID}/imgs/planet_{titl}_{i}.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test inference speed\n",
    "model = PlaNet()\n",
    "model.load_state_dict(torch.load(f\"{SAVE_DIR}/{best_model_path}\"))\n",
    "model.eval()\n",
    "ds = PlaNetDataset(EVAL_DS_PATH)\n",
    "n_samples = 100\n",
    "random_idxs = np.random.choice(n_samples, len(ds))\n",
    "#cpu\n",
    "cpu_times = []\n",
    "for i in random_idxs:\n",
    "    start_t = time()\n",
    "    input, psi_ds, rr, zz = ds[i]\n",
    "    input, psi_ds, rr, zz = input.to('cpu'), psi_ds.to('cpu'), rr.to('cpu'), zz.to('cpu')\n",
    "    input, psi_ds, rr, zz = input.view(1,-1), psi_ds.view(1,1,64,64), rr.view(1,1,64,64), zz.view(1,1,64,64)\n",
    "    psi_pred = model((input, rr, zz))\n",
    "    end_t = time()\n",
    "    cpu_times.append(end_t - start_t) \n",
    "# device\n",
    "model.to(device)\n",
    "dev_times = []\n",
    "for i in random_idxs:\n",
    "    input, psi_ds, rr, zz = ds[i]\n",
    "    input, psi_ds, rr, zz = input.view(1,-1), psi_ds.view(1,1,64,64), rr.view(1,1,64,64), zz.view(1,1,64,64)\n",
    "    start_t = time()\n",
    "    psi_pred = model((input, rr, zz))\n",
    "    end_t = time()\n",
    "    dev_times.append(end_t - start_t)    \n",
    "cpu_times, dev_times = np.array(cpu_times), np.array(dev_times)\n",
    "print(f\"cpu: inference time: {cpu_times.mean():.5f}s, std: {cpu_times.std():.5f}\")\n",
    "print(f\"dev: inference time: {dev_times.mean():.5f}s, std: {dev_times.std():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\", flush=True)\n",
    "if not HAS_SCREEN: sleep(30) # wait for files to update (for cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy the log file to the folder\n",
    "os.system(f\"cp jobs/{JOBID}.txt mg_data/{JOBID}/log.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not HAS_SCREEN: exit(0) # skip this test sections on cluster\n",
    "# testing network output on full frame dataset (not subgrids)\n",
    "TESTS_DIR = \"mg_data_deicluster/1817333\"\n",
    "for titl, best_model_path in zip([\"TOT\",\"MSE\", \"GSO\"], [\"mg_planet_tot.pth\", \"mg_planet_mse.pth\", \"mg_planet_gso.pth\"]):\n",
    "    model = PlaNet()\n",
    "    model.load_state_dict(torch.load(f\"{TESTS_DIR}/{best_model_path}\", map_location=torch.device(\"cpu\")))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # ds = PlaNetDataset(\"data/train_ds_10000_80_100.mat\")\n",
    "    ds = PlaNetDataset(\"data/eval_ds_10000_80_100.mat\")\n",
    "\n",
    "    #run an evaluation on all the dataset\n",
    "    dl = DataLoader(ds, batch_size=1000, shuffle=False)\n",
    "    mses, gsos = [],[]\n",
    "    for input, psi_ds, rr, zz in dl:\n",
    "        psi_pred = model((input, rr, zz))\n",
    "        gso, gso_pred = calc_gso_batch(psi_ds, rr, zz, device), calc_gso_batch(psi_pred, rr, zz, device)\n",
    "        loss_gso = torch.nn.MSELoss()(gso, gso_pred)\n",
    "        loss_psi = torch.nn.MSELoss()(psi_ds, psi_pred)\n",
    "        mses.append(loss_psi.item())\n",
    "        gsos.append(loss_gso.item())\n",
    "    print(f\"Model {titl} MSE: {np.mean(mses):.5f}, GSO: {np.mean(gsos):.5f}\")\n",
    "\n",
    "    os.makedirs(f\"mg_data/{JOBID}/imgs\", exist_ok=True)\n",
    "    N_PLOTS = 2 if HAS_SCREEN else 50\n",
    "    model.to('cpu')\n",
    "    for i in np.random.randint(0, len(ds), N_PLOTS):  \n",
    "        fig, axs = plt.subplots(2, 5, figsize=(15, 9))\n",
    "        input, psi_ds, rr, zz = ds[i]\n",
    "        input, psi_ds, rr, zz = input.to('cpu'), psi_ds.to('cpu'), rr.to('cpu'), zz.to('cpu')\n",
    "        input, psi_ds, rr, zz = input.view(1,-1), psi_ds.view(1,1,64,64), rr.view(1,1,64,64), zz.view(1,1,64,64)\n",
    "        psi_pred = model((input, rr, zz))\n",
    "        gso, gso_pred = calc_gso_batch(psi_ds, rr, zz), calc_gso_batch(psi_pred, rr, zz)\n",
    "        gso, gso_pred = gso.detach().numpy().reshape(64, 64), gso_pred.detach().numpy().reshape(64, 64)\n",
    "        gso_range = (gso.max(), gso.min())\n",
    "        gso_levels = np.linspace(gso_range[1], gso_range[0], 12)\n",
    "        gso_pred = np.clip(gso_pred, gso_range[1], gso_range[0]) # clip to gso range\n",
    "        \n",
    "        psi_pred = psi_pred.detach().numpy().reshape(64, 64)\n",
    "        psi_ds = psi_ds.detach().numpy().reshape(64, 64)\n",
    "        rr, zz = rr.view(64, 64).detach().numpy(), zz.view(64, 64).detach().numpy()\n",
    "        ext = [ds.rr.min(), ds.rr.max(), ds.zz.min(), ds.zz.max()]\n",
    "        bmin, bmax = np.min([psi_ds, psi_pred]), np.max([psi_ds, psi_pred]) # min max psi\n",
    "        blevels = np.linspace(bmin, bmax, 13, endpoint=True)\n",
    "        ψ_mse = (psi_ds - psi_pred)**2\n",
    "        gso_mse = (gso - gso_pred)**2\n",
    "        mse_levels1 = np.linspace(0, 0.5, 13, endpoint=True)\n",
    "        mse_levels2 = np.linspace(0, 0.05, 13, endpoint=True)\n",
    "\n",
    "        im00 = axs[0,0].contourf(rr, zz, psi_ds, blevels, cmap=\"inferno\")\n",
    "        axs[0,0].set_title(\"Actual\")\n",
    "        axs[0,0].set_aspect('equal')\n",
    "        axs[0,0].set_ylabel(\"ψ\")\n",
    "        fig.colorbar(im00, ax=axs[0,0]) \n",
    "        im01 = axs[0,1].contourf(rr, zz, psi_pred, blevels, cmap=\"inferno\")\n",
    "        axs[0,1].set_title(\"Predicted\")\n",
    "        fig.colorbar(im01, ax=axs[0,1])\n",
    "        im02 = axs[0,2].contour(rr, zz, psi_ds, blevels, linestyles='dashed', cmap=\"inferno\")\n",
    "        axs[0,2].contour(rr, zz, psi_pred, blevels, cmap=\"inferno\")\n",
    "        axs[0,2].set_title(\"Contours\")\n",
    "        fig.colorbar(im02, ax=axs[0,2])\n",
    "        im03 = axs[0,3].contourf(rr, zz, np.clip(ψ_mse, 0, 0.5), mse_levels1, cmap=\"inferno\")\n",
    "        axs[0,3].set_title(\"MSE 0.5\")\n",
    "        fig.colorbar(im03, ax=axs[0,3])\n",
    "        im04 = axs[0,4].contourf(rr, zz, np.clip(ψ_mse, 0.00001, 0.04999), mse_levels2, cmap=\"inferno\")\n",
    "        axs[0,4].set_title(\"MSE 0.05\")\n",
    "        fig.colorbar(im04, ax=axs[0,4])\n",
    "        im10 = axs[1,0].contourf(rr, zz, gso, gso_levels, cmap=\"inferno\")\n",
    "        axs[1,0].set_ylabel(\"GSO\")\n",
    "        fig.colorbar(im10, ax=axs[1,0])\n",
    "        im6 = axs[1,1].contourf(rr, zz, gso_pred, gso_levels, cmap=\"inferno\")\n",
    "        fig.colorbar(im6, ax=axs[1,1])\n",
    "        im12 = axs[1,2].contour(rr, zz, gso, gso_levels, linestyles='dashed', cmap=\"inferno\")\n",
    "        axs[1,2].contour(rr, zz, gso_pred, gso_levels, cmap=\"inferno\")\n",
    "        fig.colorbar(im12, ax=axs[1,2])\n",
    "        im13 = axs[1,3].contourf(rr, zz, np.clip(gso_mse, 0, 0.5), mse_levels1, cmap=\"inferno\")\n",
    "        fig.colorbar(im13, ax=axs[1,3])\n",
    "        im14 = axs[1,4].contourf(rr, zz, np.clip(gso_mse, 0.00001, 0.04999), mse_levels2, cmap=\"inferno\")\n",
    "        fig.colorbar(im14, ax=axs[1,4])\n",
    "\n",
    "        for ax in axs.flatten(): ax.grid(False), ax.set_xticks([]), ax.set_yticks([]), ax.set_aspect(\"equal\")\n",
    "\n",
    "        #suptitle\n",
    "        plt.suptitle(f\"PlaNet: {titl} {i}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show() if HAS_SCREEN else plt.savefig(f\"mg_data/{JOBID}/imgs/planet_{titl}_{i}.png\")\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
