{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Prepapre dataset with the prepare_dataset notebook, before running this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from utils import *\n",
    "\n",
    "SAVE_DIR = f\"data/{JOBID}\"  \n",
    "# DEV = torch.device('cpu') # for debugging, use cpu\n",
    "os.makedirs(f\"{SAVE_DIR}\", exist_ok=True)\n",
    "print(f'DEV: {DEV}, has_screen: {LOCAL}, job id: {JOBID}')\n",
    "\n",
    "# copy the python training to the directory (for cluster) (for local, it fails silently)\n",
    "os.system(f\"cp train.py {SAVE_DIR}/train.py\")\n",
    "os.system(f\"cp utils.py {SAVE_DIR}/utils.py\")\n",
    "\n",
    "# SMALL, NORM, BIG = \"small\", \"norm\", \"big\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPOCHS = 20 # number of epochs # 1000\n",
    "# BATCH_SIZE = 128 # 128 \n",
    "BATCH_SIZE = 64 # 64 <-\n",
    "\n",
    "# LOAD_PRETRAINED = \"trained_models/pretrained_1809761.pth\" # norm model\n",
    "LOAD_PRETRAINED = CURR_EVAL_MODEL if LOCAL else None # pretrained model\n",
    "# LOAD_PRETRAINED = None # Set it to None if you don't want to load pretrained model\n",
    "\n",
    "# LEARNING_RATE = 3e-4*np.logspace(0, -2, EPOCHS) \n",
    "# LEARNING_RATE = 3e-3*np.ones(EPOCHS) \n",
    "# LEARNING_RATE = 3e-3*np.logspace(0, -2, EPOCHS)  \n",
    "LEARNING_RATE = 1e-3*np.logspace(0, -2, EPOCHS) # <-\n",
    "\n",
    "# GSO_LOSS_RATIO = np.concatenate((np.linspace(1e-6, 3e-3, EPOCHS//2), np.linspace(3e-3, 0.0, EPOCHS//2))) \n",
    "MAX_GSO = 5e-3 # 3e-3 <-\n",
    "GSO_LOSS_RATIO = np.concatenate((np.linspace(1e-6, MAX_GSO, EPOCHS//4), \n",
    "                                 np.linspace(MAX_GSO, MAX_GSO, EPOCHS//4), \n",
    "                                 np.linspace(MAX_GSO, 0.0, EPOCHS//4), \n",
    "                                 np.linspace(0.0, 0.0, EPOCHS//4))) # <-\n",
    "# GSO_LOSS_RATIO = np.concatenate((MAX_GSO*np.logspace(-6, 0, EPOCHS//4), \n",
    "#                                  MAX_GSO*np.logspace(0, 0, EPOCHS//4), \n",
    "#                                  MAX_GSO*np.logspace(0, -10, EPOCHS//4), \n",
    "#                                  np.logspace(-12, -12, EPOCHS//4))) \n",
    "# GSO_LOSS_RATIO = np.zeros(EPOCHS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks\n",
    "if LOAD_PRETRAINED is not None: assert os.path.exists(LOAD_PRETRAINED), \"Pretrained model does not exist\"\n",
    "assert os.path.exists(TRAIN_DS_PATH), \"Training dataset does not exist\"\n",
    "assert os.path.exists(EVAL_DS_PATH), \"Evaluation dataset does not exist\"\n",
    "assert os.path.exists(SAVE_DIR), \"Save directory does not exist\"\n",
    "assert len(LEARNING_RATE) == EPOCHS, \"Learning rate array length does not match epochs\"\n",
    "assert len(GSO_LOSS_RATIO) == EPOCHS, \"GSO loss ratio array length does not match epochs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot schedulers: lr + gso loss ratio\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 3))\n",
    "ax[0].set_title(\"Learning Rate [Log]\")\n",
    "ax[0].plot(LEARNING_RATE, color=\"red\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Learning Rate\")\n",
    "ax[0].set_yscale(\"log\")\n",
    "ax[1].set_title(\"GSO Loss Ratio\")\n",
    "ax[1].plot(GSO_LOSS_RATIO, color=\"red\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"GSO Loss Ratio\")\n",
    "plt.tight_layout()\n",
    "plt.show() if LOCAL else plt.savefig(f\"{SAVE_DIR}/schedulers.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and evaluation datasets\n",
    "train_ds = LiuqeDataset(TRAIN_DS_PATH)\n",
    "val_ds = LiuqeDataset(EVAL_DS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test net I/O\n",
    "test_network_io()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True) # initialize DataLoader\n",
    "    val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)  \n",
    "    model = LiuqeNet()  # instantiate model\n",
    "    if LOAD_PRETRAINED is not None: # load pretrained model\n",
    "        model.load_state_dict(torch.load(LOAD_PRETRAINED, map_location=torch.device(\"cpu\")), strict=STRICT_LOAD) # load pretrained model\n",
    "        print(f\"Pretrained model loaded: {LOAD_PRETRAINED}\")\n",
    "    model.to(DEV) # move model to DEV\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE[0])\n",
    "    loss_fn = torch.nn.MSELoss() # Mean Squared Error Loss\n",
    "    tlog_tot, tlog_mse, tlog_gso, elog_tot, elog_mse, elog_gso = [], [], [], [], [], [] # logs for losses\n",
    "    start_time = time() # start time\n",
    "    for ep in range(EPOCHS): # epochs\n",
    "        epoch_time = time()\n",
    "        for pg in optimizer.param_groups: pg['lr'] = LEARNING_RATE[ep] # update learning rate\n",
    "        model.train()\n",
    "        trainloss, evalloss = [], []\n",
    "        for X, Y, r, z in train_dl:\n",
    "            X, Y, r, z = X.to(DEV), Y.to(DEV), r.to(DEV), z.to(DEV) # move to DEV\n",
    "            optimizer.zero_grad() # zero gradients\n",
    "            yp = model(X, r, z) # forward pass\n",
    "            assert yp.shape == Y.shape, f\"Output shape {yp.shape} != {Y.shape}\"\n",
    "            gso, gsop = calc_gso_batch(Y, r, z, dev=DEV), calc_gso_batch(yp, r, z, dev=DEV) # calculate grad shafranov\n",
    "            mse_loss = loss_fn(yp, Y) # mean squared error loss on Y\n",
    "            gso_loss = loss_fn(gsop, gso) # PINN loss on grad shafranov\n",
    "            loss = (1-GSO_LOSS_RATIO[ep])*mse_loss + GSO_LOSS_RATIO[ep]*gso_loss # total loss\n",
    "            loss.backward() # backprop\n",
    "            optimizer.step() # update weights \n",
    "            trainloss.append((loss.item(), mse_loss.item(), gso_loss.item())) # save batch losses\n",
    "        model.eval() # evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for X, Y, r, z in val_dl:\n",
    "                X, Y, r, z = X.to(DEV), Y.to(DEV), r.to(DEV), z.to(DEV) # move to DEV\n",
    "                yp = model(X, r, z)\n",
    "                gso, gsop = calc_gso_batch(Y, r, z, dev=DEV), calc_gso_batch(yp, r, z, dev=DEV)\n",
    "                mse_loss = loss_fn(yp, Y)\n",
    "                gso_loss = loss_fn(gsop, gso)\n",
    "                loss = (1-GSO_LOSS_RATIO[ep])*mse_loss + GSO_LOSS_RATIO[ep]*gso_loss # total loss\n",
    "                evalloss.append((loss.item(), mse_loss.item(), gso_loss.item()))\n",
    "        tloss_tot, tloss_mse, tloss_gso = map(lambda x: sum(x)/len(x), zip(*trainloss))\n",
    "        eloss_tot, eloss_mse, eloss_gso = map(lambda x: sum(x)/len(x), zip(*evalloss))\n",
    "        # save model if improved        \n",
    "        endp = \"\\n\" \n",
    "        if eloss_tot <= min(elog_tot, default=eloss_tot): \n",
    "            torch.save(model.state_dict(), f\"{SAVE_DIR}/best_tot.pth\"); endp=\" [tot]\\n\"\n",
    "        if eloss_mse <= min(elog_mse, default=eloss_mse):\n",
    "            torch.save(model.state_dict(), f\"{SAVE_DIR}/best_mse.pth\"); endp=\" [mse]\\n\"\n",
    "        if eloss_gso <= min(elog_gso, default=eloss_gso):\n",
    "            torch.save(model.state_dict(), f\"{SAVE_DIR}/best_gso.pth\"); endp=\" [gso]\\n\"\n",
    "        tlog_tot.append(tloss_tot); tlog_mse.append(tloss_mse); tlog_gso.append(tloss_gso)\n",
    "        elog_tot.append(eloss_tot); elog_mse.append(eloss_mse); elog_gso.append(eloss_gso) \n",
    "        print(f\"[{ep+1}/{EPOCHS}] \"\n",
    "            f\"Eval -> tot {eloss_tot:.1e}, mse {eloss_mse:.1e}, gso {eloss_gso:.1e}, \" + \n",
    "            f\"lr {LEARNING_RATE[ep]:.1e}, r {GSO_LOSS_RATIO[ep]:.1e}, {time()-epoch_time:.0f}s, eta {(time()-start_time)*(EPOCHS-ep)/(ep+1)/60:.0f}m\", end=endp)\n",
    "        if ep >= 10 and ((eloss_gso > 30.0 and GSO_LOSS_RATIO[ep] > 0.01) or eloss_mse > .2): return False, () # stop training, if not converging, try again\n",
    "    print(f\"Training time: {(time()-start_time)/60:.0f}mins\")\n",
    "    print(f\"Best losses: tot {min(elog_tot):.3e}, mse {min(elog_mse):.3e}, gso {min(elog_gso):.4f}\")\n",
    "    print(f\"Estimated MAE: tot {np.sqrt(min(elog_tot)):.3e}, mae {np.sqrt(min(elog_mse)):.3e}, gso {np.sqrt(min(elog_gso)):.4f}\")\n",
    "    for l, n in zip([tlog_tot, tlog_mse, tlog_gso], [\"tot\", \"mse\", \"gso\"]): np.save(f\"{SAVE_DIR}/train_{n}_losses.npy\", l) # save losses\n",
    "    for l, n in zip([elog_tot, elog_mse, elog_gso], [\"tot\", \"mse\", \"gso\"]): np.save(f\"{SAVE_DIR}/eval_{n}_losses.npy\", l) # save losses\n",
    "    return True, (tlog_tot, tlog_mse, tlog_gso, elog_tot, elog_mse, elog_gso)\n",
    "\n",
    "# train the model (multiple attempts)\n",
    "for i in range(20): \n",
    "    print('Starting training...')\n",
    "    success, logs = train()\n",
    "    if success: tlog_tot, tlog_mse, tlog_gso, elog_tot, elog_mse, elog_gso = logs; break\n",
    "    else: print(f\"Convergence failed, retrying... {i+1}/20\")\n",
    "# if not success: delete the files and exit:\n",
    "if not success: os.system(f\"rm -rf {SAVE_DIR}\")\n",
    "assert success, \"Training failed, no model saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "print(\"Plotting losses...\")\n",
    "fig, ax = plt.subplots(2, 3, figsize=(12, 6))\n",
    "ce, ct = \"yellow\", \"red\"\n",
    "lw = 1.0\n",
    "ax[0,0].set_title(\"TOT Loss\")\n",
    "ax[0,0].plot(tlog_tot, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[0,0].plot(elog_tot, color=ce, label=\"eval\", linewidth=lw)\n",
    "ax[0,1].set_title(\"MSE Loss\")\n",
    "ax[0,1].plot(tlog_mse, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[0,1].plot(elog_mse, color=ce, label=\"eval\", linewidth=lw)\n",
    "ax[0,2].set_title(\"GSO Loss\")\n",
    "ax[0,2].plot(tlog_gso, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[0,2].plot(elog_gso, color=ce, label=\"eval\", linewidth=lw)\n",
    "#now the same but with log scale\n",
    "ax[1,0].set_title(\"TOT Los (log)\")\n",
    "ax[1,0].plot(tlog_tot, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[1,0].plot(elog_tot, color=ce, label=\"eval\", linewidth=lw)\n",
    "ax[1,0].set_yscale(\"log\")\n",
    "ax[1,0].grid(True, which=\"both\", axis=\"y\")\n",
    "\n",
    "ax[1,1].set_title(\"MSE Loss (log)\")\n",
    "ax[1,1].plot(tlog_mse, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[1,1].plot(elog_mse, color=ce, label=\"eval\", linewidth=lw)\n",
    "ax[1,1].set_yscale(\"log\")\n",
    "ax[1,1].grid(True, which=\"both\", axis=\"y\")\n",
    "\n",
    "ax[1,2].set_title(\"GSO Loss (log)\")\n",
    "ax[1,2].plot(tlog_gso, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[1,2].plot(elog_gso, color=ce, label=\"eval\", linewidth=lw)\n",
    "ax[1,2].set_yscale(\"log\")\n",
    "ax[1,2].grid(True, which=\"both\", axis=\"y\")\n",
    "plt.suptitle(f\"[{JOBID}] Training losses\")\n",
    "for a in ax.flatten(): a.legend(); a.set_xlabel(\"Epoch\"); a.set_ylabel(\"Loss\")\n",
    "plt.tight_layout()\n",
    "plt.show() if LOCAL else plt.savefig(f\"{SAVE_DIR}/losses.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing network output\n",
    "print(\"Testing network output...\")\n",
    "for mn, titl in zip([\"best_tot.pth\", \"best_mse.pth\", \"best_gso.pth\"], [\"TOT\", \"MSE\", \"GSO\"]):\n",
    "    model = LiuqeNet() # instantiate model\n",
    "    model.load_state_dict(torch.load(f'{SAVE_DIR}/{mn}', map_location=torch.device(\"cpu\"))) # load pretrained model\n",
    "    plot_network_outputs(SAVE_DIR, val_ds, model, titl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test inference speed\n",
    "print(\"Testing inference speed...\")\n",
    "model = LiuqeNet()\n",
    "model.load_state_dict(torch.load(f\"{SAVE_DIR}/best_tot.pth\"))\n",
    "model.eval()\n",
    "ds = val_ds\n",
    "n_samples = 100\n",
    "random_idxs = np.random.choice(n_samples, len(ds))\n",
    "#cpu\n",
    "cpu_times1, cpu_times2 = [], []\n",
    "for i in random_idxs:\n",
    "    start_t = time()\n",
    "    x, y, r, z = ds[i]\n",
    "    x, y, r, z = x.to('cpu'), y.to('cpu'), r.to('cpu'), z.to('cpu')\n",
    "    x, y, r, z = x.view(1,-1), y.view(1,1,NGZ,NGR), r.view(1,NGZ), z.view(1,NGR)\n",
    "    start_t2 = time()\n",
    "    yp = model(x, r, z)\n",
    "    end_t = time()\n",
    "    cpu_times1.append(end_t - start_t); cpu_times2.append(end_t - start_t2) \n",
    "# DEV\n",
    "model.to(DEV)\n",
    "dev_times1, dev_times2 = [], []\n",
    "for i in random_idxs:\n",
    "    x, y, r, z = ds[i]\n",
    "    x, y, r, z = x.to(DEV), y.to(DEV), r.to(DEV), z.to(DEV)\n",
    "    x, y, r, z = x.view(1,-1), y.view(1,1,NGZ,NGR), r.view(1,NGR), z.view(1,NGZ)\n",
    "    start_t = time()\n",
    "    start_t2 = time()\n",
    "    yp = model(x, r, z)\n",
    "    end_t = time()\n",
    "    dev_times1.append(end_t - start_t); dev_times2.append(end_t - start_t2)    \n",
    "cpu_times1, dev_times1 = np.array(cpu_times1)*1000, np.array(dev_times1)*1000\n",
    "cpu_times2, dev_times2 = np.array(cpu_times2)*1000, np.array(dev_times2)*1000\n",
    "print(f\"cpu: inference time: [full -> {cpu_times1.mean():.5f}ms, std: {cpu_times1.std():.5f}]\")\n",
    "print(f\"cpu: inference time: [inference only -> {cpu_times2.mean():.5f}ms, std: {cpu_times2.std():.5f}]\")\n",
    "print(f\"dev: inference time: [full -> {dev_times1.mean():.5f}ms, std: {dev_times1.std():.5f}]\")\n",
    "print(f\"dev: inference time: [inference only -> {dev_times2.mean():.5f}ms, std: {dev_times2.std():.5f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{JOBID} done\")\n",
    "if not LOCAL: sleep(30) # wait for files to update (for cluster)\n",
    "#copy the log file to the folder\n",
    "os.system(f\"cp jobs/{JOBID}.txt {SAVE_DIR}/log.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
