{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Prepapre dataset with the prepare_dataset notebook, before running this one.\n",
    "\n",
    "Info:\n",
    "- 2777016 baseline\n",
    "  Estimated MAE: Fx 1.542e-03, Iy 1.124e+01, Br 2.100e-03, Bz 2.098e-03, sep 5.614e-03\n",
    "  Inference time -> 28.8 ± 3.2 [μs] | max 311.0 [μs]\n",
    "- 2777018 smaller Pts encoder\n",
    "  Estimated MAE: Fx 1.578e-03, Iy 1.112e+01, Br 2.235e-03, Bz 2.239e-03, sep 5.376e-03\n",
    "  Inference time -> 25.3 ± 2.1 [μs] | max 251.0 [μs]\n",
    "- 2777031 only half of the latent vector multiplied\n",
    "  Estimated MAE: Fx 1.736e-03, Iy 1.188e+01, Br 2.448e-03, Bz 2.488e-03, sep 5.026e-03\n",
    "  Inference time -> 27.2 ± 2.6 [μs] | max 172.0 [μs]\n",
    "- 2777066 even smaller pts encoder + same as 2777031\n",
    "  Estimated MAE: Fx 2.114e-03, Iy 1.263e+01, Br 2.995e-03, Bz 3.010e-03, sep 4.504e-03\n",
    "  Inference time -> 24.0 ± 2.5 [μs] | max 618.0 [μs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from utils import *\n",
    "  \n",
    "# DEV = torch.device(CPU) # for debugging, use cpu\n",
    "os.makedirs(f\"{SAVE_DIR}\", exist_ok=True)\n",
    "print(f'DEV: {DEV}, has_screen: {LOCAL}, job id: {JOBID}')\n",
    "\n",
    "# copy the python training to the directory (for cluster) (for local, it fails silently)\n",
    "os.system(f\"cp train.py {SAVE_DIR}/train.py\")\n",
    "os.system(f\"cp utils.py {SAVE_DIR}/utils.py\")\n",
    "\n",
    "# SMALL, NORM, BIG = \"small\", \"norm\", \"big\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 12 if LOCAL else 200 # number of epochs 100 <-\n",
    "# BATCH_SIZE = 128 # 128 \n",
    "BATCH_SIZE = 4 if LOCAL else 64 # 64 <-\n",
    "\n",
    "# LOAD_PRETRAINED = f'{BEST_MODEL_DIR}/best_{FX}.pth' if LOCAL else None # pretrained model\n",
    "LOAD_PRETRAINED = None # Set it to None if you don't want to load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schedulers\n",
    "from numpy import concatenate as cat, linspace as linsp, logspace as logsp\n",
    "### learning rate\n",
    "# LR = 3e-4*logsp(0, -2, EPOCHS) \n",
    "# LR = 3e-3*np.ones(EPOCHS) \n",
    "# LR = 3e-3*logsp(0, -2, EPOCHS)  \n",
    "LR = 1e-3*np.ones(EPOCHS) if LOCAL else 1e-3*logsp(0, -2, EPOCHS) # <-\n",
    "\n",
    "### loss ratios \n",
    "\n",
    "# ## gso\n",
    "# # RGSO = cat((linsp(1e-6, 3e-3, EPOCHS//2), linsp(3e-3, 0.0, EPOCHS//2))) \n",
    "# MAX_GSO = 3e-3 # 3e-3 <-\n",
    "# # RGSO = cat((linsp(1e-6*MAX_GSO, MAX_GSO, EPOCHS//4), \n",
    "# #             linsp(MAX_GSO, MAX_GSO, EPOCHS//4), \n",
    "# #             linsp(MAX_GSO, 0.0, EPOCHS//4), \n",
    "# #             linsp(0.0, 0.0, EPOCHS//4))) #\n",
    "# # RGSO = cat((MAX_GSO*logsp(-6, 0, EPOCHS//4), \n",
    "# #                           MAX_GSO*logsp(0, 0, EPOCHS//4), \n",
    "# #                           MAX_GSO*logsp(0, -10, EPOCHS//4), \n",
    "# #                           logsp(-12, -12, EPOCHS//4))) \n",
    "# RGSO = np.zeros(EPOCHS) # <- # for now gso disbled is better, probably redundant with y2\n",
    "\n",
    "## Fx\n",
    "MAX_R_FX = 1.0 # 0.5 <-\n",
    "R_FX = MAX_R_FX * np.ones(EPOCHS) \n",
    "# R_FX = cat((np.ones(3*EPOCHS//4)*MAX_R_FX, np.zeros(EPOCHS//4))) #\n",
    "#             linsp(MAX_R_FX, MAX_R_FX, EPOCHS//4), \n",
    "#             linsp(MAX_R_FX, 0.0, EPOCHS//4), \n",
    "# R_FX = MAX_R_FX * logsp(0, -2, EPOCHS) # <-\n",
    "# R_FX = MAX_R_FX * logsp(0, -3, EPOCHS) \n",
    "\n",
    "## Iy\n",
    "MAX_R_IY = 1e-5 # 0.05 <-\n",
    "# MAX_R_IY = 1.0\n",
    "# R_IY = MAX_R_IY * np.ones(EPOCHS) # <- 2\n",
    "# R_IY  = MAX_R_IY * logsp(0, -1, EPOCHS) \n",
    "# R_IY  = MAX_R_IY * logsp(0, -2, EPOCHS) \n",
    "# R_IY  = MAX_R_IY * logsp(0, -3, EPOCHS) \n",
    "R_IY  = MAX_R_IY * logsp(0, -4, EPOCHS) # <- 1\n",
    "# R_IY = np.zeros(EPOCHS)\n",
    "# R_IY = cat((linsp(1e-6*MAX_R_IY, MAX_R_IY, EPOCHS//4),\n",
    "#             linsp(MAX_R_IY, MAX_R_IY, EPOCHS//4), \n",
    "#             linsp(MAX_R_IY, 0.0, EPOCHS//4), \n",
    "#             linsp(0.0, 0.0, EPOCHS//4))) \n",
    "\n",
    "## Br Bz\n",
    "MAX_R_BRZ = 1.0 # 1.0 <-\n",
    "# R_BRZ = MAX_R_BRZ * np.ones(EPOCHS)\n",
    "# R_BRZ = cat((np.zeros(EPOCHS//4),\n",
    "#          MAX_R_BRZ*np.ones(3*EPOCHS//4))) \n",
    "R_BRZ = np.ones(EPOCHS) * MAX_R_BRZ # <-\n",
    "\n",
    "## LCFS/Sep\n",
    "MAX_R_SEP = 1e-2 # 1.0 <-\n",
    "# R_SEP = MAX_R_SEP * np.ones(EPOCHS)\n",
    "# R_SEP = cat((np.zeros(EPOCHS//4),\n",
    "#          MAX_R_SEP*np.ones(3*EPOCHS//4))) \n",
    "# R_SEP = np.ones(EPOCHS) * MAX_R_SEP \n",
    "# R_SEP = MAX_R_SEP * logsp(0, -3, EPOCHS) \n",
    "R_SEP = MAX_R_SEP * logsp(0, -4, EPOCHS) # <-\n",
    "# R_SEP = MAX_R_SEP * logsp(0, -5, EPOCHS) \n",
    "# R_SEP = np.zeros(EPOCHS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks\n",
    "if LOAD_PRETRAINED is not None: assert os.path.exists(LOAD_PRETRAINED), \"Pretrained model does not exist\"\n",
    "assert os.path.exists(TRAIN_DS_PATH), \"Training dataset does not exist\"\n",
    "assert os.path.exists(EVAL_DS_PATH), \"Evaluation dataset does not exist\"\n",
    "assert os.path.exists(SAVE_DIR), \"Save directory does not exist\"\n",
    "assert len(LR) == EPOCHS, \"Learning rate array length does not match epochs\"\n",
    "# assert len(RGSO) == EPOCHS, \"GSO loss ratio array length does not match epochs\"\n",
    "assert len(R_FX) == EPOCHS, \"R_FX loss ratio array length does not match epochs\"\n",
    "assert len(R_IY) == EPOCHS, \"R_IY loss ratio array length does not match epochs\"\n",
    "assert len(R_BRZ) == EPOCHS, \"R_BRZ loss ratio array length does not match epochs\"\n",
    "assert len(R_SEP) == EPOCHS, \"R_SEP loss ratio array length does not match epochs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot schedulers\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.subplot(5, 1, 1)\n",
    "plt.plot(LR)\n",
    "plt.ylim(0, np.max(LR)*1.1)\n",
    "plt.title('Learning Rate')\n",
    "plt.subplot(5, 1, 2)\n",
    "plt.plot(R_FX)\n",
    "plt.ylim(0, np.max(R_FX)*1.1)\n",
    "plt.title('R_FX Loss Ratio')\n",
    "plt.subplot(5, 1, 3)\n",
    "plt.plot(R_IY)\n",
    "plt.ylim(0, np.max(R_IY)*1.1)\n",
    "plt.title('R_IY Loss Ratio')\n",
    "plt.subplot(5, 1, 4)\n",
    "plt.plot(R_BRZ)\n",
    "plt.ylim(0, np.max(R_BRZ)*1.1)\n",
    "plt.title('R_BRZ Loss Ratio')\n",
    "plt.subplot(5, 1, 5)\n",
    "plt.plot(R_SEP)\n",
    "plt.ylim(0, np.max(R_SEP)*1.1)\n",
    "plt.title('R_SEP Loss Ratio')\n",
    "plt.show() if LOCAL else plt.savefig(f\"{SAVE_DIR}/schedulers.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and evaluation datasets\n",
    "train_ds = LiuqeDataset(TRAIN_DS_PATH)\n",
    "val_ds = LiuqeDataset(EVAL_DS_PATH)\n",
    "test_dataset(val_ds)\n",
    "#save the mean_std of the dataset int the save directory\n",
    "np.savez(f\"{SAVE_DIR}/x_mean_std.npz\", x_mean_std=train_ds.x_mean_std.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test net I/O\n",
    "test_network_io()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True) # initialize DataLoader\n",
    "    val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)  \n",
    "    # network\n",
    "    model = FullNet(InputNet(train_ds.x_mean_std), PtsEncoder(), FHead(3), FHead(1), LCFSHead())\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=LR[0])\n",
    "    if LOAD_PRETRAINED is not None: # load pretrained model\n",
    "        model.load_state_dict(torch.load(LOAD_PRETRAINED, map_location=torch.device(CPU)), strict=STRICT_LOAD) # load pretrained model\n",
    "        print(f\"Pretrained model loaded: {LOAD_PRETRAINED}\")\n",
    "    model.to(DEV) # move model to DEV\n",
    "    # loss_fn = F.mse_loss # Mean Squared Error\n",
    "    # loss_fn = percentage_loss # Percentage Loss (relative error)\n",
    "    loss_fn = F.l1_loss # Mean Absolute Error\n",
    "    tlog, elog = np.inf*np.ones((EPOCHS, len(train_dl), len(LOSS_NAMES))), np.inf*np.ones((EPOCHS, len(val_dl), len(LOSS_NAMES))) # init log\n",
    "    start_time = time() # start time\n",
    "    print(\"Ep:    || fx  | iy  | br  | bz  | sep || r:fx| r:iy|r:brz|r:sep| LR  || t  | eta| improved    \")\n",
    "    for ep in range(EPOCHS): # epochs\n",
    "        epoch_time = time()\n",
    "        for pg in opt.param_groups: pg['lr'] = LR[ep] # update learning rate\n",
    "        model.train()\n",
    "        for ib, batch in enumerate(train_dl):\n",
    "            if train_ds.on_dev: x, p, fx, iy, br, bz, sep = batch # unpack batch\n",
    "            else: x, p, fx, iy, br, bz, sep = map(lambda t: t.to(DEV), batch) # move to DEV\n",
    "            opt.zero_grad() # zero gradients\n",
    "            rtp, iyp, sepp = model(x, p) # forward pass\n",
    "            fxp, brp, bzp = rtp[:,:,0], rtp[:,:,1], rtp[:,:,2] # unpack outputs\n",
    "            # gso, gsop = calc_gso_batch(y1, r, z, dev=DEV), calc_gso_batch(yp1, r, z, dev=DEV) # calculate grad shafranov\n",
    "            # losses\n",
    "            l_fx = loss_fn(fx, fxp) # fx loss\n",
    "            l_iy = loss_fn(iy, iyp) # iy loss\n",
    "            l_br = loss_fn(br, brp) # br loss\n",
    "            l_bz = loss_fn(bz, bzp) # bz loss\n",
    "            l_sep = F.mse_loss(sep, sepp) # lcfs loss\n",
    "            # lgso = loss_fn(gsop, gso) # PINN loss on grad shafranov\n",
    "            loss = R_FX[ep]*l_fx + R_IY[ep]*l_iy + R_BRZ[ep]*l_br + R_BRZ[ep]*l_bz + R_SEP[ep]*l_sep#+ RGSO[ep]*lgso # total loss\n",
    "            loss.backward() # backprop\n",
    "            opt.step() # update weights\n",
    "            tlog[ep, ib] = (l_fx.item(), l_iy.item(), l_br.item(), l_bz.item(), l_sep.item()) # save batch losses\n",
    "        model.eval() # evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for ib, batch in enumerate(val_dl):\n",
    "                if val_ds.on_dev: x, p, fx, iy, br, bz, sep = batch # unpack batch\n",
    "                else: x, p, fx, iy, br, bz, sep = map(lambda t: t.to(DEV), batch) # move to DEV\n",
    "                rtp, iyp, sepp = model(x, p) # forward pass\n",
    "                fxp, brp, bzp = rtp[:,:,0], rtp[:,:,1], rtp[:,:,2]\n",
    "                # losses\n",
    "                l_fx = loss_fn(fx, fxp) # fx loss\n",
    "                l_iy = loss_fn(iy, iyp) # iy loss\n",
    "                l_br = loss_fn(br, brp) # br loss\n",
    "                l_bz = loss_fn(bz, bzp) # bz loss\n",
    "                l_sep = F.mse_loss(sep, sepp) # lcfs loss\n",
    "                # lgso = loss_fn(gsop, gso) # PINN loss on grad shafranov\n",
    "                assert not torch.isnan(l_fx) and not torch.isnan(l_iy) and not torch.isnan(l_br) and not torch.isnan(l_sep), \"Loss is NaN\"\n",
    "                elog[ep, ib] = (l_fx.item(), l_iy.item(), l_br.item(), l_bz.item(), l_sep.item())\n",
    "        # save model if improved        \n",
    "        endp = \"\" \n",
    "        epoch_losses = np.mean(elog[ep,:,:], axis=0) # epoch losses: mean across batchess\n",
    "        best_losses = np.min(np.mean(elog[:ep,:,:], axis=1), axis=0) if ep > 0 else epoch_losses # best losses: min across epochs\n",
    "        for el, bl, n in zip(epoch_losses, best_losses, LOSS_NAMES):\n",
    "            if el <= bl: torch.save(model.state_dict(), model_path(n)); endp+=f\"*{n}\"\n",
    "        # print progress\n",
    "        print(f\"{ep+1:03d}/{EPOCHS:02d}||\" +\n",
    "            '|'.join([f\"{v:.0e}\" for v in epoch_losses]) + \"||\" +\n",
    "            # '|'.join([f\"{v:.3f}\" for v in epoch_losses]) + \"||\" +\n",
    "            f\"{R_FX[ep]:.0e}|{R_IY[ep]:.0e}|{R_BRZ[ep]:.0e}|{R_SEP[ep]:.0e}|{LR[ep]:.0e}||\" +\n",
    "            f\"{int(time()-epoch_time):03d}s|{int((time()-start_time)*(EPOCHS-ep)/(ep+1)/60):03d}m|\", end=endp+'\\n')\n",
    "        # if ep >= 10 and best_losses[2] > 0.1: return False \n",
    "\n",
    "    tlosses, elosses = np.mean(tlog, axis=1), np.mean(elog, axis=1) # losses across epochs\n",
    "    print(f\"Training time: {(time()-start_time)/60:.0f}mins\")\n",
    "    print(\"Best MSE: \" + \", \".join([f\"{n} {bl:.3e} (ep {be})\" for n, bl, be in zip(LOSS_NAMES, np.min(elosses, axis=0), np.argmin(elosses, axis=0))]))\n",
    "    print(\"Estimated MAE: \" + \", \".join([f\"{n} {np.sqrt(np.min(np.mean(elog[:,:,i], axis=1))):.3e}\" for i, n in enumerate(LOSS_NAMES)]))\n",
    "    np.save(f\"{SAVE_DIR}/train_losses.npy\", tlosses) # save training losses\n",
    "    np.save(f\"{SAVE_DIR}/eval_losses.npy\", elosses) # save evaluation losses\n",
    "    return True\n",
    "\n",
    "# train the model (multiple attempts)\n",
    "for i in range(20): \n",
    "    print('Starting training...')\n",
    "    success = train()\n",
    "    if success: break\n",
    "    else: print(f\"Convergence failed, retrying... {i+1}/20\")\n",
    "# if not success: delete the files and exit:\n",
    "if not success: os.system(f\"rm -rf {SAVE_DIR}\")\n",
    "assert success, \"Training failed, no model saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "print(\"Plotting losses...\")\n",
    "fig, ax = plt.subplots(2, len(LOSS_NAMES), figsize=(4*len(LOSS_NAMES), 8))\n",
    "# Load losses\n",
    "train_loss = np.load(f\"{SAVE_DIR}/train_losses.npy\")\n",
    "eval_loss = np.load(f\"{SAVE_DIR}/eval_losses.npy\")\n",
    "for i, name in enumerate(LOSS_NAMES):\n",
    "    # Linear scale\n",
    "    ax[0,i].plot(train_loss[:,i], label='train')# y1_mean_std = np.concatenate([μy1, Σy1], axis=0)\n",
    "# y2_mean_std = np.concatenate([μy2, Σy2], axis=0)\n",
    "    ax[0,i].plot(eval_loss[:,i], label='eval')\n",
    "    ax[0,i].set_title(f\"{name}\")\n",
    "    ax[0,i].set_xlabel(\"Epoch\")\n",
    "    ax[0,i].set_ylabel(\"Loss\")\n",
    "    ax[0,i].legend(); ax[0,i].grid(True)\n",
    "    # Log scale\n",
    "    ax[1,i].plot(train_loss[:,i], label='train')\n",
    "    ax[1,i].plot(eval_loss[:,i], label='eval')\n",
    "    ax[1,i].set_yscale('log')\n",
    "    ax[1,i].set_title(f\"{name} (log)\")\n",
    "    ax[1,i].set_xlabel(\"Epoch\")\n",
    "    ax[1,i].set_ylabel(\"Loss\")\n",
    "    ax[1,i].legend(); ax[1,i].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show() if LOCAL else plt.savefig(f\"{SAVE_DIR}/losses.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing network outputs\n",
    "try:\n",
    "# if True:\n",
    "    print(\"Testing network output...\")\n",
    "    for ln in LOSS_NAMES:\n",
    "        model = FullNet(InputNet(train_ds.x_mean_std), PtsEncoder(), FHead(3), FHead(1), LCFSHead())\n",
    "        model.load_state_dict(torch.load(model_path(ln), map_location=torch.device(CPU))) # load pretrained model\n",
    "        plot_network_outputs(val_ds, model, title=ln, show_lcfs=True) # plot network outputs  \n",
    "except Exception as e:\n",
    "    print(f\"Error in testing network output: {e}\")\n",
    "    print(\"Skipping network output testing...\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test LCFS net (not the target anymore)\n",
    "try:\n",
    "    m = FullNet(InputNet(train_ds.x_mean_std), PtsEncoder(), FHead(3), FHead(1), LCFSHead())\n",
    "    m.load_state_dict(torch.load(model_path(SEP), map_location=torch.device(CPU))) # load pretrained model\n",
    "    lcfs = LCFSNet(m.input_net, m.lcfs_head)\n",
    "    plot_lcfs_net_out(val_ds, lcfs, title='') # plot LCFS net outputs\n",
    "except Exception as e:\n",
    "    print(f\"Error in testing LCFS net: {e}\")\n",
    "    print(\"Skipping LCFS net testing...\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert best LiuqRTNet network to ONNX\n",
    "try: \n",
    "    # raise NotImplementedError(\"ONNX conversion is not implemented yet\") \n",
    "    print(\"Converting best l_br network to ONNX...\")\n",
    "    m = FullNet(InputNet(train_ds.x_mean_std), PtsEncoder(), FHead(3), FHead(1), LCFSHead())\n",
    "    m.load_state_dict(torch.load(model_path(FX), map_location=torch.device(CPU))) # load pretrained model\n",
    "    net = LiuqeRTNet(m.input_net, m.pts_enc, m.rt_head) # create LiuqeRTNet\n",
    "    try: convert_to_onnx_dyn(net) # dynamic axes\n",
    "    except Exception as e: print(f\"Error exporting to ONNX: {e}\")\n",
    "    try: convert_to_onnx_static(net) # static axes\n",
    "    except Exception as e: print(f\"Error exporting to ONNX: {e}\")\n",
    "    # to keep the net.onnx name, just copy net_dyn.onnx to net.onnx\n",
    "    onnx_net_path = f\"{SAVE_DIR}/net.onnx\"\n",
    "    os.system(f\"cp {SAVE_DIR}/net_dyn.onnx {onnx_net_path}\")\n",
    "    assert os.path.exists(onnx_net_path), f\"ONNX model not saved to {onnx_net_path}\"\n",
    "    print(f'ONNX model saved to {onnx_net_path}')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in ONNX conversion: {e}\")\n",
    "    print(\"ONNX conversion failed\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{JOBID} done\")\n",
    "if not LOCAL: sleep(30) # wait for files to update (for cluster)\n",
    "#copy the log file to the folder\n",
    "os.system(f\"cp jobs/{JOBID}.txt {SAVE_DIR}/log.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
