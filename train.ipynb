{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Prepapre dataset with the prepare_dataset notebook, before running this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Module, Linear, Conv2d, MaxPool2d, BatchNorm2d, ReLU, Sequential, ConvTranspose2d\n",
    "from tqdm import tqdm\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from utils import *\n",
    "try: \n",
    "    JOBID = os.environ[\"SLURM_JOB_ID\"] # get job id from slurm, when training on cluster\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") # nvidia\n",
    "    HAS_SCREEN = False # for plotting or saving images\n",
    "except:\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\") # apple silicon / cpu\n",
    "    JOBID = \"local\"\n",
    "    HAS_SCREEN = True\n",
    "SAVE_DIR = f\"data/{JOBID}\"  \n",
    "# device = torch.device('cpu') # for debugging, use cpu\n",
    "os.makedirs(f\"{SAVE_DIR}\", exist_ok=True)\n",
    "os.makedirs(f\"{SAVE_DIR}/models\", exist_ok=True)\n",
    "print(f'device: {device}, has_screen: {HAS_SCREEN}, job id: {JOBID}')\n",
    "\n",
    "# copy the python training to the directory (for cluster) (for local, it fails silently)\n",
    "os.system(f\"cp train.py {SAVE_DIR}/train.py\")\n",
    "os.system(f\"cp utils.py {SAVE_DIR}/utils.py\")\n",
    "\n",
    "def to_tensor(x, device=torch.device(\"cpu\")): return torch.tensor(x, dtype=torch.float32, device=device)\n",
    "\n",
    "# SMALL, NORM, BIG = \"small\", \"norm\", \"big\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPOCHS = 100 # number of epochs # 1000\n",
    "BATCH_SIZE = 128 # 128 <-\n",
    "\n",
    "# MODEL_NAME = SMALL\n",
    "# MODEL_NAME = NORM\n",
    "# MODEL_NAME = BIG\n",
    "\n",
    "LOAD_PRETRAINED = None # Set it to None if you don't want to load pretrained model\n",
    "# LOAD_PRETRAINED = \"trained_models/pretrained_1809761.pth\" # norm model\n",
    "# LOAD_PRETRAINED = \"trained_models/pretrained_small_1810888.pth\" # small model\n",
    "# LOAD_PRETRAINED = \"trained_models/pretrained_big_1811142.pth\" # big model\n",
    "\n",
    "# LEARNING_RATE = 3e-4*np.logspace(0, -2, EPOCHS) \n",
    "# LEARNING_RATE = 3e-3*np.ones(EPOCHS) \n",
    "# LEARNING_RATE = 3e-3*np.logspace(0, -2, EPOCHS)  # <-\n",
    "LEARNING_RATE = 1e-3*np.logspace(0, -2, EPOCHS) \n",
    "\n",
    "# GSO_LOSS_RATIO = np.concatenate((np.linspace(1e-6, 3e-3, EPOCHS//2), np.linspace(3e-3, 0.0, EPOCHS//2))) \n",
    "MAX_GSO = 5e-3 # 3e-3 <-\n",
    "GSO_LOSS_RATIO = np.concatenate((np.linspace(1e-6, MAX_GSO, EPOCHS//4), \n",
    "                                 np.linspace(MAX_GSO, MAX_GSO, EPOCHS//4), \n",
    "                                 np.linspace(MAX_GSO, 0.0, EPOCHS//4), \n",
    "                                 np.linspace(0.0, 0.0, EPOCHS//4))) \n",
    "# GSO_LOSS_RATIO = np.concatenate((MAX_GSO*np.logspace(-6, 0, EPOCHS//4), \n",
    "#                                  MAX_GSO*np.logspace(0, 0, EPOCHS//4), \n",
    "#                                  MAX_GSO*np.logspace(0, -10, EPOCHS//4), \n",
    "#                                  np.logspace(-12, -12, EPOCHS//4))) \n",
    "# GSO_LOSS_RATIO = np.zeros(EPOCHS) \n",
    "\n",
    "TRAIN_DS_PATH = \"dss/train_ds.npz\" # generated from prepapre_dataset\n",
    "EVAL_DS_PATH = \"dss/eval_ds.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks\n",
    "if LOAD_PRETRAINED is not None: assert os.path.exists(LOAD_PRETRAINED), \"Pretrained model does not exist\"\n",
    "assert os.path.exists(TRAIN_DS_PATH), \"Training dataset does not exist\"\n",
    "assert os.path.exists(EVAL_DS_PATH), \"Evaluation dataset does not exist\"\n",
    "assert os.path.exists(SAVE_DIR), \"Save directory does not exist\"\n",
    "assert len(LEARNING_RATE) == EPOCHS, \"Learning rate array length does not match epochs\"\n",
    "assert len(GSO_LOSS_RATIO) == EPOCHS, \"GSO loss ratio array length does not match epochs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot schedulers: lr + gso loss ratio\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 3))\n",
    "ax[0].set_title(\"Learning Rate [Log]\")\n",
    "ax[0].plot(LEARNING_RATE, color=\"red\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Learning Rate\")\n",
    "ax[0].set_yscale(\"log\")\n",
    "ax[1].set_title(\"GSO Loss Ratio\")\n",
    "ax[1].plot(GSO_LOSS_RATIO, color=\"red\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"GSO Loss Ratio\")\n",
    "plt.tight_layout()\n",
    "plt.show() if HAS_SCREEN else plt.savefig(f\"{SAVE_DIR}/schedulers.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlaNetDataset(Dataset):\n",
    "    def __init__(self, ds_mat_path):\n",
    "        d = np.load(ds_mat_path)\n",
    "        # output: magnetic flux, transposed (matlab is column-major)\n",
    "        self.X =  to_tensor(d[\"X\"]) # (n, NIN) # inputs: currents + measurements + profiles\n",
    "        self.Y =  to_tensor(d[\"Y\"]).view(-1,1,NGZ,NGR)\n",
    "        self.r = to_tensor(d[\"r\"]).view(-1,1,NGZ,NGR) # radial position of pixels \n",
    "        self.z = to_tensor(d[\"z\"]).view(-1,1,NGZ,NGR) # vertical position of pixels \n",
    "        #move to device (doable bc the dataset is fairly small, check memory usage)\n",
    "        self.Y, self.X, self.r, self.z = self.Y.to(device), self.X.to(device), self.r.to(device), self.z.to(device)\n",
    "        total_memory = sum([x.element_size()*x.nelement() for x in [self.Y, self.X, self.r, self.z]])\n",
    "        print(f\"Dataset: {len(self)}, memory: {total_memory/1024**2:.0f} MB\")\n",
    "    def __len__(self): return len(self.Y)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.Y[idx], self.r[idx], self.z[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset\n",
    "ds = PlaNetDataset(EVAL_DS_PATH)\n",
    "print(f\"Dataset length: {len(ds)}\")\n",
    "print(f\"Input shape: {ds[0][0].shape}\")\n",
    "print(f\"Output shape: {ds[0][1].shape}\")\n",
    "n_plot = 10\n",
    "print(len(ds))\n",
    "idxs = np.random.randint(0, len(ds), n_plot)\n",
    "fig, axs = plt.subplots(1, n_plot, figsize=(3*n_plot, 5))\n",
    "for i, j in enumerate(idxs):\n",
    "    Y, rr, zz = ds[j][1].cpu().numpy().squeeze(), ds[j][2].cpu().numpy().squeeze(), ds[j][3].cpu().numpy().squeeze()\n",
    "    axs[i].contourf(rr, zz, Y, 100, cmap=\"inferno\")\n",
    "    axs[i].plot(VESS[:,0], VESS[:,1], color=\"white\", linewidth=2)\n",
    "    # axs[i].contour(rr, zz, -Y, 20, colors=\"black\", linestyles=\"dotted\")\n",
    "    fig.colorbar(axs[i].collections[0], ax=axs[i])\n",
    "    axs[i].axis(\"off\")\n",
    "    axs[i].set_aspect(\"equal\")\n",
    "plt.show() if HAS_SCREEN else plt.savefig(f\"{SAVE_DIR}/dataset.png\")\n",
    "\n",
    "# now do the same fot the input:\n",
    "fig, axs = plt.subplots(1, n_plot, figsize=(3*n_plot, 5))\n",
    "for i, j in enumerate(idxs):\n",
    "    inputs = ds[j][0].cpu().numpy().squeeze()\n",
    "    if USE_CURRENTS: axs[i].plot(inputs[:19], label=\"currents\")\n",
    "    if USE_MAGNETIC: axs[i].plot(inputs[19:57], label=\"magnetic\")\n",
    "    if USE_PROFILES: axs[i].plot(inputs[57:], label=\"profiles\")\n",
    "    axs[i].legend()\n",
    "    axs[i].set_title(f\"Sample {j}\")\n",
    "    axs[i].set_xlabel(\"Input index\")\n",
    "plt.show() if HAS_SCREEN else plt.savefig(f\"{SAVE_DIR}/dataset_inputs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and evaluation datasets\n",
    "train_ds = PlaNetDataset(TRAIN_DS_PATH)\n",
    "val_ds = PlaNetDataset(EVAL_DS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions\n",
    "# custom trainable swish\n",
    "class Swish(Module):\n",
    "    def __init__(self, β=1.0): \n",
    "        super(Swish, self).__init__()\n",
    "        # self.β = torch.nn.Parameter(torch.tensor(β, device=device), requires_grad=True)\n",
    "        self.β = torch.nn.Parameter(torch.tensor(β), requires_grad=True)\n",
    "    def forward(self, x): \n",
    "        return x*torch.sigmoid(self.β*x)\n",
    "    def to(self, device): \n",
    "        self.β = self.β.to(device)\n",
    "        return super().to(device)\n",
    "\n",
    "# Λ = ReLU() # ReLU activation function\n",
    "# Λ = Swish() # Swish activation function\n",
    "\n",
    "# class Λ(Module): # relu\n",
    "#     def __init__(self): super(Λ, self).__init__()\n",
    "#     def forward(self, x): return torch.relu(x)\n",
    "\n",
    "class Λ(Module): # swish\n",
    "    def __init__(self): \n",
    "        super(Λ, self).__init__()\n",
    "        self.β = torch.nn.Parameter(torch.tensor(1.0), requires_grad=True)\n",
    "    def forward(self, x): return x*torch.sigmoid(self.β*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EasyPlaNet(Module): # Paper net: branch + trunk conenction and everything\n",
    "    def __init__(self, input_size=NIN, latent_size=32, grid_size=(NGZ,NGR)):\n",
    "        super(EasyPlaNet, self).__init__()\n",
    "        assert latent_size % 2 == 0, \"latent size should be even\"\n",
    "        self.input_size, self.latent_size, self.grid_size = input_size, latent_size, grid_size\n",
    "        self.fgs = grid_size[0]*grid_size[1] # fgs: flattened grid size\n",
    "        #branch\n",
    "        self.branch = Sequential(\n",
    "            View(-1, input_size),\n",
    "            Linear(input_size, 64), Λ(),\n",
    "            Linear(64, 32), Λ(),\n",
    "            Linear(32, latent_size), Λ(),\n",
    "        )\n",
    "        #trunk\n",
    "        def trunk_block(): \n",
    "            return  Sequential(\n",
    "                View(-1, self.fgs),\n",
    "                Linear(self.fgs, 32), Λ(),\n",
    "                Linear(32, latent_size//2), Λ(),\n",
    "            )\n",
    "        self.trunk_r, self.trunk_z = trunk_block(), trunk_block()\n",
    "        # head\n",
    "        self.head = Sequential(\n",
    "            Linear(latent_size, 64), Λ(),\n",
    "            Linear(64, self.fgs), Λ(),\n",
    "            View(-1, 1, *self.grid_size),\n",
    "        )\n",
    "    def forward(self, xb, r, z):\n",
    "        assert xb.shape[1] == self.input_size, f\"branch input shape {xb.shape} != {self.input_size}\"\n",
    "        #branch net\n",
    "        xb = self.branch(xb)\n",
    "        assert xb.shape[1] == self.latent_size, f\"branch output shape {xb.shape} != {self.latent_size}\"\n",
    "        #trunk net\n",
    "        r, z = self.trunk_r(r), self.trunk_z(z) \n",
    "        xt = torch.cat((r, z), 1) # concatenate\n",
    "        assert xt.shape[1] == self.latent_size, f\"trunk output shape {xt.shape} != {self.latent_size}\"\n",
    "        x = xt * xb # multiply trunk and branch\n",
    "        x = self.head(x) # head net\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model inputs / outputs\n",
    "x, rr, zz = (torch.rand(1, NIN), torch.rand(1, 1, NGZ, NGR), torch.rand(1, 1, NGZ, NGR))\n",
    "net = EasyPlaNet()\n",
    "y = net(x, rr, zz)\n",
    "print(f\"in: {x.shape}, {rr.shape}, {zz.shape}, \\nout: {y.shape}\")\n",
    "n_sampl = 7\n",
    "nx, rr, zz = torch.rand(n_sampl, NIN), torch.rand(n_sampl, 1, NGZ, NGR), torch.rand(n_sampl, 1, NGZ, NGR)\n",
    "ny = net(nx, rr, zz)\n",
    "print(f\"in: {nx.shape}, {rr.shape}, {zz.shape}, \\nout: {ny.shape}\")\n",
    "assert ny.shape == (n_sampl, 1, NGZ, NGR), f\"Wrong output shape: {ny.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True) # initialize DataLoader\n",
    "    val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)  \n",
    "    model = EasyPlaNet()  # instantiate model\n",
    "    if LOAD_PRETRAINED is not None: # load pretrained model\n",
    "        model.load_state_dict(torch.load(LOAD_PRETRAINED, map_location=torch.device(\"cpu\"))) # load pretrained model\n",
    "        print(f\"Pretrained model loaded: {LOAD_PRETRAINED}\")\n",
    "    model.to(device) # move model to device\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE[0])\n",
    "    loss_fn = torch.nn.MSELoss() # Mean Squared Error Loss\n",
    "    tlog_tot, tlog_mse, tlog_gso, elog_tot, elog_mse, elog_gso = [], [], [], [], [], [] # logs for losses\n",
    "    start_time = time() # start time\n",
    "    for ep in range(EPOCHS): # epochs\n",
    "        epoch_time = time()\n",
    "        for pg in optimizer.param_groups: pg['lr'] = LEARNING_RATE[ep] # update learning rate\n",
    "        model.train()\n",
    "        trainloss, evalloss = [], []\n",
    "        for input, Y, rr, zz in train_dl:\n",
    "            optimizer.zero_grad() # zero gradients\n",
    "            psi_pred = model(input, rr, zz) # forward pass\n",
    "            gso, gso_pred = calc_gso_batch(Y, rr, zz, dev=device), calc_gso_batch(psi_pred, rr, zz, dev=device) # calculate grad shafranov\n",
    "            mse_loss = loss_fn(psi_pred, Y) # mean squared error loss on Y\n",
    "            gso_loss = loss_fn(gso_pred, gso) # PINN loss on grad shafranov\n",
    "            loss = (1-GSO_LOSS_RATIO[ep])*mse_loss + GSO_LOSS_RATIO[ep]*gso_loss # total loss\n",
    "            loss.backward() # backprop\n",
    "            optimizer.step() # update weights \n",
    "            trainloss.append((loss.item(), mse_loss.item(), gso_loss.item())) # save batch losses\n",
    "        model.eval() # evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for input, Y, rr, zz in val_dl:\n",
    "                psi_pred = model(input, rr, zz)\n",
    "                gso, gso_pred = calc_gso_batch(Y, rr, zz, dev=device), calc_gso_batch(psi_pred, rr, zz, dev=device)\n",
    "                mse_loss = loss_fn(psi_pred, Y)\n",
    "                gso_loss = loss_fn(gso_pred, gso)\n",
    "                loss = (1-GSO_LOSS_RATIO[ep])*mse_loss + GSO_LOSS_RATIO[ep]*gso_loss # total loss\n",
    "                evalloss.append((loss.item(), mse_loss.item(), gso_loss.item()))\n",
    "        tloss_tot, tloss_mse, tloss_gso = map(lambda x: sum(x)/len(x), zip(*trainloss))\n",
    "        eloss_tot, eloss_mse, eloss_gso = map(lambda x: sum(x)/len(x), zip(*evalloss))\n",
    "        # save model if improved        \n",
    "        endp = \"\\n\" \n",
    "        if eloss_tot <= min(elog_tot, default=eloss_tot): \n",
    "            torch.save(model.state_dict(), f\"{SAVE_DIR}/mg_planet_tot.pth\"); endp=\" [tot]\\n\"\n",
    "        if eloss_mse <= min(elog_mse, default=eloss_mse):\n",
    "            torch.save(model.state_dict(), f\"{SAVE_DIR}/mg_planet_mse.pth\"); endp=\" [mse]\\n\"\n",
    "        if eloss_gso <= min(elog_gso, default=eloss_gso):\n",
    "            torch.save(model.state_dict(), f\"{SAVE_DIR}/mg_planet_gso.pth\"); endp=\" [gso]\\n\"\n",
    "        tlog_tot.append(tloss_tot); tlog_mse.append(tloss_mse); tlog_gso.append(tloss_gso)\n",
    "        elog_tot.append(eloss_tot); elog_mse.append(eloss_mse); elog_gso.append(eloss_gso) \n",
    "        print(f\"[{ep+1}/{EPOCHS}] \"\n",
    "            f\"Eval -> tot {eloss_tot:.4f}, mse {eloss_mse:.4f}, gso {eloss_gso:.2e}, \" + \n",
    "            f\"lr {LEARNING_RATE[ep]:.1e}, r {GSO_LOSS_RATIO[ep]:.1e}, {time()-epoch_time:.0f}s, eta {(time()-start_time)*(EPOCHS-ep)/(ep+1)/60:.0f}m\", end=endp,  flush=True)\n",
    "        if ep >= 10 and ((eloss_gso > 30.0 and GSO_LOSS_RATIO[ep] > 0.01) or eloss_mse > .2): return False, () # stop training, if not converging, try again\n",
    "    print(f\"Training time: {(time()-start_time)/60:.0f}mins\")\n",
    "    print(f\"Best losses: tot {min(elog_tot):.3e}, mse {min(elog_mse):.3e}, gso {min(elog_gso):.4f}\")\n",
    "    print(f\"Estimated MAE: tot {np.sqrt(min(elog_tot)):.3e}, mae {np.sqrt(min(elog_mse)):.3e}, gso {np.sqrt(min(elog_gso)):.4f}\")\n",
    "    for l, n in zip([tlog_tot, tlog_mse, tlog_gso], [\"tot\", \"mse\", \"gso\"]): np.save(f\"{SAVE_DIR}/train_{n}_losses.npy\", l) # save losses\n",
    "    for l, n in zip([elog_tot, elog_mse, elog_gso], [\"tot\", \"mse\", \"gso\"]): np.save(f\"{SAVE_DIR}/eval_{n}_losses.npy\", l) # save losses\n",
    "    return True, (tlog_tot, tlog_mse, tlog_gso, elog_tot, elog_mse, elog_gso)\n",
    "\n",
    "# train the model (multiple attempts)\n",
    "for i in range(20): \n",
    "    success, logs = train()\n",
    "    if success: tlog_tot, tlog_mse, tlog_gso, elog_tot, elog_mse, elog_gso = logs; break\n",
    "    else: print(f\"Convergence failed, retrying... {i+1}/20\")\n",
    "# if not success: delete the files and exit:\n",
    "if not success: \n",
    "    os.system(f\"rm -rf {SAVE_DIR}\")\n",
    "    assert success, \"Training failed, no model saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "fig, ax = plt.subplots(2, 3, figsize=(12, 6))\n",
    "ce, ct = \"yellow\", \"red\"\n",
    "lw = 1.0\n",
    "ax[0,0].set_title(\"TOT Loss\")\n",
    "ax[0,0].plot(tlog_tot, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[0,0].plot(elog_tot, color=ce, label=\"eval\", linewidth=lw)\n",
    "ax[0,1].set_title(\"MSE Loss\")\n",
    "ax[0,1].plot(tlog_mse, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[0,1].plot(elog_mse, color=ce, label=\"eval\", linewidth=lw)\n",
    "ax[0,2].set_title(\"GSO Loss\")\n",
    "ax[0,2].plot(tlog_gso, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[0,2].plot(elog_gso, color=ce, label=\"eval\", linewidth=lw)\n",
    "#now the same but with log scale\n",
    "ax[1,0].set_title(\"TOT Loss (log)\")\n",
    "ax[1,0].plot(tlog_tot, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[1,0].plot(elog_tot, color=ce, label=\"eval\", linewidth=lw)\n",
    "ax[1,0].set_yscale(\"log\")\n",
    "ax[1,0].grid(True, which=\"both\", axis=\"y\")\n",
    "\n",
    "ax[1,1].set_title(\"MSE Loss (log)\")\n",
    "ax[1,1].plot(tlog_mse, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[1,1].plot(elog_mse, color=ce, label=\"eval\", linewidth=lw)\n",
    "ax[1,1].set_yscale(\"log\")\n",
    "ax[1,1].grid(True, which=\"both\", axis=\"y\")\n",
    "\n",
    "ax[1,2].set_title(\"GSO Loss (log)\")\n",
    "ax[1,2].plot(tlog_gso, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[1,2].plot(elog_gso, color=ce, label=\"eval\", linewidth=lw)\n",
    "ax[1,2].set_yscale(\"log\")\n",
    "ax[1,2].grid(True, which=\"both\", axis=\"y\")\n",
    "plt.suptitle(f\"[{JOBID}] Training losses\")\n",
    "for a in ax.flatten(): a.legend(); a.set_xlabel(\"Epoch\"); a.set_ylabel(\"Loss\")\n",
    "plt.tight_layout()\n",
    "plt.show() if HAS_SCREEN else plt.savefig(f\"{SAVE_DIR}/losses.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing network output\n",
    "for titl, best_model_path in zip([\"TOT\",\"MSE\", \"GSO\"], [\"mg_planet_tot.pth\", \"mg_planet_mse.pth\", \"mg_planet_gso.pth\"]):\n",
    "    model = EasyPlaNet()\n",
    "    model.load_state_dict(torch.load(f\"{SAVE_DIR}/{best_model_path}\"))\n",
    "    model.eval()\n",
    "    ds = val_ds\n",
    "    os.makedirs(f\"{SAVE_DIR}/imgs\", exist_ok=True)\n",
    "    N_PLOTS = 2 if HAS_SCREEN else 50\n",
    "    for i in np.random.randint(0, len(ds), N_PLOTS):  \n",
    "        fig, axs = plt.subplots(2, 5, figsize=(15, 9))\n",
    "        input, psi_ds, rr, zz = ds[i]\n",
    "        input, psi_ds, rr, zz = input.to('cpu'), psi_ds.to('cpu'), rr.to('cpu'), zz.to('cpu')\n",
    "        input, psi_ds, rr, zz = input.reshape(1,-1), psi_ds.reshape(1,1,NGZ,NGR), rr.reshape(1,1,NGZ,NGR), zz.reshape(1,1,NGZ,NGR)\n",
    "        psi_pred = model(input, rr, zz)\n",
    "        gso, gso_pred = calc_gso_batch(psi_ds, rr, zz), calc_gso_batch(psi_pred, rr, zz)\n",
    "        gso, gso_pred = gso.detach().numpy().reshape(NGZ,NGR), gso_pred.detach().numpy().reshape(NGZ,NGR)\n",
    "        gso_min, gso_max = np.min([gso, gso_pred]), np.max([gso, gso_pred])\n",
    "        gso_levels = np.linspace(gso_min, gso_max, 13, endpoint=True)\n",
    "        # gso_pred = np.clip(gso_pred, gso_range[1], gso_range[0]) # clip to gso range\n",
    "        \n",
    "        psi_pred = psi_pred.detach().numpy().reshape(NGZ,NGR)\n",
    "        psi_ds = psi_ds.detach().numpy().reshape(NGZ,NGR)\n",
    "        rr, zz = rr.view(NGZ,NGR).detach().numpy(), zz.view(NGZ,NGR).detach().numpy()\n",
    "        ext = [ds.r.min(), ds.r.max(), ds.z.min(), ds.z.max()]\n",
    "        bmin, bmax = np.min([psi_ds, psi_pred]), np.max([psi_ds, psi_pred]) # min max Y\n",
    "        blevels = np.linspace(bmin, bmax, 13, endpoint=True)\n",
    "        # ψ_msex = (psi_ds - psi_pred)**2\n",
    "        # gso_msex = (gso - gso_pred)**2\n",
    "        ψ_mae = np.abs(psi_ds - psi_pred)\n",
    "        gso_mae = np.abs(gso - gso_pred)\n",
    "        lev0 = np.linspace(0, 5.0, 13, endpoint=True)\n",
    "        lev1 = np.linspace(0, 0.5, 13, endpoint=True) \n",
    "        lev2 = np.linspace(0, 0.05, 13, endpoint=True)\n",
    "        lev3 = np.linspace(0, 0.005, 13, endpoint=True)\n",
    "        ε = 1e-12\n",
    "\n",
    "        im00 = axs[0,0].contourf(rr, zz, psi_ds, blevels, cmap=\"inferno\")\n",
    "        axs[0,0].set_title(\"Actual\")\n",
    "        axs[0,0].set_aspect('equal')\n",
    "        axs[0,0].set_ylabel(\"ψ\")\n",
    "        fig.colorbar(im00, ax=axs[0,0]) \n",
    "        im01 = axs[0,1].contourf(rr, zz, psi_pred, blevels, cmap=\"inferno\")\n",
    "        axs[0,1].set_title(\"Predicted\")\n",
    "        fig.colorbar(im01, ax=axs[0,1])\n",
    "        im02 = axs[0,2].contour(rr, zz, psi_ds, blevels, linestyles='dashed', cmap=\"inferno\")\n",
    "        axs[0,2].contour(rr, zz, psi_pred, blevels, cmap=\"inferno\")\n",
    "        axs[0,2].set_title(\"Contours\")\n",
    "        fig.colorbar(im02, ax=axs[0,2])\n",
    "        im03 = axs[0,3].contourf(rr, zz, np.clip(ψ_mae, lev2[0]+ε, lev2[-1]-ε), lev2, cmap=\"inferno\")\n",
    "        axs[0,3].set_title(\"MAE 0.05\")\n",
    "        fig.colorbar(im03, ax=axs[0,3])\n",
    "        im04 = axs[0,4].contourf(rr, zz, np.clip(ψ_mae, lev3[0]+ε, lev3[-1]-ε), lev3, cmap=\"inferno\")\n",
    "        axs[0,4].set_title(\"MAE 0.005\")\n",
    "        fig.colorbar(im04, ax=axs[0,4])\n",
    "\n",
    "        im10 = axs[1,0].contourf(rr, zz, gso, gso_levels, cmap=\"inferno\")\n",
    "        axs[1,0].set_ylabel(\"GSO\")\n",
    "        fig.colorbar(im10, ax=axs[1,0])\n",
    "        im11 = axs[1,1].contourf(rr, zz, gso_pred, gso_levels, cmap=\"inferno\")\n",
    "        fig.colorbar(im11, ax=axs[1,1])\n",
    "        im12 = axs[1,2].contour(rr, zz, gso, gso_levels, linestyles='dashed', cmap=\"inferno\")\n",
    "        axs[1,2].contour(rr, zz, gso_pred, gso_levels, cmap=\"inferno\")\n",
    "        fig.colorbar(im12, ax=axs[1,2])\n",
    "        im13 = axs[1,3].contourf(rr, zz, np.clip(gso_mae, lev0[0]+ε, lev0[-1]-ε), lev0, cmap=\"inferno\")\n",
    "        fig.colorbar(im13, ax=axs[1,3])\n",
    "        im14 = axs[1,4].contourf(rr, zz, np.clip(gso_mae, lev1[0]+ε, lev1[-1]-ε), lev1, cmap=\"inferno\")\n",
    "        fig.colorbar(im14, ax=axs[1,4])\n",
    "\n",
    "        for ax in axs.flatten(): \n",
    "            ax.grid(False), ax.set_xticks([]), ax.set_yticks([]), ax.set_aspect(\"equal\")\n",
    "            ax.plot(VESS[:,0], VESS[:,1], color=\"white\", linewidth=2)\n",
    "\n",
    "        #suptitle\n",
    "        plt.suptitle(f\"[{JOBID}] EasyPlaNet: {titl} {i}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show() if HAS_SCREEN else plt.savefig(f\"{SAVE_DIR}/imgs/planet_{titl}_{i}.png\")\n",
    "        \n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test inference speed\n",
    "model = EasyPlaNet()\n",
    "model.load_state_dict(torch.load(f\"{SAVE_DIR}/{best_model_path}\"))\n",
    "model.eval()\n",
    "ds = PlaNetDataset(EVAL_DS_PATH)\n",
    "n_samples = 100\n",
    "random_idxs = np.random.choice(n_samples, len(ds))\n",
    "#cpu\n",
    "￼\n",
    "cpu_times1, cpu_times2 = [], []\n",
    "for i in random_idxs:\n",
    "    start_t = time()\n",
    "    input, psi_ds, rr, zz = ds[i]\n",
    "    input, psi_ds, rr, zz = input.to('cpu'), psi_ds.to('cpu'), rr.to('cpu'), zz.to('cpu')\n",
    "    input, psi_ds, rr, zz = input.view(1,-1), psi_ds.view(1,1,NGZ,NGR), rr.view(1,1,NGZ,NGR), zz.view(1,1,NGZ,NGR)\n",
    "    start_t2 = time()\n",
    "    psi_pred = model(input, rr, zz)\n",
    "    end_t = time()\n",
    "    cpu_times1.append(end_t - start_t); cpu_times2.append(end_t - start_t2) \n",
    "# device\n",
    "model.to(device)\n",
    "dev_times1, dev_times2 = [], []\n",
    "for i in random_idxs:\n",
    "    input, psi_ds, rr, zz = ds[i]\n",
    "    input, psi_ds, rr, zz = input.view(1,-1), psi_ds.view(1,1,NGZ,NGR), rr.view(1,1,NGZ,NGR), zz.view(1,1,NGZ,NGR)\n",
    "    start_t = time()\n",
    "    start_t2 = time()\n",
    "    psi_pred = model(input, rr, zz)\n",
    "    end_t = time()\n",
    "    dev_times1.append(end_t - start_t); dev_times2.append(end_t - start_t2)    \n",
    "cpu_times1, dev_times1 = np.array(cpu_times1)*1000, np.array(dev_times1)*1000\n",
    "cpu_times2, dev_times2 = np.array(cpu_times2)*1000, np.array(dev_times2)*1000\n",
    "print(f\"cpu: inference time: [full -> {cpu_times1.mean():.5f}ms, std: {cpu_times1.std():.5f}]\")\n",
    "print(f\"cpu: inference time: [inference only -> {cpu_times2.mean():.5f}ms, std: {cpu_times2.std():.5f}]\")\n",
    "print(f\"dev: inference time: [full -> {dev_times1.mean():.5f}ms, std: {dev_times1.std():.5f}]\")\n",
    "print(f\"dev: inference time: [inference only -> {dev_times2.mean():.5f}ms, std: {dev_times2.std():.5f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{JOBID} done\", flush=True)\n",
    "if not HAS_SCREEN: sleep(30) # wait for files to update (for cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy the log file to the folder\n",
    "os.system(f\"cp jobs/{JOBID}.txt {SAVE_DIR}/log.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
